{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy matplotlib seaborn scikit-learn"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZQs-TxkXiEGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1yF5XWxnebp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "4차시 실습 통합 실행 파일\n",
        "모든 실습을 한 번에 실행할 수 있습니다.\n",
        "\n",
        "Part 1: 기본 설정 및 모델 정의\n",
        "Part 2: 지도학습 (분류와 회귀)\n",
        "Part 3: 비지도학습과 편향-분산\n",
        "Part 4: K-Fold 교차검증\n",
        "Part 5: 평가 지표 계산\n",
        "Part 6: 전체 ML 파이프라인\n",
        "\n",
        "필수 라이브러리:\n",
        "pip install torch numpy matplotlib seaborn scikit-learn\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, make_regression, make_blobs\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_auc_score, roc_curve,\n",
        "    mean_absolute_error, mean_squared_error, r2_score\n",
        ")\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 재현성을 위한 시드 고정\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 한글 깨짐 방지\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"4차시 실습: 인공지능 개론 - 통합 실행\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# Part 1: 모델 정의\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 1] 모델 정의 중...\")\n",
        "\n",
        "class BinaryClassifier(nn.Module):\n",
        "    \"\"\"이진 분류용 다층 퍼셉트론\"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()          # 사용할 활성화 함수(은닉층)\n",
        "        self.dropout = nn.Dropout(0.3) # 과적합 방지 위해\n",
        "        self.sigmoid = nn.Sigmoid()    # 이진분류(1,0) 위함(출력층)\n",
        "\n",
        "    # 순전파\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    \"\"\"회귀용 다층 퍼셉트론\"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "print(\"모델 정의 완료\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# Part 2: 지도학습\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 2] 지도학습 - 분류와 회귀\")\n",
        "\n",
        "# 분류 데이터 생성 및 학습\n",
        "X_class, y_class = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=15,\n",
        "    n_redundant=5, weights=[0.7, 0.3], random_state=42\n",
        ")\n",
        "\n",
        "X_train_c, X_temp_c, y_train_c, y_temp_c = train_test_split(\n",
        "    X_class, y_class, test_size=0.4, random_state=42, stratify=y_class\n",
        ")\n",
        "X_val_c, X_test_c, y_val_c, y_test_c = train_test_split(\n",
        "    X_temp_c, y_temp_c, test_size=0.5, random_state=42, stratify=y_temp_c\n",
        ")\n",
        "\n",
        "# 정규화 (표준화: 평균 0, 분산 1) 스케일링\n",
        "scaler_c = StandardScaler()\n",
        "X_train_c_scaled = scaler_c.fit_transform(X_train_c)\n",
        "X_val_c_scaled = scaler_c.transform(X_val_c)\n",
        "X_test_c_scaled = scaler_c.transform(X_test_c)\n",
        "\n",
        "# 스케일링 된 데이터를 가지고, 텐서로 변환\n",
        "X_train_c_t = torch.FloatTensor(X_train_c_scaled)\n",
        "y_train_c_t = torch.FloatTensor(y_train_c).unsqueeze(1)\n",
        "X_val_c_t = torch.FloatTensor(X_val_c_scaled)\n",
        "y_val_c_t = torch.FloatTensor(y_val_c).unsqueeze(1)\n",
        "\n",
        "model_class = BinaryClassifier(input_dim=20)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model_class.parameters(), lr=0.001)\n",
        "\n",
        "print(\"분류 모델 학습 중...\")\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "for epoch in range(100):\n",
        "    model_class.train()\n",
        "    # 훈련 모드로 진입\n",
        "    optimizer.zero_grad()\n",
        "    # 최적화 초기화\n",
        "    outputs = model_class(X_train_c_t)\n",
        "    # 예측값 (모델이 예측 )\n",
        "    loss = criterion(outputs, y_train_c_t)\n",
        "    # 손실 계산 (예측값, 실제값)\n",
        "    loss.backward()\n",
        "    # 역전파\n",
        "    optimizer.step()\n",
        "    # 학습률(learning rate) 활용, 학습 >> 손실 줄이기 위해서\n",
        "    # w(next step) = w(current step) -lr (diff f(x)/diff(xtr))\n",
        "\n",
        "    # 평가용 모드 선언(모델의 성능평가)\n",
        "    model_class.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model_class(X_val_c_t)         # 반드시, 평가용 데이터 활용\n",
        "        val_loss = criterion(val_outputs, y_val_c_t)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = model_class.state_dict()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= 10:\n",
        "        break\n",
        "        # 정지조건\n",
        "\n",
        "model_class.load_state_dict(best_model_state)\n",
        "print(\"분류 모델 학습 완료\")\n",
        "\n",
        "# 회귀 데이터 생성 및 학습\n",
        "X_reg, y_reg = make_regression(\n",
        "    n_samples=800, n_features=10, n_informative=8,\n",
        "    noise=10.0, random_state=42\n",
        ")\n",
        "\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_r = StandardScaler()\n",
        "X_train_r_scaled = scaler_r.fit_transform(X_train_r)\n",
        "X_test_r_scaled = scaler_r.transform(X_test_r)\n",
        "\n",
        "X_train_r_t = torch.FloatTensor(X_train_r_scaled)\n",
        "y_train_r_t = torch.FloatTensor(y_train_r).unsqueeze(1)\n",
        "\n",
        "model_reg = Regressor(input_dim=10)\n",
        "criterion_reg = nn.MSELoss()\n",
        "optimizer_reg = optim.Adam(model_reg.parameters(), lr=0.01)\n",
        "\n",
        "print(\"회귀 모델 학습 중...\")\n",
        "for epoch in range(100):\n",
        "    model_reg.train()\n",
        "    optimizer_reg.zero_grad()\n",
        "    outputs = model_reg(X_train_r_t)\n",
        "    loss = criterion_reg(outputs, y_train_r_t)\n",
        "    loss.backward()\n",
        "    optimizer_reg.step()\n",
        "\n",
        "print(\"회귀 모델 학습 완료\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# Part 3: 비지도학습과 편향-분산\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 3] 비지도학습과 편향-분산\")\n",
        "\n",
        "# K-Means 군집화\n",
        "X_cluster, y_true_cluster = make_blobs(\n",
        "    n_samples=300, centers=3, n_features=2,\n",
        "    cluster_std=1.0, random_state=42\n",
        ")\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "y_pred_cluster = kmeans.fit_predict(X_cluster)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(X_cluster[:, 0], X_cluster[:, 1],\n",
        "            c=y_true_cluster, cmap='viridis',\n",
        "            alpha=0.6, edgecolors='k', s=50)\n",
        "plt.title('True Labels', fontsize=12)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(X_cluster[:, 0], X_cluster[:, 1],\n",
        "            c=y_pred_cluster, cmap='plasma',\n",
        "            alpha=0.6, edgecolors='k', s=50)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0],\n",
        "            kmeans.cluster_centers_[:, 1],\n",
        "            c='red', marker='X', s=300,\n",
        "            edgecolors='black', linewidths=2)\n",
        "plt.title('K-Means Result', fontsize=12)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "cluster_counts = np.bincount(y_pred_cluster)\n",
        "plt.bar(range(len(cluster_counts)), cluster_counts,\n",
        "        color=['#440154', '#31688e', '#fde724'], edgecolor='black')\n",
        "plt.title('Cluster Sizes', fontsize=12)\n",
        "plt.xlabel('Cluster ID')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('result_clustering.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"저장: result_clustering.png\")\n",
        "\n",
        "# 편향-분산 트레이드오프\n",
        "np.random.seed(42)\n",
        "X_bias = np.sort(np.random.rand(100, 1) * 10, axis=0)\n",
        "y_bias = np.sin(X_bias).ravel() + np.random.randn(100) * 0.5\n",
        "X_test_bias = np.linspace(0, 10, 200).reshape(-1, 1)\n",
        "y_test_bias = np.sin(X_test_bias).ravel()\n",
        "\n",
        "degrees = [1, 3, 9, 20]\n",
        "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
        "\n",
        "plt.figure(figsize=(16, 4))\n",
        "\n",
        "for idx, degree in enumerate(degrees):\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X_bias)\n",
        "    X_test_poly = poly.transform(X_test_bias)\n",
        "\n",
        "    model_bias = Ridge(alpha=0.01)\n",
        "    model_bias.fit(X_poly, y_bias)\n",
        "\n",
        "    train_pred = model_bias.predict(X_poly)\n",
        "    test_pred = model_bias.predict(X_test_poly)\n",
        "\n",
        "    train_mse = mean_squared_error(y_bias, train_pred)\n",
        "    test_mse = mean_squared_error(y_test_bias, test_pred)\n",
        "\n",
        "    plt.subplot(1, 4, idx + 1)\n",
        "    plt.scatter(X_bias, y_bias, alpha=0.5, s=30,\n",
        "                color='gray', edgecolors='black')\n",
        "    plt.plot(X_test_bias, y_test_bias, 'g--', linewidth=2.5)\n",
        "    plt.plot(X_test_bias, test_pred, color=colors[idx], linewidth=2.5)\n",
        "    plt.title(f'Degree {degree}\\nTrain: {train_mse:.3f} | Test: {test_mse:.3f}')\n",
        "    plt.ylim(-2.5, 2.5)\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('result_bias_variance.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"저장: result_bias_variance.png\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# Part 4: K-Fold 교차검증\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 4] K-Fold 교차검증\")\n",
        "\n",
        "X_kfold, y_kfold = make_classification(\n",
        "    n_samples=500, n_features=20, n_informative=15, random_state=42\n",
        ")\n",
        "\n",
        "k = 5\n",
        "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "fold_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_kfold)):\n",
        "    X_train_fold = X_kfold[train_idx]\n",
        "    y_train_fold = y_kfold[train_idx]\n",
        "    X_val_fold = X_kfold[val_idx]\n",
        "    y_val_fold = y_kfold[val_idx]\n",
        "\n",
        "    scaler_fold = StandardScaler()\n",
        "    X_train_fold = scaler_fold.fit_transform(X_train_fold)\n",
        "    X_val_fold = scaler_fold.transform(X_val_fold)\n",
        "\n",
        "    X_train_fold_t = torch.FloatTensor(X_train_fold)\n",
        "    y_train_fold_t = torch.FloatTensor(y_train_fold).unsqueeze(1)\n",
        "    X_val_fold_t = torch.FloatTensor(X_val_fold)\n",
        "\n",
        "    model_fold = BinaryClassifier(input_dim=20)\n",
        "    optimizer_fold = optim.Adam(model_fold.parameters(), lr=0.01)\n",
        "    criterion_fold = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(30):\n",
        "        model_fold.train()\n",
        "        optimizer_fold.zero_grad()\n",
        "        outputs = model_fold(X_train_fold_t)\n",
        "        loss = criterion_fold(outputs, y_train_fold_t)\n",
        "        loss.backward()\n",
        "        optimizer_fold.step()\n",
        "\n",
        "    model_fold.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred_prob = model_fold(X_val_fold_t).numpy().flatten()\n",
        "        val_pred = (val_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(y_val_fold, val_pred)\n",
        "    fold_scores.append(accuracy)\n",
        "\n",
        "print(f\"K-Fold 평균 Accuracy: {np.mean(fold_scores):.4f} (std: {np.std(fold_scores):.4f})\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# Part 5: 평가 지표\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 5] 평가 지표 계산\")\n",
        "\n",
        "# 분류 평가\n",
        "model_class.eval()\n",
        "X_test_c_t = torch.FloatTensor(X_test_c_scaled)\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred_prob_c = model_class(X_test_c_t).numpy().flatten()\n",
        "    y_pred_c = (y_pred_prob_c > 0.5).astype(int)\n",
        "\n",
        "cm = confusion_matrix(y_test_c, y_pred_c)\n",
        "accuracy = accuracy_score(y_test_c, y_pred_c)\n",
        "precision = precision_score(y_test_c, y_pred_c, zero_division=0)\n",
        "recall = recall_score(y_test_c, y_pred_c, zero_division=0)\n",
        "f1 = f1_score(y_test_c, y_pred_c, zero_division=0)\n",
        "auc = roc_auc_score(y_test_c, y_pred_prob_c)\n",
        "\n",
        "# acc(정확도), precision(정밀도), recall(재현율), f1-score, auc\n",
        "print(f\"분류 성능: Acc={accuracy:.3f}, Prec={precision:.3f}, Rec={recall:.3f}, F1={f1:.3f}, AUC={auc:.3f}\")\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Pred 0', 'Pred 1'],\n",
        "            yticklabels=['True 0', 'True 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
        "values = [accuracy, precision, recall, f1, auc]\n",
        "plt.barh(metrics, values, color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12', '#9b59b6'])\n",
        "plt.xlim(0, 1.0)\n",
        "plt.title('Metrics')\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "fpr, tpr, _ = roc_curve(y_test_c, y_pred_prob_c)\n",
        "plt.plot(fpr, tpr, linewidth=3, label=f'AUC={auc:.3f}')\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
        "plt.xlabel('FPR') # false positive rate\n",
        "plt.ylabel('TPR') # true positive rate\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('result_classification.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"저장: result_classification.png\")\n",
        "\n",
        "# 회귀 평가\n",
        "model_reg.eval()\n",
        "X_test_r_t = torch.FloatTensor(X_test_r_scaled)\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred_reg = model_reg(X_test_r_t).numpy().flatten()\n",
        "\n",
        "mae = mean_absolute_error(y_test_r, y_pred_reg)\n",
        "mse = mean_squared_error(y_test_r, y_pred_reg)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test_r, y_pred_reg)  # 결정계수 : 모델의 설명력(해석)\n",
        "\n",
        "print(f\"회귀 성능: MAE={mae:.2f}, RMSE={rmse:.2f}, R2={r2:.3f}\")\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(y_test_r, y_pred_reg, alpha=0.6, s=50)\n",
        "plt.plot([y_test_r.min(), y_test_r.max()],\n",
        "         [y_test_r.min(), y_test_r.max()], 'r--', linewidth=3)\n",
        "plt.xlabel('True')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Prediction vs True')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "residuals = y_test_r - y_pred_reg\n",
        "# 각 관측치(관측된 포인트)에서 예측치를 뺀 값 >> 잔차\n",
        "plt.scatter(y_pred_reg, residuals, alpha=0.6, s=50)\n",
        "plt.axhline(y=0, color='r', linestyle='--', linewidth=3)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "plt.axvline(x=0, color='red', linestyle='--', linewidth=3)\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('result_regression.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"저장: result_regression.png\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# Part 6: ML 파이프라인\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 6] ML 파이프라인 실행\")\n",
        "\n",
        "class MLPipeline:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.best_score = 0\n",
        "        self.baseline_score = 0\n",
        "\n",
        "    def run(self, X, y):\n",
        "        print(\"\\nSTEP 1: 문제 정의\")\n",
        "        print(\"  목표: 고객 이탈 예측 (F1 > 0.80)\")\n",
        "\n",
        "        print(\"\\nSTEP 2: 데이터 준비\")\n",
        "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "            X, y, test_size=0.3, random_state=42, stratify=y\n",
        "        )\n",
        "        X_val, X_test, y_val, y_test = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        "        )\n",
        "\n",
        "        self.scaler = StandardScaler()\n",
        "        X_train = self.scaler.fit_transform(X_train)\n",
        "        X_val = self.scaler.transform(X_val)\n",
        "        X_test = self.scaler.transform(X_test)\n",
        "\n",
        "        print(f\"  Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "        print(\"\\nSTEP 3: 베이스라인\")\n",
        "        majority_class = np.bincount(y_train).argmax()\n",
        "        baseline_pred = np.full(len(y_test), majority_class)\n",
        "        self.baseline_score = f1_score(y_test, baseline_pred, zero_division=0)\n",
        "        print(f\"  베이스라인 F1: {self.baseline_score:.4f}\")\n",
        "\n",
        "        print(\"\\nSTEP 4: 모델 학습\")\n",
        "        self.model = BinaryClassifier(input_dim=X.shape[1])\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\n",
        "        X_train_t = torch.FloatTensor(X_train)\n",
        "        y_train_t = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "        X_val_t = torch.FloatTensor(X_val)\n",
        "        y_val_t = torch.FloatTensor(y_val).unsqueeze(1)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(50):\n",
        "            self.model.train()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self.model(X_train_t)\n",
        "            loss = criterion(outputs, y_train_t)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = self.model(X_val_t)\n",
        "                val_loss = criterion(val_outputs, y_val_t)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model_state = self.model.state_dict()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= 10:\n",
        "                break\n",
        "\n",
        "        self.model.load_state_dict(best_model_state)\n",
        "        print(\"  학습 완료\")\n",
        "\n",
        "        print(\"\\nSTEP 5: 최종 평가\")\n",
        "        self.model.eval()\n",
        "        X_test_t = torch.FloatTensor(X_test)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_pred_prob = self.model(X_test_t).numpy().flatten()\n",
        "            y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "        test_f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "        print(f\"  테스트 F1: {test_f1:.4f}\")\n",
        "        print(f\"  베이스라인 대비: {test_f1 - self.baseline_score:+.4f}\")\n",
        "\n",
        "        if test_f1 > 0.80:\n",
        "            print(\"  성공! 목표 달성\")\n",
        "        else:\n",
        "            print(\"  목표 미달, 추가 개선 필요\")\n",
        "\n",
        "X_proj, y_proj = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=15,\n",
        "    weights=[0.65, 0.35], random_state=42\n",
        ")\n",
        "\n",
        "pipeline = MLPipeline()\n",
        "pipeline.run(X_proj, y_proj)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 최종 요약\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"전체 실습 완료\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n생성된 파일:\")\n",
        "print(\"  1. result_clustering.png     - 군집화 결과\")\n",
        "print(\"  2. result_bias_variance.png  - 편향-분산 트레이드오프\")\n",
        "print(\"  3. result_classification.png - 분류 평가\")\n",
        "print(\"  4. result_regression.png     - 회귀 평가\")\n",
        "print(\"\\n모든 실습이 정상적으로 완료되었습니다.\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ]
}
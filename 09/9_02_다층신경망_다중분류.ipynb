{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuhTXI74BM1a"
      },
      "source": [
        "# 심화 실습 1: 다층 신경망을 활용한 다중 분류 (Deep Multi-class Classification)\n",
        "\n",
        "## 학습 목표\n",
        "- Hidden Layer를 추가하여 더 복잡한 패턴 학습\n",
        "- ReLU, Dropout 등 다양한 기법 활용\n",
        "- 학습 과정을 시각화하여 모델 성능 분석\n",
        "- 과적합(Overfitting) 방지 기법 이해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwaXlrlqBM1c"
      },
      "source": [
        "## 1. 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIrqbdb9BM1c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine  # Wine 데이터셋 (13개 특성, 3개 클래스)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 한글 폰트 설정 (Google Colab)\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(f\"PyTorch 버전: {torch.__version__}\")\n",
        "print(f\"사용 가능한 디바이스: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me7v0f3qBM1d"
      },
      "source": [
        "## 2. 데이터 준비 및 전처리\n",
        "\n",
        "**Wine 데이터셋**: 와인의 화학적 특성 13개를 바탕으로 3종류의 와인을 분류하는 문제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEajonu9BM1d"
      },
      "outputs": [],
      "source": [
        "def load_and_prepare_data():\n",
        "    \"\"\"\n",
        "    데이터를 로드하고 전처리하는 함수\n",
        "\n",
        "    Returns:\n",
        "        tuple: (x_train, x_val, x_test, y_train, y_val, y_test, scaler)\n",
        "    \"\"\"\n",
        "    # 데이터 로드\n",
        "    wine = load_wine()\n",
        "    X, y = wine.data, wine.target\n",
        "\n",
        "    print(f\"전체 데이터 크기: {X.shape}\")\n",
        "    print(f\"특성(Feature) 수: {X.shape[1]}\")\n",
        "    print(f\"클래스 수: {len(np.unique(y))}\")\n",
        "    print(f\"클래스별 분포: {np.bincount(y)}\")\n",
        "\n",
        "    # Train:Val:Test = 60:20:20 비율로 분할\n",
        "    X_temp, x_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    x_train, x_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    # 표준화 (Standardization): 평균 0, 표준편차 1로 조정\n",
        "    # 신경망 학습 시 수렴 속도와 안정성 향상\n",
        "    # 코드를 작성하세요\n",
        "    scaler = StandardScaler()\n",
        "    x_train = scaler.fit_transform(x_train)\n",
        "    x_val = scaler.transform(x_val)\n",
        "    x_test = scaler.transform(x_test)\n",
        "\n",
        "    print(f\"\\n훈련 데이터: {x_train.shape[0]}개\")\n",
        "    print(f\"검증 데이터: {x_val.shape[0]}개\")\n",
        "    print(f\"테스트 데이터: {x_test.shape[0]}개\")\n",
        "\n",
        "    return x_train, x_val, x_test, y_train, y_val, y_test, scaler\n",
        "\n",
        "# 데이터 준비\n",
        "x_train, x_val, x_test, y_train, y_val, y_test, scaler = load_and_prepare_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVlTwZTWBM1e"
      },
      "source": [
        "## 3. 텐서 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0_9kbecBM1e"
      },
      "outputs": [],
      "source": [
        "def convert_to_tensors(x_train, x_val, x_test, y_train, y_val, y_test):\n",
        "    \"\"\"\n",
        "    NumPy 배열을 PyTorch 텐서로 변환\n",
        "\n",
        "    Args:\n",
        "        x_train, x_val, x_test: 입력 데이터 (numpy array)\n",
        "        y_train, y_val, y_test: 레이블 데이터 (numpy array)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (inputs_train, inputs_val, inputs_test, labels_train, labels_val, labels_test)\n",
        "    \"\"\"\n",
        "    # 입력 데이터는 float32 타입\n",
        "    # 코드 작성\n",
        "    inputs_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "    inputs_val = torch.tensor(x_val, dtype=torch.float32)\n",
        "    inputs_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "    # 레이블은 long(int64) 타입 - CrossEntropyLoss 요구사항\n",
        "    # 코드 작성\n",
        "    labels_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    labels_val = torch.tensor(y_val, dtype=torch.long)\n",
        "    labels_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    print(f\"훈련 입력 텐서 shape: {inputs_train.shape}, dtype: {inputs_train.dtype}\")\n",
        "    print(f\"훈련 레이블 텐서 shape: {labels_train.shape}, dtype: {labels_train.dtype}\")\n",
        "\n",
        "    return inputs_train, inputs_val, inputs_test, labels_train, labels_val, labels_test\n",
        "\n",
        "# 텐서 변환\n",
        "inputs_train, inputs_val, inputs_test, labels_train, labels_val, labels_test = convert_to_tensors(\n",
        "    x_train, x_val, x_test, y_train, y_val, y_test\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14J3YXeCBM1e"
      },
      "source": [
        "## 4. 다층 신경망 모델 정의\n",
        "\n",
        "### 주요 개념\n",
        "- **Hidden Layer**: 입력과 출력 사이의 은닉층으로, 복잡한 패턴 학습 가능\n",
        "- **ReLU (Rectified Linear Unit)**: 활성화 함수, f(x) = max(0, x)\n",
        "  - 음수는 0으로, 양수는 그대로 통과\n",
        "  - 기울기 소실(Gradient Vanishing) 문제 완화\n",
        "- **Dropout**: 과적합 방지 기법\n",
        "  - 학습 시 일부 뉴런을 무작위로 비활성화\n",
        "  - 모델의 일반화 성능 향상"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZMhfsX7BM1e"
      },
      "outputs": [],
      "source": [
        "class DeepMultiClassNet(nn.Module):\n",
        "    def __init__(self, n_input, n_hidden1, n_hidden2, n_output, dropout_rate=0.3):\n",
        "        \"\"\"\n",
        "        다층 신경망 모델\n",
        "\n",
        "        구조: Input -> Hidden1 -> ReLU -> Dropout -> Hidden2 -> ReLU -> Dropout -> Output\n",
        "\n",
        "        Args:\n",
        "            n_input: 입력 특성 수\n",
        "            n_hidden1: 첫 번째 은닉층 뉴런 수\n",
        "            n_hidden2: 두 번째 은닉층 뉴런 수\n",
        "            n_output: 출력 클래스 수\n",
        "            dropout_rate: 드롭아웃 비율 (0.0 ~ 1.0)\n",
        "        \"\"\"\n",
        "        super(DeepMultiClassNet, self).__init__()\n",
        "\n",
        "        # 계층 정의\n",
        "        self.fc1 = nn.Linear(n_input, n_hidden1)      # 첫 번째 완전연결층\n",
        "        self.relu1 = nn.ReLU()                        # ReLU 활성화 함수\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)      # 드롭아웃\n",
        "\n",
        "        self.fc2 = nn.Linear(n_hidden1, n_hidden2)    # 두 번째 완전연결층\n",
        "        self.relu2 = nn.ReLU()                        # ReLU 활성화 함수\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)      # 드롭아웃\n",
        "\n",
        "        self.fc3 = nn.Linear(n_hidden2, n_output)     # 출력층 (Softmax는 손실함수에 포함)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        순전파(Forward Propagation)\n",
        "\n",
        "        Args:\n",
        "            x: 입력 텐서\n",
        "\n",
        "        Returns:\n",
        "            logits: 원본 출력값 (Softmax 적용 전)\n",
        "        \"\"\"\n",
        "        # 첫 번째 은닉층\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # 두 번째 은닉층\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # 출력층 (logits 반환)\n",
        "        # logits : 모델이 예측한 값\n",
        "        logits = self.fc3(x)\n",
        "        return logits\n",
        "\n",
        "# 모델 하이퍼파라미터\n",
        "n_input = inputs_train.shape[1]   # 13개 특성\n",
        "n_hidden1 = 64                     # 첫 번째 은닉층: 64개 뉴런\n",
        "n_hidden2 = 32                     # 두 번째 은닉층: 32개 뉴런\n",
        "n_output = 3                       # 3개 클래스\n",
        "dropout_rate = 0.3                 # 30% 드롭아웃\n",
        "\n",
        "# 모델 생성\n",
        "model = DeepMultiClassNet(n_input, n_hidden1, n_hidden2, n_output, dropout_rate)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"모델 구조:\")\n",
        "print(\"=\" * 60)\n",
        "print(model)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 총 파라미터 수 계산\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\n총 파라미터 수: {total_params:,}\")\n",
        "print(f\"학습 가능한 파라미터 수: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHXq4nCiBM1e"
      },
      "source": [
        "## 5. 손실 함수 및 옵티마이저 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxBTx6LzBM1f"
      },
      "outputs": [],
      "source": [
        "# 손실 함수: CrossEntropyLoss (Softmax + NLLLoss 포함)\n",
        "# 코드 작성\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# 옵티마이저: Adam (Adaptive Moment Estimation)\n",
        "# SGD보다 학습률 조정이 자동화되어 더 빠르고 안정적인 학습 가능\n",
        "learning_rate = 0.001\n",
        "# 코드 작성\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "print(f\"손실 함수: {criterion}\")\n",
        "print(f\"옵티마이저: {optimizer.__class__.__name__}\")\n",
        "print(f\"학습률: {learning_rate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtLthn5YBM1f"
      },
      "source": [
        "## 6. 모델 평가 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EjnJNL-BM1f"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, inputs, labels):\n",
        "    \"\"\"\n",
        "    모델의 손실과 정확도를 계산하는 함수\n",
        "\n",
        "    Args:\n",
        "        model: 평가할 모델\n",
        "        inputs: 입력 텐서\n",
        "        labels: 정답 레이블 텐서\n",
        "\n",
        "    Returns:\n",
        "        tuple: (loss, accuracy)\n",
        "    \"\"\"\n",
        "    model.eval()  # 평가 모드 (Dropout, BatchNorm 등 비활성화)\n",
        "    # 코드 작성\n",
        "    with torch.no_grad():\n",
        "      # 기울기 계산 비활성화\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # 손실 계산\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # 예측 클래스 추출(가장 높은 logit 값의 index)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      # _ : 최대값 >> 여기서는 인덱스만 필요하니깐 필요없어서 무시(_)\n",
        "      # predicted: 인덱스(클래스 번호)\n",
        "\n",
        "      # 정확도 계산\n",
        "      correct = (predicted == labels).sum().item()\n",
        "      accuracy = correct / len(labels)\n",
        "\n",
        "    return loss.item(), accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs 뭔가요? (jhp) 모델을 통과한 logits(모델이 예측한 값)\n",
        "output_temp = torch.tensor([[2.5, 1.1, 0.3]])\n",
        "torch.max(output_temp, 1)\n",
        "# tensor([2.5000]), tensor([0])"
      ],
      "metadata": {
        "id": "i5e2msZOg7Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, 예측치 = torch.max(output_temp, 1)\n",
        "\n",
        "print(예측치)\n",
        "print(예측치.item())"
      ],
      "metadata": {
        "id": "t7lRhyuphyFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0UzGc8vBM1f"
      },
      "source": [
        "## 7. 학습 루프 (Training Loop)\n",
        "\n",
        "### 학습 과정\n",
        "1. **순전파(Forward)**: 입력 → 출력 계산\n",
        "2. **손실 계산**: 예측값과 실제값의 차이\n",
        "3. **역전파(Backward)**: 손실에 대한 각 파라미터의 기울기 계산\n",
        "4. **가중치 업데이트**: 기울기를 사용하여 파라미터 조정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HqmUf6-BM1f"
      },
      "outputs": [],
      "source": [
        "def train_model(model, inputs_train, labels_train, inputs_val, labels_val,\n",
        "                num_epochs=200, print_interval=20):\n",
        "    \"\"\"\n",
        "    모델을 학습시키는 함수\n",
        "\n",
        "    Args:\n",
        "        model: 학습할 모델\n",
        "        inputs_train: 훈련 입력 텐서\n",
        "        labels_train: 훈련 레이블 텐서\n",
        "        inputs_val: 검증 입력 텐서\n",
        "        labels_val: 검증 레이블 텐서\n",
        "        num_epochs: 학습 에포크 수\n",
        "        print_interval: 출력 간격\n",
        "\n",
        "    Returns:\n",
        "        dict: 학습 이력 (train_losses, val_losses, train_accs, val_accs)\n",
        "    \"\"\"\n",
        "    # 학습 이력 저장용 리스트\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"{'Epoch':^10} | {'Train Loss':^12} | {'Train Acc':^10} | {'Val Loss':^12} | {'Val Acc':^10}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # --- 학습 단계 ---\n",
        "        model.train()  # 학습 모드 (Dropout 활성화)\n",
        "\n",
        "        # 1. 순전파\n",
        "        outputs = model(inputs_train)\n",
        "\n",
        "        # 2. 손실 계산\n",
        "        loss = criterion(outputs, labels_train)\n",
        "\n",
        "        # 3. 역전파\n",
        "        optimizer.zero_grad()  # 기울기 초기화\n",
        "        loss.backward()        # 역전파 (기울기 계산)\n",
        "\n",
        "        # 4. 가중치 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- 평가 단계 ---\n",
        "        # 훈련 데이터 평가\n",
        "        train_loss, train_acc = evaluate_model(model, inputs_train, labels_train)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # 검증 데이터 평가\n",
        "        val_loss, val_acc = evaluate_model(model, inputs_val, labels_val)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # 결과 출력\n",
        "        if (epoch + 1) % print_interval == 0:\n",
        "            print(f\"{epoch+1:^10} | {train_loss:^12.4f} | {train_acc:^10.4f} | {val_loss:^12.4f} | {val_acc:^10.4f}\")\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"학습 완료!\")\n",
        "\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_accs': val_accs\n",
        "    }\n",
        "\n",
        "# 모델 학습 실행\n",
        "history = train_model(\n",
        "    model,\n",
        "    inputs_train, labels_train,\n",
        "    inputs_val, labels_val,\n",
        "    num_epochs=200,\n",
        "    print_interval=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRcJ0WlMBM1f"
      },
      "source": [
        "## 8. 학습 과정 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16Vdz2pBBM1f"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    학습 과정을 시각화하는 함수\n",
        "\n",
        "    Args:\n",
        "        history: 학습 이력 딕셔너리\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # 손실(Loss) 그래프\n",
        "    axes[0].plot(history['train_losses'], label='Train Loss', linewidth=2)\n",
        "    axes[0].plot(history['val_losses'], label='Validation Loss', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title('Loss Curve', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=11)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 정확도(Accuracy) 그래프\n",
        "    axes[1].plot(history['train_accs'], label='Train Accuracy', linewidth=2)\n",
        "    axes[1].plot(history['val_accs'], label='Validation Accuracy', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[1].set_title('Accuracy Curve', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=11)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 최종 결과 출력\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"최종 학습 결과\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"훈련 손실: {history['train_losses'][-1]:.4f}\")\n",
        "    print(f\"검증 손실: {history['val_losses'][-1]:.4f}\")\n",
        "    print(f\"훈련 정확도: {history['train_accs'][-1]:.4f} ({history['train_accs'][-1]*100:.2f}%)\")\n",
        "    print(f\"검증 정확도: {history['val_accs'][-1]:.4f} ({history['val_accs'][-1]*100:.2f}%)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# 학습 과정 시각화\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q86zyWnOBM1f"
      },
      "source": [
        "## 9. 테스트 데이터 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvaB4251BM1g"
      },
      "outputs": [],
      "source": [
        "def test_model(model, inputs_test, labels_test):\n",
        "    \"\"\"\n",
        "    테스트 데이터로 최종 성능 평가\n",
        "\n",
        "    Args:\n",
        "        model: 평가할 모델\n",
        "        inputs_test: 테스트 입력 텐서\n",
        "        labels_test: 테스트 레이블 텐서\n",
        "    \"\"\"\n",
        "    test_loss, test_acc = evaluate_model(model, inputs_test, labels_test)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"테스트 데이터 최종 성능\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"테스트 손실: {test_loss:.4f}\")\n",
        "    print(f\"테스트 정확도: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 클래스별 예측 결과\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs_test)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # 클래스별 정확도 계산\n",
        "    for class_idx in range(3):\n",
        "        class_mask = (labels_test == class_idx)\n",
        "        class_correct = (predicted[class_mask] == labels_test[class_mask]).sum().item()\n",
        "        class_total = class_mask.sum().item()\n",
        "        class_acc = class_correct / class_total if class_total > 0 else 0\n",
        "        print(f\"클래스 {class_idx} 정확도: {class_acc:.4f} ({class_acc*100:.2f}%) - {class_correct}/{class_total}개\")\n",
        "\n",
        "# 테스트 평가\n",
        "test_model(model, inputs_test, labels_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDO3LFI9BM1g"
      },
      "source": [
        "## 10. 예측 예시"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHbS6q9KBM1g"
      },
      "outputs": [],
      "source": [
        "def predict_sample(model, inputs, labels, sample_idx=0):\n",
        "    \"\"\"\n",
        "    개별 샘플에 대한 예측 및 확률 출력\n",
        "\n",
        "    Args:\n",
        "        model: 예측할 모델\n",
        "        inputs: 입력 텐서\n",
        "        labels: 정답 레이블 텐서\n",
        "        sample_idx: 예측할 샘플 인덱스\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 단일 샘플 선택 (배치 차원 추가)\n",
        "    sample_input = inputs[sample_idx:sample_idx+1]\n",
        "    true_label = labels[sample_idx].item()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 모델 출력 (logits)\n",
        "        output = model(sample_input)\n",
        "\n",
        "        # Softmax 적용하여 확률 계산\n",
        "        probabilities = torch.softmax(output, dim=1)[0]\n",
        "\n",
        "        # 예측 클래스\n",
        "        predicted_label = torch.argmax(probabilities).item()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(f\"샘플 {sample_idx} 예측 결과\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"실제 클래스: {true_label}\")\n",
        "    print(f\"예측 클래스: {predicted_label}\")\n",
        "    print(f\"예측 결과: {'정답 ✓' if predicted_label == true_label else '오답 ✗'}\")\n",
        "    print(\"\\n각 클래스별 확률:\")\n",
        "    for i, prob in enumerate(probabilities):\n",
        "        print(f\"  클래스 {i}: {prob:.4f} ({prob*100:.2f}%)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# 테스트 샘플 예측\n",
        "predict_sample(model, inputs_test, labels_test, sample_idx=0)\n",
        "predict_sample(model, inputs_test, labels_test, sample_idx=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuMKRMT9BM1g"
      },
      "source": [
        "## 11. 핵심 개념 정리\n",
        "\n",
        "### 1. 다층 신경망의 장점\n",
        "- **표현력 향상**: Hidden layer를 통해 복잡한 비선형 패턴 학습 가능\n",
        "- **계층적 특성 학습**: 각 층이 점진적으로 추상화된 특성 추출\n",
        "\n",
        "### 2. ReLU 활성화 함수\n",
        "```python\n",
        "# ReLU: f(x) = max(0, x)\n",
        "# 장점: 계산이 빠르고, 기울기 소실 문제 완화\n",
        "# 단점: 음수 입력에 대해 기울기가 0 (Dying ReLU 문제)\n",
        "# 개선하기 위해 leaky ReLU 등장\n",
        "```\n",
        "\n",
        "### 3. Dropout의 역할\n",
        "- **학습 시**: 무작위로 뉴런을 비활성화하여 과적합 방지\n",
        "- **평가 시**: 모든 뉴런 사용 (model.eval()로 자동 전환)\n",
        "\n",
        "### 4. CrossEntropyLoss 동작 원리\n",
        "```python\n",
        "# CrossEntropyLoss = Softmax + Log + NLLLoss\n",
        "# 입력: logits (Softmax 적용 전 값)\n",
        "# 출력: 스칼라 손실 값\n",
        "```\n",
        "\n",
        "### 5. 학습 vs 평가 모드\n",
        "- `model.train()`: Dropout, BatchNorm 활성화\n",
        "- `model.eval()`: Dropout, BatchNorm 비활성화\n",
        "- `torch.no_grad()`: 기울기 계산 비활성화 (메모리 절약)\n",
        "\n",
        "### 6. Adam 옵티마이저\n",
        "- SGD보다 진보된 최적화 알고리즘\n",
        "- 각 파라미터마다 학습률을 자동 조정\n",
        "- Momentum과 RMSProp의 장점 결합"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fXWudl8kk37"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
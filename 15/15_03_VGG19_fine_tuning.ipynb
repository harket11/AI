{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ì‹¤ìŠµ 2: VGG-19-BN íŒŒì¸íŠœë‹ (ì‹¤ë¬´í˜• ê³ ê¸‰)\n",
        "\n",
        "## í•™ìŠµ ëª©í‘œ\n",
        "- VGG-19-BN ì‚¬ì „ í•™ìŠµ ëª¨ë¸ í™œìš©\n",
        "- **Mixed Precision Training**ìœ¼ë¡œ í•™ìŠµ ì†ë„ í–¥ìƒ\n",
        "- **Grad-CAM**ìœ¼ë¡œ ëª¨ë¸ í•´ì„ì„± í™•ë³´\n",
        "- **TensorBoard** ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§\n",
        "- **ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸** ì €ì¥ ë° ë³µì›\n",
        "- **ê³ ê¸‰ ë°ì´í„° ì¦ê°•** ê¸°ë²• ì ìš©\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
      ],
      "metadata": {
        "id": "section1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "!pip install grad-cam tensorboard -q\n",
        "\n",
        "print('ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!')"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler  # Mixed Precision Training\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Grad-CAM\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "print('ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!')"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í™˜ê²½ ì„¤ì •\n",
        "def setup_environment(seed=42):\n",
        "    \"\"\"ì¬í˜„ì„± í™•ë³´ ë° í™˜ê²½ ì„¤ì •\"\"\"\n",
        "    # ì‹œë“œ ê³ ì •\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # í•œê¸€ í°íŠ¸ ì„¤ì •\n",
        "    plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "    plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "    return device\n",
        "\n",
        "device = setup_environment(seed=42)\n",
        "\n",
        "print('=' * 60)\n",
        "print('í™˜ê²½ ì„¤ì • ì™„ë£Œ')\n",
        "print('=' * 60)\n",
        "print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU ì´ë¦„: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
        "print(f'PyTorch ë²„ì „: {torch.__version__}')\n",
        "print('=' * 60)"
      ],
      "metadata": {
        "id": "environment"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ê³ ê¸‰ ë°ì´í„° ì¦ê°• ë° ë¡œë“œ\n",
        "\n",
        "**ê³ ê¸‰ ë°ì´í„° ì¦ê°• ê¸°ë²•:**\n",
        "- **RandomResizedCrop**: ëœë¤ í¬ê¸°ë¡œ ìë¥´ê³  ë¦¬ì‚¬ì´ì¦ˆ\n",
        "- **ColorJitter**: ë°ê¸°, ëŒ€ë¹„, ì±„ë„, ìƒ‰ì¡° ì¡°ì •\n",
        "- **RandomRotation**: ëœë¤ íšŒì „\n",
        "- **RandomErasing**: ì´ë¯¸ì§€ ì¼ë¶€ë¥¼ ëœë¤í•˜ê²Œ ì§€ì›€\n",
        "- **CutOut** íš¨ê³¼ë¡œ ê°•ê±´ì„± í–¥ìƒ"
      ],
      "metadata": {
        "id": "section2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê³ ê¸‰ ë°ì´í„° ì¦ê°•\n",
        "def get_advanced_transforms():\n",
        "    \"\"\"\n",
        "    ê³ ê¸‰ ë°ì´í„° ì¦ê°• íŒŒì´í”„ë¼ì¸\n",
        "    ì‹¤ë¬´ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‹¤ì–‘í•œ ì¦ê°• ê¸°ë²• ì ìš©\n",
        "    \"\"\"\n",
        "    # í›ˆë ¨ ë°ì´í„° ë³€í™˜\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize(128), # img 128*128 resize\n",
        "        transforms.RandomResizedCrop(112, scale=(0.8, 1.0)),\n",
        "        # ëœë¤ í¬ë¡­, 112*112 80%-100% í¬ê¸°ë¡œ resize\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        # 50% í™•ë¥ ë¡œ ì¢Œìš°ë°˜ì „\n",
        "        transforms.RandomRotation(degrees=15),  # ëœë¤ íšŒì „(-15~+15ë„)\n",
        "        transforms.ColorJitter(\n",
        "            brightness=0.3,  # ë°ê¸° ë³€í™”(+-30%)\n",
        "            contrast=0.3,    # ëŒ€ë¹„ ë³€í™”(+-30%)\n",
        "            saturation=0.3,  # ì±„ë„ ë³€í™”(+-30%)\n",
        "            hue=0.1          # ìƒ‰ì¡° ë³€í™”(+-10%)\n",
        "        ),\n",
        "        transforms.ToTensor(), # PIL >> [0,1] í…ì„œë¡œ ë³€í™˜\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        ),\n",
        "        transforms.RandomErasing(p=0.3, scale=(0.02, 0.15))\n",
        "        # CutOut íš¨ê³¼, 30% í™•ë¥ ë¡œ 2-15% ì˜ì—­ ì§€ì›€\n",
        "    ])\n",
        "\n",
        "    # ê²€ì¦ ë°ì´í„° ë³€í™˜ (ì¦ê°• ì—†ìŒ)\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(112),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
        "def load_data(batch_size=50, num_workers=4):\n",
        "    \"\"\"\n",
        "    CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "\n",
        "    Args:\n",
        "        batch_size: ë°°ì¹˜ í¬ê¸°\n",
        "        num_workers: ë°ì´í„° ë¡œë”© ì›Œì»¤ ìˆ˜\n",
        "    \"\"\"\n",
        "    train_transform, val_transform = get_advanced_transforms()\n",
        "\n",
        "    # ë°ì´í„°ì…‹ ìƒì„±\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=train_transform\n",
        "    )\n",
        "\n",
        "    val_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=val_transform\n",
        "    )\n",
        "\n",
        "    # ë°ì´í„° ë¡œë” ìƒì„±\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ\n",
        "        persistent_workers=True  # ì›Œì»¤ ì¬ì‚¬ìš©(ì†ë„ í–¥ìƒ)\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, train_dataset, val_dataset\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ\n",
        "train_loader, val_loader, train_dataset, val_dataset = load_data(\n",
        "    batch_size=50,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# í´ë˜ìŠ¤ ì´ë¦„\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "print('\\në°ì´í„° ë¡œë“œ ì™„ë£Œ')\n",
        "print('=' * 60)\n",
        "print(f'í›ˆë ¨ ë°ì´í„°: {len(train_dataset):,}ê°œ')\n",
        "print(f'ê²€ì¦ ë°ì´í„°: {len(val_dataset):,}ê°œ')\n",
        "print(f'ë°°ì¹˜ í¬ê¸°: {train_loader.batch_size}')\n",
        "print(f'ë°°ì¹˜ ìˆ˜ (í›ˆë ¨): {len(train_loader)}')\n",
        "print(f'ë°°ì¹˜ ìˆ˜ (ê²€ì¦): {len(val_loader)}')\n",
        "print(f'í´ë˜ìŠ¤ ìˆ˜: {len(class_names)}')\n",
        "print('=' * 60)"
      ],
      "metadata": {
        "id": "data_load"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. VGG-19-BN ëª¨ë¸ ìƒì„± ë° ìˆ˜ì •\n",
        "\n",
        "**VGG-19-BN êµ¬ì¡°:**\n",
        "- 19ê°œ ë ˆì´ì–´ (16ê°œ Conv + 3ê°œ FC)\n",
        "- Batch Normalizationìœ¼ë¡œ í•™ìŠµ ì•ˆì •í™”\n",
        "- `classifier[6]` ë ˆì´ì–´ë¥¼ CIFAR-10ì— ë§ê²Œ êµì²´"
      ],
      "metadata": {
        "id": "section3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG-19-BN ëª¨ë¸ ìƒì„±\n",
        "def create_vgg19_bn(num_classes=10, pretrained=True):\n",
        "    \"\"\"\n",
        "    VGG-19-BN ëª¨ë¸ ìƒì„± ë° ìˆ˜ì •\n",
        "\n",
        "    Args:\n",
        "        num_classes: ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜\n",
        "        pretrained: ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€\n",
        "    \"\"\"\n",
        "    # ì‚¬ì „ í•™ìŠµëœ VGG-19-BN ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "    if pretrained:\n",
        "        model = models.vgg19_bn(weights='IMAGENET1K_V1')\n",
        "    else:\n",
        "        model = models.vgg19_bn(weights=None)\n",
        "\n",
        "    # classifier êµ¬ì¡° ì¶œë ¥\n",
        "    print('ì›ë³¸ Classifier êµ¬ì¡°:')\n",
        "    print(model.classifier)\n",
        "    print('\\n' + '=' * 60)\n",
        "\n",
        "    # ë§ˆì§€ë§‰ FC ë ˆì´ì–´ êµì²´\n",
        "    num_features = model.classifier[6].in_features\n",
        "    print(f'ì›ë³¸ FC ë ˆì´ì–´: Linear(in_features={num_features}, out_features=1000)')\n",
        "\n",
        "    # CIFAR-10ì˜ 10ê°œ í´ë˜ìŠ¤ì— ë§ê²Œ êµì²´\n",
        "    model.classifier[6] = nn.Linear(num_features, num_classes)\n",
        "    print(f'êµì²´ëœ FC ë ˆì´ì–´: Linear(in_features={num_features}, out_features={num_classes})')\n",
        "    print('=' * 60)\n",
        "\n",
        "    # ëª¨ë¸ì„ GPUë¡œ ì´ë™\n",
        "    model = model.to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "# ëª¨ë¸ ìƒì„±\n",
        "model = create_vgg19_bn(num_classes=10, pretrained=True)\n",
        "\n",
        "# ëª¨ë¸ íŒŒë¼ë¯¸í„° í†µê³„\n",
        "def count_parameters(model):\n",
        "    \"\"\"ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\"\"\"\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "total_params, trainable_params = count_parameters(model)\n",
        "\n",
        "print('\\nëª¨ë¸ íŒŒë¼ë¯¸í„° í†µê³„:')\n",
        "print('=' * 60)\n",
        "print(f'ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}')\n",
        "print(f'í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}')\n",
        "print(f'ëª¨ë¸ í¬ê¸°: {total_params * 4 / 1024 / 1024:.2f} MB (float32 ê¸°ì¤€)')\n",
        "print('=' * 60)"
      ],
      "metadata": {
        "id": "model_creation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. í•™ìŠµ ì„¤ì • (Mixed Precision Training)\n",
        "\n",
        "**Mixed Precision Training:**\n",
        "- FP16 (16ë¹„íŠ¸)ê³¼ FP32 (32ë¹„íŠ¸)ë¥¼ í˜¼í•© ì‚¬ìš©\n",
        "- í•™ìŠµ ì†ë„ í–¥ìƒ (ìµœëŒ€ 2-3ë°°)\n",
        "- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ\n",
        "- `GradScaler`ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ë§"
      ],
      "metadata": {
        "id": "section4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ì„¤ì •\n",
        "def setup_training(model, train_loader, num_epochs=10, lr=0.001):\n",
        "    \"\"\"\n",
        "    í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  êµ¬ì„± ìš”ì†Œ ì„¤ì •\n",
        "\n",
        "    Returns:\n",
        "        criterion: ì†ì‹¤ í•¨ìˆ˜\n",
        "        optimizer: ìµœì í™” í•¨ìˆ˜\n",
        "        scheduler: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬\n",
        "        scaler: Mixed Precisionìš© GradScaler\n",
        "    \"\"\"\n",
        "    # ì†ì‹¤ í•¨ìˆ˜\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    # Label Smoothing=0.1 >> ì •ë‹µ : 0.9 ì˜¤ë‹µ : 0.1/9\n",
        "\n",
        "    # ìµœì í™” í•¨ìˆ˜\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        momentum=0.9,\n",
        "        weight_decay=5e-4,  # L2 ì •ê·œí™”\n",
        "        nesterov=True  # Nesterov Momentum\n",
        "    )\n",
        "\n",
        "    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (OneCycleLR)\n",
        "    # Learning Rateë¥¼ ì‚¼ê°íŒŒ í˜•íƒœë¡œ ë³€í™”ì‹œì¼œ ë¹ ë¥¸ ìˆ˜ë ´ê³¼ ì¢‹ì€ ì„±ëŠ¥ ë‹¬ì„±\n",
        "    # OneCycleLR : warm-up, annealing (cosign ê°ì‡ )\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr * 10,  # ìµœëŒ€ í•™ìŠµë¥ \n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.3,  # warm-up ë¹„ìœ¨\n",
        "        anneal_strategy='cos',  # ì½”ì‚¬ì¸ ê°ì†Œ\n",
        "        div_factor=10,  # ì´ˆê¸° í•™ìŠµë¥  = max_lr / div_factor(10) >> 0.001\n",
        "        final_div_factor=100  # ìµœì¢… í•™ìŠµë¥  = ì´ˆê¸° í•™ìŠµë¥  / final_div_factor\n",
        "    )\n",
        "\n",
        "    # Mixed Precision Trainingì„ ìœ„í•œ GradScaler\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    return criterion, optimizer, scheduler, scaler\n",
        "\n",
        "# í•™ìŠµ ì„¤ì •\n",
        "num_epochs = 10\n",
        "criterion, optimizer, scheduler, scaler = setup_training(\n",
        "    model,\n",
        "    train_loader,\n",
        "    num_epochs=num_epochs,\n",
        "    lr=0.001\n",
        ")\n",
        "\n",
        "print('í•™ìŠµ ì„¤ì • ì™„ë£Œ')\n",
        "print('=' * 60)\n",
        "print(f'ì—í­ ìˆ˜: {num_epochs}')\n",
        "print(f'ì´ˆê¸° í•™ìŠµë¥ : {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "print(f'ìµœì í™” í•¨ìˆ˜: {optimizer.__class__.__name__}')\n",
        "print(f'ìŠ¤ì¼€ì¤„ëŸ¬: {scheduler.__class__.__name__}')\n",
        "print(f'ì†ì‹¤ í•¨ìˆ˜: {criterion.__class__.__name__} (Label Smoothing=0.1)')\n",
        "print(f'Mixed Precision Training: í™œì„±í™”')\n",
        "print('=' * 60)"
      ],
      "metadata": {
        "id": "training_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. TensorBoard ì„¤ì •\n",
        "\n",
        "**TensorBoard:**\n",
        "- í•™ìŠµ ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§\n",
        "- ì†ì‹¤, ì •í™•ë„, í•™ìŠµë¥  ë“±ì„ ì‹œê°í™”\n",
        "- ëª¨ë¸ ê·¸ë˜í”„ ë° íˆìŠ¤í† ê·¸ë¨ í™•ì¸"
      ],
      "metadata": {
        "id": "section5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datetime.now()\n",
        "datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
      ],
      "metadata": {
        "id": "zc5YIofWsc02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorBoard ì„¤ì •\n",
        "log_dir = f'runs/vgg19_bn_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
        "writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "print(f'TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬: {log_dir}')\n",
        "print('\\nTensorBoard ì‹¤í–‰ ë°©ë²•:')\n",
        "print('1. ìƒˆ ì…€ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰:')\n",
        "print('   %load_ext tensorboard')\n",
        "print(f'   %tensorboard --logdir {log_dir}')\n",
        "print('\\në˜ëŠ” ë¡œì»¬ì—ì„œ:')\n",
        "print(f'   tensorboard --logdir={log_dir}')"
      ],
      "metadata": {
        "id": "tensorboard"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. í•™ìŠµ ë° ê²€ì¦ í•¨ìˆ˜ (Mixed Precision)\n",
        "\n",
        "**Mixed Precision Training í”„ë¡œì„¸ìŠ¤:**\n",
        "1. `autocast()`ë¡œ ìˆœì „íŒŒë¥¼ FP16ìœ¼ë¡œ ìˆ˜í–‰\n",
        "2. ì†ì‹¤ ê³„ì‚°\n",
        "3. `scaler.scale()`ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ë§\n",
        "4. ì—­ì „íŒŒ ë° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸"
      ],
      "metadata": {
        "id": "section6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í›ˆë ¨ í•¨ìˆ˜ (Mixed Precision)\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, device, epoch):\n",
        "    \"\"\"\n",
        "    í•œ ì—í­ í›ˆë ¨ (Mixed Precision Training)\n",
        "\n",
        "    Returns:\n",
        "        epoch_loss: í‰ê·  ì†ì‹¤\n",
        "        epoch_acc: ì •í™•ë„\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1} [Train]')\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(pbar):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed Precision Training\n",
        "        with autocast():  # FP16 ëª¨ë“œ ìˆœì „íŒŒ\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ë§ ë° ì—­ì „íŒŒ\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (OneCycleLRì€ ë§¤ ë°°ì¹˜ë§ˆë‹¤ ì—…ë°ì´íŠ¸)\n",
        "        scheduler.step()\n",
        "\n",
        "        # í†µê³„ ê³„ì‚°\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
        "        current_acc = 100 * correct / total\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'acc': f'{current_acc:.2f}%',\n",
        "            'lr': f'{current_lr:.6f}'\n",
        "        })\n",
        "\n",
        "        # TensorBoard ë¡œê¹… (ë§¤ 100 ë°°ì¹˜ë§ˆë‹¤)\n",
        "        if batch_idx % 100 == 0:\n",
        "            global_step = epoch * len(train_loader) + batch_idx\n",
        "            writer.add_scalar('Train/Loss_step', loss.item(), global_step)\n",
        "            writer.add_scalar('Train/Acc_step', current_acc, global_step)\n",
        "            writer.add_scalar('Train/LR', current_lr, global_step)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# ê²€ì¦ í•¨ìˆ˜\n",
        "def validate(model, val_loader, criterion, device, epoch):\n",
        "    \"\"\"\n",
        "    ê²€ì¦ ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€\n",
        "\n",
        "    Returns:\n",
        "        epoch_loss: í‰ê·  ì†ì‹¤\n",
        "        epoch_acc: ì •í™•ë„\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc=f'Epoch {epoch+1} [Val]')\n",
        "\n",
        "        for inputs, labels in pbar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Mixed Precision (ê²€ì¦ ì‹œì—ë„ ì ìš© ê°€ëŠ¥)\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # í†µê³„ ê³„ì‚°\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100 * correct / total:.2f}%'\n",
        "            })\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "print('í•™ìŠµ ë° ê²€ì¦ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')"
      ],
      "metadata": {
        "id": "train_val_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "\n",
        "**ì²´í¬í¬ì¸íŠ¸:**\n",
        "- ëª¨ë¸ ê°€ì¤‘ì¹˜\n",
        "- ìµœì í™” í•¨ìˆ˜ ìƒíƒœ\n",
        "- ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ\n",
        "- í•™ìŠµ íˆìŠ¤í† ë¦¬"
      ],
      "metadata": {
        "id": "section7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í•¨ìˆ˜\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, val_acc, history, filename):\n",
        "    \"\"\"\n",
        "    ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "\n",
        "    Args:\n",
        "        filename: ì €ì¥í•  íŒŒì¼ëª…\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'val_acc': val_acc,\n",
        "        'history': history\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "    print(f'ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filename} (Val Acc: {val_acc:.2f}%)')\n",
        "\n",
        "# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í•¨ìˆ˜\n",
        "def load_checkpoint(model, optimizer, scheduler, filename):\n",
        "    \"\"\"\n",
        "    ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\n",
        "\n",
        "    Returns:\n",
        "        start_epoch: ì‹œì‘ ì—í­\n",
        "        history: í•™ìŠµ íˆìŠ¤í† ë¦¬\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(filename)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    history = checkpoint['history']\n",
        "\n",
        "    print(f'ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {filename}')\n",
        "    print(f'ì¬ì‹œì‘ ì—í­: {start_epoch}')\n",
        "    print(f'ì´ì „ ê²€ì¦ ì •í™•ë„: {checkpoint[\"val_acc\"]:.2f}%')\n",
        "\n",
        "    return start_epoch, history\n",
        "\n",
        "print('ì²´í¬í¬ì¸íŠ¸ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')"
      ],
      "metadata": {
        "id": "checkpoint"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. ì „ì²´ í•™ìŠµ ì‹¤í–‰\n",
        "\n",
        "**í•™ìŠµ í”„ë¡œì„¸ìŠ¤:**\n",
        "- Mixed Precision Training\n",
        "- OneCycleLR ìŠ¤ì¼€ì¤„ë§\n",
        "- TensorBoard ì‹¤ì‹œê°„ ë¡œê¹…\n",
        "- ìµœì  ëª¨ë¸ ìë™ ì €ì¥"
      ],
      "metadata": {
        "id": "section8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "# í•™ìŠµ ì‹œì‘\n",
        "print('\\n' + '=' * 60)\n",
        "print('í•™ìŠµ ì‹œì‘')\n",
        "print('=' * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'\\nì—í­ [{epoch+1}/{num_epochs}]')\n",
        "    print('-' * 60)\n",
        "\n",
        "    # í›ˆë ¨\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, criterion, optimizer, scheduler, scaler, device, epoch\n",
        "    )\n",
        "\n",
        "    # ê²€ì¦\n",
        "    val_loss, val_acc = validate(\n",
        "        model, val_loader, criterion, device, epoch\n",
        "    )\n",
        "\n",
        "    # íˆìŠ¤í† ë¦¬ ì €ì¥\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    # TensorBoard ë¡œê¹… (ì—í­ ë‹¨ìœ„)\n",
        "    writer.add_scalars('Loss', {\n",
        "        'train': train_loss,\n",
        "        'val': val_loss\n",
        "    }, epoch)\n",
        "\n",
        "    writer.add_scalars('Accuracy', {\n",
        "        'train': train_acc,\n",
        "        'val': val_acc\n",
        "    }, epoch)\n",
        "\n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    print(f'\\nì—í­ {epoch+1} ê²°ê³¼:')\n",
        "    print(f'  í›ˆë ¨ ì†ì‹¤: {train_loss:.4f} | í›ˆë ¨ ì •í™•ë„: {train_acc:.2f}%')\n",
        "    print(f'  ê²€ì¦ ì†ì‹¤: {val_loss:.4f} | ê²€ì¦ ì •í™•ë„: {val_acc:.2f}%')\n",
        "\n",
        "    # ìµœì  ëª¨ë¸ ì €ì¥\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_epoch = epoch + 1\n",
        "        save_checkpoint(\n",
        "            model, optimizer, scheduler, epoch, val_acc, history,\n",
        "            'vgg19_bn_best.pth'\n",
        "        )\n",
        "\n",
        "    # ì£¼ê¸°ì ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        save_checkpoint(\n",
        "            model, optimizer, scheduler, epoch, val_acc, history,\n",
        "            f'vgg19_bn_epoch{epoch+1}.pth'\n",
        "        )\n",
        "\n",
        "# í•™ìŠµ ì¢…ë£Œ\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print('\\n' + '=' * 60)\n",
        "print('í•™ìŠµ ì™„ë£Œ')\n",
        "print('=' * 60)\n",
        "print(f'ì´ ì†Œìš” ì‹œê°„: {elapsed_time/60:.2f}ë¶„')\n",
        "print(f'ì—í­ë‹¹ í‰ê·  ì‹œê°„: {elapsed_time/num_epochs:.2f}ì´ˆ')\n",
        "print(f'ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.2f}% (ì—í­ {best_epoch})')\n",
        "print('=' * 60)\n",
        "\n",
        "# TensorBoard ì¢…ë£Œ\n",
        "writer.close()\n",
        "\n",
        "# ìµœì  ëª¨ë¸ ë¡œë“œ\n",
        "checkpoint = torch.load('vgg19_bn_best.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print('\\nìµœì  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ')"
      ],
      "metadata": {
        "id": "training_loop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Grad-CAM ì‹œê°í™”\n",
        "\n",
        "**Grad-CAM (Gradient-weighted Class Activation Mapping):**\n",
        "- ëª¨ë¸ì´ ì´ë¯¸ì§€ì˜ ì–´ëŠ ë¶€ë¶„ì„ ë³´ê³  ê²°ì •í–ˆëŠ”ì§€ ì‹œê°í™”\n",
        "- ëª¨ë¸ì˜ í•´ì„ ê°€ëŠ¥ì„±(Interpretability) í–¥ìƒ\n",
        "- ë””ë²„ê¹… ë° ëª¨ë¸ ì‹ ë¢°ë„ í™•ì¸ì— ìœ ìš©"
      ],
      "metadata": {
        "id": "section9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grad-CAM ì‹œê°í™” í•¨ìˆ˜\n",
        "def visualize_gradcam(model, image, true_label, pred_label, class_names, device):\n",
        "    \"\"\"\n",
        "    Grad-CAMì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì£¼ëª© ì˜ì—­ ì‹œê°í™”\n",
        "\n",
        "    Args:\n",
        "        model: VGG-19-BN ëª¨ë¸\n",
        "        image: ì…ë ¥ ì´ë¯¸ì§€ (ì •ê·œí™”ëœ í…ì„œ)\n",
        "        true_label: ì‹¤ì œ ë ˆì´ë¸”\n",
        "        pred_label: ì˜ˆì¸¡ ë ˆì´ë¸”\n",
        "        class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "    # VGG-19-BNì˜ ë§ˆì§€ë§‰ Conv ë ˆì´ì–´ íƒ€ê²Ÿ\n",
        "    target_layers = [model.features[-1]]\n",
        "\n",
        "    # Grad-CAM ê°ì²´ ìƒì„±\n",
        "    cam = GradCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "    # ì…ë ¥ ì´ë¯¸ì§€ ì¤€ë¹„\n",
        "    input_tensor = image.unsqueeze(0).to(device)\n",
        "    # (C, H, W) >> (B, C, H, W)\n",
        "\n",
        "    # Grad-CAM ìƒì„± (ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ì— ëŒ€í•´)\n",
        "    targets = [ClassifierOutputTarget(pred_label)]\n",
        "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "    grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "    # ì´ë¯¸ì§€ ì—­ì •ê·œí™”\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    # ì±„ë„ë³„ë¡œ í‰ê· , í‘œì¤€í¸ì°¨ ê°ê° ì ìš©í•´ì¤˜\n",
        "\n",
        "    # ì—­ì •ê·œí™”\n",
        "    img_denorm = image * std + mean\n",
        "    img_denorm = torch.clamp(img_denorm, 0, 1)\n",
        "    rgb_img = img_denorm.permute(1, 2, 0).cpu().numpy()\n",
        "    # permute(1, 2, 0) C, H, W >> H, W, C\n",
        "\n",
        "    # Grad-CAM ì˜¤ë²„ë ˆì´\n",
        "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    # ì‹œê°í™”\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # ì›ë³¸ ì´ë¯¸ì§€\n",
        "    axes[0].imshow(rgb_img)\n",
        "    axes[0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Grad-CAM íˆíŠ¸ë§µ\n",
        "    axes[1].imshow(grayscale_cam, cmap='jet')\n",
        "    axes[1].set_title('Grad-CAM Heatmap', fontsize=12, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # ì˜¤ë²„ë ˆì´\n",
        "    axes[2].imshow(cam_image)\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    axes[2].set_title(\n",
        "        f'True: {class_names[true_label]}\\nPred: {class_names[pred_label]}',\n",
        "        color=color,\n",
        "        fontsize=12,\n",
        "        fontweight='bold'\n",
        "    )\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Grad-CAM ì‹¤í–‰ (ì—¬ëŸ¬ ìƒ˜í”Œ)\n",
        "def run_gradcam_on_samples(model, val_loader, class_names, device, num_samples=6):\n",
        "    \"\"\"\n",
        "    ì—¬ëŸ¬ ìƒ˜í”Œì— ëŒ€í•´ Grad-CAM ì‹œê°í™”\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°\n",
        "    dataiter = iter(val_loader)\n",
        "    images, labels = next(dataiter)\n",
        "\n",
        "    # ì˜ˆì¸¡\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images.to(device))\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # ìƒ˜í”Œë³„ Grad-CAM ìƒì„±\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        print(f'\\nìƒ˜í”Œ {i+1}:')\n",
        "        visualize_gradcam(\n",
        "            model,\n",
        "            images[i],\n",
        "            labels[i].item(),\n",
        "            predicted[i].item(),\n",
        "            class_names,\n",
        "            device\n",
        "        )\n",
        "\n",
        "# Grad-CAM ì‹¤í–‰\n",
        "print('\\nGrad-CAM ì‹œê°í™” ì‹œì‘...')\n",
        "print('=' * 60)\n",
        "run_gradcam_on_samples(model, val_loader, class_names, device, num_samples=6)"
      ],
      "metadata": {
        "id": "gradcam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. í˜¼ë™ í–‰ë ¬ ë° ìµœì¢… í‰ê°€"
      ],
      "metadata": {
        "id": "section10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "def get_all_predictions(model, loader, device):\n",
        "    \"\"\"ëª¨ë“  ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc='Predicting'):\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
        "\n",
        "# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
        "    \"\"\"í˜¼ë™ í–‰ë ¬ ì‹œê°í™” ë° ë¶„ì„\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "    # ì›ë³¸ í˜¼ë™ í–‰ë ¬\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "        ax=axes[0],\n",
        "        cbar_kws={'label': 'Count'}\n",
        "    )\n",
        "    axes[0].set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_ylabel('True', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬\n",
        "    sns.heatmap(\n",
        "        cm_normalized,\n",
        "        annot=True,\n",
        "        fmt='.2f',\n",
        "        cmap='Blues',\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "        ax=axes[1],\n",
        "        cbar_kws={'label': 'Proportion'},\n",
        "        vmin=0,\n",
        "        vmax=1\n",
        "    )\n",
        "    axes[1].set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_ylabel('True', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥\n",
        "    print('\\ní´ë˜ìŠ¤ë³„ ì„±ëŠ¥:')\n",
        "    print('=' * 70)\n",
        "    print(f'{\"Class\":15s} {\"Precision\":>10s} {\"Recall\":>10s} {\"F1-Score\":>10s} {\"Support\":>10s}')\n",
        "    print('-' * 70)\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        precision = cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0\n",
        "        recall = cm[i, i] / cm[i, :].sum() if cm[i, :].sum() > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        support = cm[i, :].sum()\n",
        "\n",
        "        print(f'{name:15s} {precision:10.4f} {recall:10.4f} {f1:10.4f} {support:10d}')\n",
        "\n",
        "    print('=' * 70)\n",
        "\n",
        "# ìµœì¢… í‰ê°€ ì‹¤í–‰\n",
        "print('\\nìµœì¢… í‰ê°€ ì‹œì‘...')\n",
        "print('=' * 60)\n",
        "\n",
        "y_pred, y_true, y_probs = get_all_predictions(model, val_loader, device)\n",
        "\n",
        "# ì „ì²´ ì •í™•ë„\n",
        "accuracy = 100 * (y_pred == y_true).sum() / len(y_true)\n",
        "print(f'\\nìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.2f}%')\n",
        "\n",
        "# í˜¼ë™ í–‰ë ¬\n",
        "plot_confusion_matrix(y_true, y_pred, class_names)\n",
        "\n",
        "# ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
        "print('\\n\\nìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:')\n",
        "print('=' * 60)\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))"
      ],
      "metadata": {
        "id": "evaluation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. í•™ìŠµ ê³¡ì„  ì‹œê°í™”"
      ],
      "metadata": {
        "id": "section11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
        "def plot_training_history(history):\n",
        "    \"\"\"í•™ìŠµ íˆìŠ¤í† ë¦¬ ì‹œê°í™”\"\"\"\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # ì†ì‹¤ ê³¡ì„ \n",
        "    axes[0].plot(epochs, history['train_loss'], 'b-o', label='Train Loss', linewidth=2, markersize=6)\n",
        "    axes[0].plot(epochs, history['val_loss'], 'r-s', label='Val Loss', linewidth=2, markersize=6)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
        "    axes[0].set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
        "    axes[0].set_title('Loss Curve', fontsize=15, fontweight='bold')\n",
        "    axes[0].legend(fontsize=11, loc='upper right')\n",
        "    axes[0].grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "    # ì •í™•ë„ ê³¡ì„ \n",
        "    axes[1].plot(epochs, history['train_acc'], 'b-o', label='Train Acc', linewidth=2, markersize=6)\n",
        "    axes[1].plot(epochs, history['val_acc'], 'r-s', label='Val Acc', linewidth=2, markersize=6)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
        "    axes[1].set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
        "    axes[1].set_title('Accuracy Curve', fontsize=15, fontweight='bold')\n",
        "    axes[1].legend(fontsize=11, loc='lower right')\n",
        "    axes[1].grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ìµœì¢… í†µê³„\n",
        "    print('\\nìµœì¢… í•™ìŠµ í†µê³„:')\n",
        "    print('=' * 60)\n",
        "    print(f'ìµœì¢… í›ˆë ¨ ì†ì‹¤: {history[\"train_loss\"][-1]:.4f}')\n",
        "    print(f'ìµœì¢… í›ˆë ¨ ì •í™•ë„: {history[\"train_acc\"][-1]:.2f}%')\n",
        "    print(f'ìµœì¢… ê²€ì¦ ì†ì‹¤: {history[\"val_loss\"][-1]:.4f}')\n",
        "    print(f'ìµœì¢… ê²€ì¦ ì •í™•ë„: {history[\"val_acc\"][-1]:.2f}%')\n",
        "    print(f'\\nìµœê³  ê²€ì¦ ì •í™•ë„: {max(history[\"val_acc\"]):.2f}%')\n",
        "    print(f'ìµœì € ê²€ì¦ ì†ì‹¤: {min(history[\"val_loss\"]):.4f}')\n",
        "    print('=' * 60)\n",
        "\n",
        "# ì‹œê°í™” ì‹¤í–‰\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "visualization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. ì‹¤ìŠµ ì •ë¦¬ ë° í•µì‹¬ ìš”ì•½\n",
        "\n",
        "### ğŸ¯ ì´ë²ˆ ì‹¤ìŠµì—ì„œ ë°°ìš´ ê³ ê¸‰ ê¸°ë²•\n",
        "\n",
        "#### 1. **Mixed Precision Training**\n",
        "```python\n",
        "# FP16ê³¼ FP32ë¥¼ í˜¼í•©í•˜ì—¬ í•™ìŠµ ì†ë„ í–¥ìƒ\n",
        "with autocast():\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "scaler.scale(loss).backward()\n",
        "scaler.step(optimizer)\n",
        "scaler.update()\n",
        "```\n",
        "- **ì¥ì **: í•™ìŠµ ì†ë„ 2-3ë°° í–¥ìƒ, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ\n",
        "- **ë‹¨ì **: ì¼ë¶€ ëª¨ë¸ì—ì„œ ì •í™•ë„ ì•½ê°„ í•˜ë½ ê°€ëŠ¥\n",
        "\n",
        "#### 2. **OneCycleLR ìŠ¤ì¼€ì¤„ëŸ¬**\n",
        "```python\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=lr * 10,\n",
        "    epochs=num_epochs,\n",
        "    steps_per_epoch=len(train_loader)\n",
        ")\n",
        "```\n",
        "- í•™ìŠµë¥ ì„ ì‚¼ê°íŒŒ í˜•íƒœë¡œ ë³€í™”\n",
        "- ì´ˆë°˜: ë¹ ë¥¸ í•™ìŠµ, í›„ë°˜: ì„¸ë°€í•œ ì¡°ì •\n",
        "\n",
        "#### 3. **Grad-CAM ì‹œê°í™”**\n",
        "- ëª¨ë¸ì´ ì£¼ëª©í•˜ëŠ” ì˜ì—­ í™•ì¸\n",
        "- ë””ë²„ê¹… ë° ëª¨ë¸ ì‹ ë¢°ë„ ê²€ì¦\n",
        "- ì˜¤ë¶„ë¥˜ ì›ì¸ ë¶„ì„\n",
        "\n",
        "#### 4. **TensorBoard ëª¨ë‹ˆí„°ë§**\n",
        "- ì‹¤ì‹œê°„ í•™ìŠµ ê³¡ì„  í™•ì¸\n",
        "- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¹„êµ\n",
        "- ëª¨ë¸ ê·¸ë˜í”„ ì‹œê°í™”\n",
        "\n",
        "#### 5. **ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ**\n",
        "- í•™ìŠµ ì¤‘ë‹¨ ì‹œ ì¬ê°œ ê°€ëŠ¥\n",
        "- ìµœì  ëª¨ë¸ ìë™ ì €ì¥\n",
        "- íˆìŠ¤í† ë¦¬ ë³´ì¡´\n",
        "\n",
        "### ğŸ’¡ ì‹¤ë¬´ ì ìš© íŒ\n",
        "\n",
        "1. **ë°ì´í„° ì¦ê°• ê°•ë„ ì¡°ì ˆ**\n",
        "   - ë°ì´í„°ê°€ ì ì„ìˆ˜ë¡ ê°•í•œ ì¦ê°•\n",
        "   - ê²€ì¦ ë°ì´í„°ì—ëŠ” ì¦ê°• ì ìš© ì•ˆ í•¨\n",
        "\n",
        "2. **í•™ìŠµë¥  ì°¾ê¸°**\n",
        "   - Learning Rate Finder ì‚¬ìš©\n",
        "   - ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ ëŠë¦° ìˆ˜ë ´\n",
        "\n",
        "3. **ë°°ì¹˜ í¬ê¸° ì„ íƒ**\n",
        "   - GPU ë©”ëª¨ë¦¬ í—ˆìš© ë²”ìœ„ ë‚´ ìµœëŒ€í™”\n",
        "   - ì‘ì€ ë°°ì¹˜: ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ, í•™ìŠµ ë¶ˆì•ˆì •\n",
        "   - í° ë°°ì¹˜: ì•ˆì •ì  í•™ìŠµ, ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜\n",
        "\n",
        "4. **ê³¼ì í•© ë°©ì§€**\n",
        "   - Early Stopping\n",
        "   - Dropout, Batch Normalization\n",
        "   - L2 Regularization (Weight Decay)\n",
        "   - Data Augmentation\n",
        "\n",
        "### ğŸš€ ì¶”ê°€ ê°œì„  ë°©í–¥\n",
        "\n",
        "- **ì•™ìƒë¸”**: ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°í•©\n",
        "- **TTA (Test Time Augmentation)**: í…ŒìŠ¤íŠ¸ ì‹œ ì¦ê°•\n",
        "- **Knowledge Distillation**: í° ëª¨ë¸ â†’ ì‘ì€ ëª¨ë¸\n",
        "- **AutoML**: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íƒìƒ‰\n",
        "\n",
        "### ğŸ“Š ì„±ëŠ¥ ë¹„êµ\n",
        "\n",
        "| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | í•™ìŠµ ì‹œê°„ | ìµœì¢… ì •í™•ë„ |\n",
        "|------|----------|-----------|-------------|\n",
        "| ResNet-18 | 11M | ë¹ ë¦„ | ~94-95% |\n",
        "| VGG-19-BN | 144M | ëŠë¦¼ | ~95-96% |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
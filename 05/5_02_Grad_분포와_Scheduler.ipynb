{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "before .item()\n",
        "\n",
        "tensor([[0.1,0.2,.......],\n",
        "        [0.1,0.2,.......]\n",
        "])\n",
        "\n",
        "shape(256,10)\n",
        ">> detach(), abs(): 음수가 있다면 다 양수로 변환, mean(): 차원축소(**)\n",
        ">> mean()으로 인해 차원축소가 됨 (256*10=2,560) tensor(0.0034....) 이런 식으로 출력\n",
        "\n",
        "after .item() : tensor >> python 숫자로 변환\n",
        "0.0034 이렇게 바뀌어요\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "ZnlJcjAizGSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRFkQMrCdI-n"
      },
      "outputs": [],
      "source": [
        "# 5차시 보강 실습: Grad 분포 & Scheduler (Colab 호환)\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tfm = transforms.Compose([transforms.ToTensor()])\n",
        "train_ds = datasets.MNIST(root='/tmp/mnist', train=True, download=True, transform=tfm)\n",
        "test_ds  = datasets.MNIST(root='/tmp/mnist', train=False, download=True, transform=tfm)\n",
        "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# MLP: multi layer perceptron 다층 퍼셉트론\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Flatten(), # 1차원 배열로 변형\n",
        "            nn.Linear(28*28, 512), nn.ReLU(),\n",
        "            nn.Linear(512, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "        # He 초기화 (ReLU에 적합)\n",
        "        for m in self.f:\n",
        "            if isinstance(m, nn.Linear):  # 선형회귀 레이어(층) 있다면,\n",
        "                nn.init.kaiming_normal_(m.weight) # He 초기화\n",
        "                nn.init.zeros_(m.bias)            # bias 편향 0으로 초기화\n",
        "    def forward(self, x): return self.f(x)\n",
        "\n",
        "model = MLP().to(device)\n",
        "opt = optim.AdamW(model.parameters(), lr=3e-3)\n",
        "# AdamW : adam(adaptive memotum) + weight decay(가중치 감쇠)\n",
        "sched = optim.lr_scheduler.OneCycleLR(opt, max_lr=3e-3, steps_per_epoch=len(train_loader), epochs=5)\n",
        "# OneCycleL : 초반에 학습을 빠르게, 점점 세밀하게 조정\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "# Grad 히스토그램 수집용(기울기/weight가중치 모니터링)\n",
        "grads = []\n",
        "def hook_fn(m, gi, go):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        if m.weight.grad is not None:\n",
        "            grads.append(m.weight.grad.detach().abs().mean().item())\n",
        "            # detach: 계산 그래프에서 분리(메모리 절약)\n",
        "            # abs().mean(): 절대값의 평균\n",
        "            # item() : 텐서 >> 파이썬의 숫자로 변환(**)\n",
        "\n",
        "hooks = [m.register_full_backward_hook(hook_fn) for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "\n",
        "def train_epoch():\n",
        "    model.train(); tot=0; correct=0\n",
        "    for x,y in train_loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        out = model(x); loss = crit(out, y)\n",
        "        loss.backward(); opt.step(); sched.step()\n",
        "        tot += y.size(0); correct += (out.argmax(1)==y).sum().item()\n",
        "    return loss.item(), correct/tot\n",
        "    # out.argmax(1): 각 샘플의 최대 확률이 있는 클래스\n",
        "\n",
        "def eval_epoch():\n",
        "    model.eval(); tot=0; correct=0\n",
        "    with torch.no_grad():\n",
        "        for x,y in test_loader:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            tot += y.size(0); correct += (out.argmax(1)==y).sum().item()\n",
        "    return correct/tot\n",
        "\n",
        "hist_grad = []  # 한 번 학습할 때(epoch) gradient(기울기/가중치)\n",
        "hist_acc  = []  # 정확도\n",
        "for ep in range(5):\n",
        "    loss, tr_acc = train_epoch()  # train_epoch[0] = loss 값\n",
        "    acc = eval_epoch()\n",
        "    hist_grad.append(sum(grads[-len(train_loader):])/max(1,len(train_loader)))\n",
        "    # grads[-len(train_loader):] 마지막 epoch 의 gradient 기울기(가중치) 만 사용\n",
        "    hist_acc.append(acc)\n",
        "    print(f\"EP{ep+1} loss={loss:.3f} test_acc={acc:.3f}\")\n",
        "\n",
        "plt.figure(); plt.plot(hist_grad); plt.title('Average |grad|'); plt.show()\n",
        "plt.figure(); plt.plot(hist_acc); plt.title('Test ACC'); plt.show()\n",
        "\n",
        "for h in hooks: h.remove()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r31QsnwgdKJp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
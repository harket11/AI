{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxS7OqMPkDgI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Part 5: BatchNorm과 Gradient 안정성\n",
        "- Minibatch 크기의 영향\n",
        "- Batch Normalization의 효과\n",
        "- Gradient 안정성 비교\n",
        "\n",
        "독립적으로 실행 가능합니다.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 재현성을 위한 시드 고정\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Part 5: BatchNorm과 Gradient 안정성\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 실습 5-1: Batch Size의 영향 관찰\n",
        "# =====================================================================\n",
        "print(\"\\n[실습 5-1] Batch Size가 학습에 미치는 영향\")\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    \"\"\"BatchNorm 없는 기본 신경망\"\"\"\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(20, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# 데이터셋 준비\n",
        "print(\"\\n데이터셋 생성 중...\")\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=15,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train_t = torch.FloatTensor(X_train)\n",
        "y_train_t = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "\n",
        "print(f\"훈련 데이터: {X_train.shape}\")\n",
        "\n",
        "# 다양한 Batch Size로 학습\n",
        "batch_sizes = [8, 32, 128, 512]\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "batch_size_results = []\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    print(f\"\\nBatch Size {batch_size} 학습 중...\")\n",
        "\n",
        "    # DataLoader 생성\n",
        "    dataset = TensorDataset(X_train_t, y_train_t)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = SimpleNet()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # 학습\n",
        "    losses = []\n",
        "    num_epochs = 30\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(batch_X)\n",
        "            loss = criterion(output, batch_y)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "    batch_size_results.append((batch_size, losses))\n",
        "    print(f\"  최종 손실: {losses[-1]:.4f}\")\n",
        "\n",
        "# 시각화\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for batch_size, losses in batch_size_results:\n",
        "    plt.plot(losses, label=f'Batch={batch_size}', linewidth=2)\n",
        "\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Training Loss', fontsize=12)\n",
        "plt.title('Effect of Batch Size', fontsize=14, weight='bold')\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "final_losses = [losses[-1] for _, losses in batch_size_results]\n",
        "colors = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']\n",
        "bars = plt.bar([str(bs) for bs in batch_sizes], final_losses,\n",
        "               color=colors, edgecolor='black', alpha=0.7)\n",
        "\n",
        "plt.xlabel('Batch Size', fontsize=12)\n",
        "plt.ylabel('Final Loss', fontsize=12)\n",
        "plt.title('Final Loss by Batch Size', fontsize=14, weight='bold')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 값 표시\n",
        "for bar, loss in zip(bars, final_losses):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{loss:.4f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('part5_batch_size_effect.png', dpi=150, bbox_inches='tight')\n",
        "print(\"\\n저장: part5_batch_size_effect.png\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 실습 5-2: BatchNorm 없는 네트워크 vs BatchNorm 있는 네트워크\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[실습 5-2] BatchNorm의 효과 비교\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "class NetWithoutBN(nn.Module):\n",
        "    \"\"\"BatchNorm 없는 깊은 신경망\"\"\"\n",
        "    def __init__(self):\n",
        "        super(NetWithoutBN, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(20, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class NetWithBN(nn.Module):\n",
        "    \"\"\"BatchNorm 있는 깊은 신경망\"\"\"\n",
        "    def __init__(self):\n",
        "        super(NetWithBN, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(20, 64),\n",
        "            nn.BatchNorm1d(64),  # BatchNorm 추가\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "print(\"\\nBatchNorm 원리:\")\n",
        "print(\"  1. 각 배치의 평균과 분산 계산\")\n",
        "print(\"  2. 정규화: (x - 평균) / sqrt(분산 + epsilon)\")\n",
        "print(\"  3. 스케일 및 이동: gamma * 정규화값 + beta\")\n",
        "print(\"  4. gamma, beta는 학습 가능한 파라미터\")\n",
        "\n",
        "# 두 모델 학습\n",
        "models_to_compare = [\n",
        "    ('Without BatchNorm', NetWithoutBN()),\n",
        "    ('With BatchNorm', NetWithBN())\n",
        "]\n",
        "\n",
        "comparison_results = []\n",
        "\n",
        "# DataLoader 생성 (Batch Size 32)\n",
        "dataset = TensorDataset(X_train_t, y_train_t)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "for model_name, model in models_to_compare:\n",
        "    print(f\"\\n{model_name} 학습 중...\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    losses = []\n",
        "    num_epochs = 50\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(batch_X)\n",
        "            loss = criterion(output, batch_y)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"  Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    comparison_results.append((model_name, losses))\n",
        "\n",
        "# 시각화\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for model_name, losses in comparison_results:\n",
        "    plt.plot(losses, label=model_name, linewidth=2)\n",
        "\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Training Loss', fontsize=12)\n",
        "plt.title('BatchNorm Effect on Training', fontsize=14, weight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "for model_name, losses in comparison_results:\n",
        "    plt.plot(losses, label=model_name, linewidth=2)\n",
        "\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Training Loss (log scale)', fontsize=12)\n",
        "plt.title('BatchNorm Effect (Log Scale)', fontsize=14, weight='bold')\n",
        "plt.yscale('log')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('part5_batchnorm_effect.png', dpi=150, bbox_inches='tight')\n",
        "print(\"\\n저장: part5_batchnorm_effect.png\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 실습 5-3: BatchNorm의 Gradient 안정화 효과\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[실습 5-3] BatchNorm의 Gradient 안정화 효과\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 그래디언트 수집\n",
        "gradient_history_without_bn = []\n",
        "gradient_history_with_bn = []\n",
        "\n",
        "# 새 모델 생성\n",
        "model_without_bn = NetWithoutBN()\n",
        "model_with_bn = NetWithBN()\n",
        "\n",
        "# 그래디언트 기록 Hook\n",
        "def create_gradient_hook(gradient_list):\n",
        "    \"\"\"그래디언트를 리스트에 저장하는 Hook 생성\"\"\"\n",
        "    def hook(grad):\n",
        "        gradient_list.append(grad.abs().mean().item())\n",
        "        return grad\n",
        "    return hook\n",
        "\n",
        "# Hook 등록\n",
        "print(\"\\nHook 등록 중...\")\n",
        "\n",
        "# Without BN\n",
        "for name, param in model_without_bn.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        param.register_hook(create_gradient_hook(gradient_history_without_bn))\n",
        "\n",
        "# With BN\n",
        "for name, param in model_with_bn.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        param.register_hook(create_gradient_hook(gradient_history_with_bn))\n",
        "\n",
        "# 한 번의 Forward/Backward\n",
        "print(\"\\nForward/Backward 수행 중...\")\n",
        "\n",
        "batch_X = X_train_t[:32]  # 배치 크기 32\n",
        "batch_y = y_train_t[:32]\n",
        "\n",
        "# Without BN\n",
        "output_without = model_without_bn(batch_X)\n",
        "loss_without = criterion(output_without, batch_y)\n",
        "loss_without.backward()\n",
        "\n",
        "# With BN\n",
        "output_with = model_with_bn(batch_X)\n",
        "loss_with = criterion(output_with, batch_y)\n",
        "loss_with.backward()\n",
        "\n",
        "# 그래디언트 역순 정렬 (Layer 1부터 표시)\n",
        "gradient_history_without_bn.reverse()\n",
        "gradient_history_with_bn.reverse()\n",
        "\n",
        "print(\"\\n각 층의 그래디언트 크기:\")\n",
        "print(\"\\nWithout BatchNorm:\")\n",
        "for i, grad in enumerate(gradient_history_without_bn):\n",
        "    print(f\"  Layer {i+1}: {grad:.6f}\")\n",
        "\n",
        "print(\"\\nWith BatchNorm:\")\n",
        "for i, grad in enumerate(gradient_history_with_bn):\n",
        "    print(f\"  Layer {i+1}: {grad:.6f}\")\n",
        "\n",
        "# 시각화\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Without BN\n",
        "ax1 = axes[0]\n",
        "layers = list(range(1, len(gradient_history_without_bn) + 1))\n",
        "ax1.bar(layers, gradient_history_without_bn, color='#e74c3c',\n",
        "        edgecolor='black', alpha=0.7)\n",
        "ax1.set_xlabel('Layer Number', fontsize=11)\n",
        "ax1.set_ylabel('Gradient Magnitude', fontsize=11)\n",
        "ax1.set_title('Without BatchNorm', fontsize=12, weight='bold')\n",
        "ax1.set_xticks(layers)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# With BN\n",
        "ax2 = axes[1]\n",
        "layers = list(range(1, len(gradient_history_with_bn) + 1))\n",
        "ax2.bar(layers, gradient_history_with_bn, color='#2ecc71',\n",
        "        edgecolor='black', alpha=0.7)\n",
        "ax2.set_xlabel('Layer Number', fontsize=11)\n",
        "ax2.set_ylabel('Gradient Magnitude', fontsize=11)\n",
        "ax2.set_title('With BatchNorm', fontsize=12, weight='bold')\n",
        "ax2.set_xticks(layers)\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('part5_gradient_stability.png', dpi=150, bbox_inches='tight')\n",
        "print(\"\\n저장: part5_gradient_stability.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"\\n분석:\")\n",
        "print(\"  - BatchNorm 없음: 층마다 그래디언트 크기 차이가 큼\")\n",
        "print(\"  - BatchNorm 있음: 모든 층에서 그래디언트가 비슷한 크기\")\n",
        "print(\"  - 결과: 더 안정적인 학습 가능\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 실습 5-4: 다양한 Batch Size에서 BatchNorm 효과\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[실습 5-4] 다양한 Batch Size에서 BatchNorm 효과\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "batch_sizes_test = [8, 16, 32, 64]\n",
        "\n",
        "results_comparison = {}\n",
        "\n",
        "for batch_size in batch_sizes_test:\n",
        "    print(f\"\\nBatch Size {batch_size} 테스트 중...\")\n",
        "\n",
        "    dataset = TensorDataset(X_train_t, y_train_t)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Without BN\n",
        "    model_no_bn = NetWithoutBN()\n",
        "    optimizer_no_bn = optim.Adam(model_no_bn.parameters(), lr=0.001)\n",
        "\n",
        "    losses_no_bn = []\n",
        "    for epoch in range(30):\n",
        "        epoch_loss = 0\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            model_no_bn.train()\n",
        "            optimizer_no_bn.zero_grad()\n",
        "            output = model_no_bn(batch_X)\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer_no_bn.step()\n",
        "            epoch_loss += loss.item()\n",
        "        losses_no_bn.append(epoch_loss / len(dataloader))\n",
        "\n",
        "    # With BN\n",
        "    model_with_bn = NetWithBN()\n",
        "    optimizer_with_bn = optim.Adam(model_with_bn.parameters(), lr=0.001)\n",
        "\n",
        "    losses_with_bn = []\n",
        "    for epoch in range(30):\n",
        "        epoch_loss = 0\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            model_with_bn.train()\n",
        "            optimizer_with_bn.zero_grad()\n",
        "            output = model_with_bn(batch_X)\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer_with_bn.step()\n",
        "            epoch_loss += loss.item()\n",
        "        losses_with_bn.append(epoch_loss / len(dataloader))\n",
        "\n",
        "    results_comparison[batch_size] = {\n",
        "        'without_bn': losses_no_bn,\n",
        "        'with_bn': losses_with_bn\n",
        "    }\n",
        "\n",
        "    print(f\"  Without BN 최종 손실: {losses_no_bn[-1]:.4f}\")\n",
        "    print(f\"  With BN 최종 손실: {losses_with_bn[-1]:.4f}\")\n",
        "\n",
        "# 시각화\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "for idx, batch_size in enumerate(batch_sizes_test):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "\n",
        "    losses_no_bn = results_comparison[batch_size]['without_bn']\n",
        "    losses_with_bn = results_comparison[batch_size]['with_bn']\n",
        "\n",
        "    ax.plot(losses_no_bn, label='Without BN', linewidth=2, color='#e74c3c')\n",
        "    ax.plot(losses_with_bn, label='With BN', linewidth=2, color='#2ecc71')\n",
        "\n",
        "    ax.set_xlabel('Epoch', fontsize=11)\n",
        "    ax.set_ylabel('Training Loss', fontsize=11)\n",
        "    ax.set_title(f'Batch Size = {batch_size}', fontsize=12, weight='bold')\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('part5_batchnorm_various_batch_sizes.png', dpi=150, bbox_inches='tight')\n",
        "print(\"\\n저장: part5_batchnorm_various_batch_sizes.png\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 실습 5-5: BatchNorm 학습 모드 vs 평가 모드\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[실습 5-5] BatchNorm의 학습/평가 모드 차이\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# BatchNorm이 있는 모델 생성 및 학습\n",
        "model_bn = NetWithBN()\n",
        "optimizer = optim.Adam(model_bn.parameters(), lr=0.001)\n",
        "\n",
        "dataset = TensorDataset(X_train_t, y_train_t)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"\\n모델 학습 중...\")\n",
        "for epoch in range(30):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        model_bn.train()  # 학습 모드\n",
        "        optimizer.zero_grad()\n",
        "        output = model_bn(batch_X)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "print(\"학습 완료\")\n",
        "\n",
        "# 테스트 데이터\n",
        "X_test_t = torch.FloatTensor(X_test)\n",
        "\n",
        "print(\"\\n학습 모드 vs 평가 모드 출력 비교:\")\n",
        "\n",
        "# 학습 모드에서 여러 번 예측\n",
        "model_bn.train()\n",
        "predictions_train_mode = []\n",
        "for _ in range(5):\n",
        "    with torch.no_grad():\n",
        "        pred = model_bn(X_test_t[:10])\n",
        "        predictions_train_mode.append(pred.numpy())\n",
        "\n",
        "# 평가 모드에서 여러 번 예측\n",
        "model_bn.eval()\n",
        "predictions_eval_mode = []\n",
        "for _ in range(5):\n",
        "    with torch.no_grad():\n",
        "        pred = model_bn(X_test_t[:10])\n",
        "        predictions_eval_mode.append(pred.numpy())\n",
        "\n",
        "# 예측 분산 계산\n",
        "variance_train = np.var(predictions_train_mode, axis=0).mean()\n",
        "variance_eval = np.var(predictions_eval_mode, axis=0).mean()\n",
        "\n",
        "print(f\"\\n학습 모드 예측 분산: {variance_train:.8f}\")\n",
        "print(f\"평가 모드 예측 분산: {variance_eval:.8f}\")\n",
        "\n",
        "print(\"\\n설명:\")\n",
        "print(\"  - 학습 모드: 각 배치의 통계 사용 -> 예측이 약간 다름\")\n",
        "print(\"  - 평가 모드: 전체 데이터의 Moving Average 사용 -> 일관된 예측\")\n",
        "print(\"  - 중요: model.eval()을 반드시 사용해야 함!\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 최종 요약\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Part 5 완료\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n핵심 개념:\")\n",
        "print(\"\\n1. Batch Size의 영향\")\n",
        "print(\"   - 작은 배치: 노이즈 많음, 정규화 효과\")\n",
        "print(\"   - 큰 배치: 안정적, GPU 활용 좋음\")\n",
        "print(\"   - 권장: 32-128 사이\")\n",
        "\n",
        "print(\"\\n2. Batch Normalization\")\n",
        "print(\"   - 목적: 각 층의 입력 분포 안정화\")\n",
        "print(\"   - 효과: 빠른 학습, 높은 학습률 가능\")\n",
        "print(\"   - 위치: Linear/Conv -> BN -> Activation\")\n",
        "\n",
        "print(\"\\n3. Gradient 안정화\")\n",
        "print(\"   - BN 없음: 층마다 그래디언트 크기 차이\")\n",
        "print(\"   - BN 있음: 모든 층에서 균일한 그래디언트\")\n",
        "print(\"   - 결과: 깊은 네트워크 학습 가능\")\n",
        "\n",
        "print(\"\\n4. 학습/평가 모드\")\n",
        "print(\"   - 학습: 현재 배치 통계 사용\")\n",
        "print(\"   - 평가: Moving Average 사용\")\n",
        "print(\"   - 필수: model.train() / model.eval() 구분\")\n",
        "\n",
        "print(\"\\n생성된 파일:\")\n",
        "print(\"  1. part5_batch_size_effect.png - Batch Size 영향\")\n",
        "print(\"  2. part5_batchnorm_effect.png - BatchNorm 효과\")\n",
        "print(\"  3. part5_gradient_stability.png - Gradient 안정성\")\n",
        "print(\"  4. part5_batchnorm_various_batch_sizes.png - 다양한 배치에서 BN 효과\")\n",
        "\n",
        "print(\"\\n5차시 실습 완료!\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ]
}
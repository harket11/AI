{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKemtmfGK01N"
      },
      "source": [
        "# 14ì°¨ì‹œ: Transfer Learning ì‹¤í—˜ ì‹¤ìŠµ\n",
        "\n",
        "##  ì‹¤ìŠµ ê°œìš”\n",
        "ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ ì§ì ‘ ì‹¤í—˜í•´ë´…ë‹ˆë‹¤:\n",
        "1. **ì¸µë³„ ë™ê²° ì „ëµ ë¹„êµ**: Full Freeze vs Partial Fine-tuning vs Full Fine-tuning\n",
        "2. **ì°¨ë“± í•™ìŠµë¥ (Differential LR)** ì ìš©\n",
        "3. **ë°ì´í„° ì¦ê°• ê°•ë„**ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”\n",
        "4. **ì‘ì€ ë°ì´í„°ì…‹**ì„ ìœ„í•œ ìµœì  í”„ë¡œí† ì½œ\n",
        "\n",
        "---\n",
        "\n",
        "**ë°ì´í„°ì…‹**: CIFAR-10 (ìƒ˜í”Œë§í•˜ì—¬ ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹¤í—˜)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y fonts-nanum* | tail -n 1\n",
        "!sudo fc-cache -fv\n",
        "!rm -rf ~/.cache/matplotlib"
      ],
      "metadata": {
        "id": "HRnVUkCqM_JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "\n",
        "!pip install torchviz | tail -n 1\n",
        "!pip install torchinfo | tail -n 1"
      ],
      "metadata": {
        "id": "nLNCOx12M_25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘í•˜ì„¸ìš”"
      ],
      "metadata": {
        "id": "5tFHuFIQNCrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib\n",
        "\n",
        "# ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ê²½ë¡œ ì„¤ì •\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)\n",
        "\n",
        "# matplotlib ê¸°ë³¸ í°íŠ¸ë¡œ ì§€ì •\n",
        "matplotlib.rc('font', family='NanumGothic')\n",
        "\n",
        "# ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ:\", matplotlib.rcParams['font.family'])"
      ],
      "metadata": {
        "id": "Bvg6xGkRNCME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spewWZ6eK01O"
      },
      "source": [
        "---\n",
        "## Section 1: í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWkxPQTlK01P"
      },
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì • (ì½”ë©ì—ì„œ í•œê¸€ ê¹¨ì§ ë°©ì§€)\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}')\n",
        "\n",
        "# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDxugI_SK01P"
      },
      "source": [
        "---\n",
        "## Section 2: ë°ì´í„°ì…‹ ì¤€ë¹„\n",
        "\n",
        "CIFAR-10 ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ , ì‘ì€ ë°ì´í„°ì…‹ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ìƒ˜í”Œë§í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aobgk1e7K01P"
      },
      "outputs": [],
      "source": [
        "# ê¸°ë³¸ ì „ì²˜ë¦¬ (ì¦ê°• ì—†ìŒ)\n",
        "transform_basic = transforms.Compose([\n",
        "    transforms.Resize(224),  # ResNet ì…ë ¥ í¬ê¸°ì— ë§ì¶¤\n",
        "    transforms.ToTensor(),  # í…ì„œë¡œ ë³€í™˜\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet í‰ê· \n",
        "                        std=[0.229, 0.224, 0.225])   # ImageNet í‘œì¤€í¸ì°¨\n",
        "])\n",
        "\n",
        "# CIFAR-10 ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                      download=True, transform=transform_basic)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                download=True, transform=transform_basic)\n",
        "\n",
        "# í´ë˜ìŠ¤ ì´ë¦„\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(f'ì „ì²´ í•™ìŠµ ë°ì´í„°: {len(full_train_dataset)}ê°œ')\n",
        "print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)}ê°œ')\n",
        "print(f'í´ë˜ìŠ¤ ìˆ˜: {num_classes}ê°œ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy897dE9K01Q"
      },
      "outputs": [],
      "source": [
        "# ì‘ì€ ë°ì´í„°ì…‹ ì‹œë‚˜ë¦¬ì˜¤: í´ë˜ìŠ¤ë‹¹ 50ê°œì”©ë§Œ ì‚¬ìš© (ì´ 500ê°œ)\n",
        "samples_per_class = 50  # í´ë˜ìŠ¤ë‹¹ ìƒ˜í”Œ ìˆ˜\n",
        "\n",
        "# ê° í´ë˜ìŠ¤ë³„ë¡œ ì¸ë±ìŠ¤ ìˆ˜ì§‘\n",
        "class_indices = {i: [] for i in range(num_classes)}  # í´ë˜ìŠ¤ë³„ ì¸ë±ìŠ¤ ì €ì¥ ë”•ì…”ë„ˆë¦¬\n",
        "\n",
        "# ì „ì²´ ë°ì´í„°ì…‹ì„ ìˆœíšŒí•˜ë©° í´ë˜ìŠ¤ë³„ë¡œ ì¸ë±ìŠ¤ ë¶„ë¥˜\n",
        "for idx, (_, label) in enumerate(full_train_dataset):\n",
        "    if len(class_indices[label]) < samples_per_class:  # í´ë˜ìŠ¤ë‹¹ ìƒ˜í”Œ ìˆ˜ ì œí•œ\n",
        "        class_indices[label].append(idx)\n",
        "\n",
        "# ì„ íƒëœ ì¸ë±ìŠ¤ë“¤ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°\n",
        "selected_indices = []\n",
        "for indices in class_indices.values():\n",
        "    selected_indices.extend(indices)\n",
        "\n",
        "# ì‘ì€ ë°ì´í„°ì…‹ ìƒì„± (Subset ì‚¬ìš©)\n",
        "small_train_dataset = Subset(full_train_dataset, selected_indices)\n",
        "\n",
        "print(f'\\nì‘ì€ í•™ìŠµ ë°ì´í„°ì…‹ í¬ê¸°: {len(small_train_dataset)}ê°œ')\n",
        "print(f'í´ë˜ìŠ¤ë‹¹ ìƒ˜í”Œ ìˆ˜: {samples_per_class}ê°œ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjftukmzK01Q"
      },
      "outputs": [],
      "source": [
        "# ë°ì´í„° ë¡œë” ìƒì„±\n",
        "batch_size = 32  # ë°°ì¹˜ í¬ê¸°\n",
        "\n",
        "# í•™ìŠµìš© ë°ì´í„° ë¡œë”\n",
        "train_loader = DataLoader(small_train_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True,  # ë°ì´í„° ì„ê¸°\n",
        "                         num_workers=2)  # ë³‘ë ¬ ì²˜ë¦¬\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë”\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=2)\n",
        "\n",
        "print(f'ë°°ì¹˜ ìˆ˜ (í•™ìŠµ): {len(train_loader)}')\n",
        "print(f'ë°°ì¹˜ ìˆ˜ (í…ŒìŠ¤íŠ¸): {len(test_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJvY582YK01Q"
      },
      "outputs": [],
      "source": [
        "# ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™”\n",
        "def show_sample_images(dataset, num_images=5):\n",
        "    \"\"\"ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜\"\"\"\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        img, label = dataset[i]  # ië²ˆì§¸ ì´ë¯¸ì§€ì™€ ë ˆì´ë¸” ê°€ì ¸ì˜¤ê¸°\n",
        "\n",
        "        # ì •ê·œí™” í•´ì œ (ì‹œê°í™”ë¥¼ ìœ„í•´)\n",
        "        img = img.numpy().transpose((1, 2, 0))  # CHW -> HWC\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        img = std * img + mean  # ì •ê·œí™” ì—­ë³€í™˜\n",
        "        img = np.clip(img, 0, 1)  # 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘\n",
        "\n",
        "        # ì´ë¯¸ì§€ í‘œì‹œ\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f'{class_names[label]}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ìƒ˜í”Œ ì´ë¯¸ì§€ ì¶œë ¥\n",
        "print('ìƒ˜í”Œ ì´ë¯¸ì§€:')\n",
        "show_sample_images(small_train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXXCVUZZK01Q"
      },
      "source": [
        "---\n",
        "## Section 3: í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
        "\n",
        "ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€í•˜ëŠ” ê¸°ë³¸ í•¨ìˆ˜ë“¤ì„ ë§Œë“­ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYOu1nupK01Q"
      },
      "outputs": [],
      "source": [
        "# í•œ ì—í­ í•™ìŠµ í•¨ìˆ˜(í•œë²ˆ í›ˆë ¨ì‹œ ì–¼ë§ˆí¼ í›ˆë ¨í•  ê²ƒì¸ê°€?)\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"í•œ ì—í­ ë™ì•ˆ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
        "    model.train()  # í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜\n",
        "\n",
        "    running_loss = 0.0  # ëˆ„ì  ì†ì‹¤\n",
        "    correct = 0  # ë§ì¶˜ ê°œìˆ˜\n",
        "    total = 0  # ì „ì²´ ê°œìˆ˜\n",
        "\n",
        "    # ë°ì´í„° ë¡œë”ì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ í•™ìŠµ\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)  # GPUë¡œ ì´ë™\n",
        "        labels = labels.to(device)  # GPUë¡œ ì´ë™\n",
        "\n",
        "        optimizer.zero_grad()  # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”\n",
        "\n",
        "        outputs = model(inputs)  # ìˆœì „íŒŒ(ì˜ˆì¸¡ê°’)\n",
        "        loss = criterion(outputs, labels)  # ì†ì‹¤ ê³„ì‚°\n",
        "\n",
        "        loss.backward()  # ì—­ì „íŒŒ\n",
        "        optimizer.step()  # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
        "\n",
        "        # í†µê³„ ì—…ë°ì´íŠ¸\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        # inputs.size(0) : ë°°ì¹˜ í¬ê¸°(batch_size)\n",
        "        _, predicted = outputs.max(1)  # ì˜ˆì¸¡ê°’\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # ì—í­ í‰ê·  ê³„ì‚°\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK6pkXxSK01R"
      },
      "outputs": [],
      "source": [
        "# í‰ê°€ í•¨ìˆ˜\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
        "    model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (í‰ê°€ ì‹œì—ëŠ” ë¶ˆí•„ìš”)\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)  # ìˆœì „íŒŒë§Œ ìˆ˜í–‰\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # í†µê³„ ì—…ë°ì´íŠ¸\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # í‰ê·  ê³„ì‚°\n",
        "    eval_loss = running_loss / total\n",
        "    eval_acc = 100.0 * correct / total\n",
        "\n",
        "    return eval_loss, eval_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhGmWGaRK01R"
      },
      "source": [
        "---\n",
        "## Section 4: ì‹¤í—˜ 1 - ì¸µë³„ ë™ê²° ì „ëµ ë¹„êµ\n",
        "\n",
        "ì„¸ ê°€ì§€ ì „ëµì„ ë¹„êµí•©ë‹ˆë‹¤:\n",
        "1. **Full Freeze**: ë°±ë³¸ ì „ì²´ ê³ ì •, ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµ\n",
        "2. **Partial Fine-tuning**: ë§ˆì§€ë§‰ ë¸”ë¡ë§Œ í•´ì œ\n",
        "3. **Full Fine-tuning**: ëª¨ë“  ì¸µ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1KIIK32K01R"
      },
      "outputs": [],
      "source": [
        "# ì‹¤í—˜ ì„¤ì •\n",
        "num_epochs = 10  # ì—í­ ìˆ˜ (ë¹ ë¥¸ ì‹¤ìŠµì„ ìœ„í•´ 10ìœ¼ë¡œ ì„¤ì •)\n",
        "learning_rate = 0.001  # í•™ìŠµë¥ \n",
        "\n",
        "print(f'ì‹¤í—˜ ì„¤ì •:')\n",
        "print(f'  - ì—í­ ìˆ˜: {num_epochs}')\n",
        "print(f'  - í•™ìŠµë¥ : {learning_rate}')\n",
        "print(f'  - ë°°ì¹˜ í¬ê¸°: {batch_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBk8DPIbK01R"
      },
      "source": [
        "### ì „ëµ 1: Full Freeze (ë°±ë³¸ ì „ì²´ ê³ ì •)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMy8Af8zK01R"
      },
      "outputs": [],
      "source": [
        "print('\\n' + '='*60)\n",
        "print('ì „ëµ 1: Full Freeze - ë°±ë³¸ ì „ì²´ ê³ ì •, ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµ')\n",
        "print('='*60)\n",
        "\n",
        "# ResNet18 ëª¨ë¸ ë¡œë“œ (ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©)\n",
        "model_freeze = models.resnet18(pretrained=True)\n",
        "\n",
        "# ëª¨ë“  íŒŒë¼ë¯¸í„° ë™ê²°\n",
        "for param in model_freeze.parameters():\n",
        "    param.requires_grad = False  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n",
        "\n",
        "# ë¶„ë¥˜ê¸°ë§Œ ìƒˆë¡œ ì •ì˜ (CIFAR-10ì€ 10ê°œ í´ë˜ìŠ¤)\n",
        "num_features = model_freeze.fc.in_features  # ì›ë˜ fc(ë¶„ë¥˜ê¸°)ì˜ ì…ë ¥ ì°¨ì›\n",
        "model_freeze.fc = nn.Linear(num_features, num_classes)  # ìƒˆë¡œìš´ ë¶„ë¥˜ê¸°\n",
        "\n",
        "# GPUë¡œ ì´ë™\n",
        "model_freeze = model_freeze.to(device)\n",
        "\n",
        "# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €\n",
        "criterion = nn.CrossEntropyLoss()  # ë¶„ë¥˜ ì†ì‹¤\n",
        "optimizer_freeze = optim.Adam(model_freeze.fc.parameters(), lr=learning_rate)  # ë¶„ë¥˜ê¸°ë§Œ ìµœì í™”\n",
        "\n",
        "print(f'í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model_freeze.parameters() if p.requires_grad):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwukxDRoK01R"
      },
      "outputs": [],
      "source": [
        "# Full Freeze ì „ëµ í•™ìŠµ\n",
        "history_freeze = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "\n",
        "print('\\ní•™ìŠµ ì‹œì‘...')\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # í•œ ì—í­ í•™ìŠµ\n",
        "    train_loss, train_acc = train_one_epoch(model_freeze, train_loader,\n",
        "                                           criterion, optimizer_freeze, device)\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
        "    _, test_acc = evaluate_model(model_freeze, test_loader, criterion, device)\n",
        "\n",
        "    # ê¸°ë¡ ì €ì¥\n",
        "    history_freeze['train_loss'].append(train_loss)\n",
        "    history_freeze['train_acc'].append(train_acc)\n",
        "    history_freeze['test_acc'].append(test_acc)\n",
        "\n",
        "    # ì§„í–‰ìƒí™© ì¶œë ¥\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
        "          f'Train Loss: {train_loss:.4f}, '\n",
        "          f'Train Acc: {train_acc:.2f}%, '\n",
        "          f'Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "elapsed_time_freeze = time.time() - start_time  # ê²½ê³¼ì‹œê°„\n",
        "print(f'\\ní•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed_time_freeze:.2f}ì´ˆ')\n",
        "print(f'ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {history_freeze[\"test_acc\"][-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFo7nDIDK01R"
      },
      "source": [
        "### ì „ëµ 2: Partial Fine-tuning (ë§ˆì§€ë§‰ ë¸”ë¡ë§Œ í•´ì œ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4stZGSeK01R"
      },
      "outputs": [],
      "source": [
        "print('\\n' + '='*60)\n",
        "print('ì „ëµ 2: Partial Fine-tuning - ë§ˆì§€ë§‰ ë¸”ë¡(layer4)ë§Œ í•™ìŠµ')\n",
        "print('='*60)\n",
        "\n",
        "# ResNet18 ëª¨ë¸ ë¡œë“œ\n",
        "model_partial = models.resnet18(pretrained=True)\n",
        "\n",
        "# ë¨¼ì € ëª¨ë“  íŒŒë¼ë¯¸í„° ë™ê²°\n",
        "for param in model_partial.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# ë§ˆì§€ë§‰ ë¸”ë¡(layer4)ë§Œ í•´ì œ\n",
        "for param in model_partial.layer4.parameters():\n",
        "    param.requires_grad = True  # layer4ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ\n",
        "\n",
        "# ë¶„ë¥˜ê¸° êµì²´\n",
        "model_partial.fc = nn.Linear(model_partial.fc.in_features, num_classes)\n",
        "\n",
        "# GPUë¡œ ì´ë™\n",
        "model_partial = model_partial.to(device)\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì € (layer4 + fcë§Œ ìµœì í™”)\n",
        "optimizer_partial = optim.Adam(\n",
        "    [{'params': model_partial.layer4.parameters()},\n",
        "     {'params': model_partial.fc.parameters()}],\n",
        "    lr=learning_rate\n",
        ")\n",
        "\n",
        "print(f'í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model_partial.parameters() if p.requires_grad):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBeubEHWK01R"
      },
      "outputs": [],
      "source": [
        "# Partial Fine-tuning ì „ëµ í•™ìŠµ\n",
        "history_partial = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "\n",
        "print('\\ní•™ìŠµ ì‹œì‘...')\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # í•œ ì—í­ í•™ìŠµ\n",
        "    train_loss, train_acc = train_one_epoch(model_partial, train_loader,\n",
        "                                           criterion, optimizer_partial, device)\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
        "    _, test_acc = evaluate_model(model_partial, test_loader, criterion, device)\n",
        "\n",
        "    # ê¸°ë¡ ì €ì¥\n",
        "    history_partial['train_loss'].append(train_loss)\n",
        "    history_partial['train_acc'].append(train_acc)\n",
        "    history_partial['test_acc'].append(test_acc)\n",
        "\n",
        "    # ì§„í–‰ìƒí™© ì¶œë ¥\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
        "          f'Train Loss: {train_loss:.4f}, '\n",
        "          f'Train Acc: {train_acc:.2f}%, '\n",
        "          f'Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "elapsed_time_partial = time.time() - start_time\n",
        "print(f'\\ní•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed_time_partial:.2f}ì´ˆ')\n",
        "print(f'ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {history_partial[\"test_acc\"][-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "377BVGCMK01S"
      },
      "source": [
        "### ì „ëµ 3: Full Fine-tuning (ëª¨ë“  ì¸µ í•™ìŠµ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZiNSG0tK01S"
      },
      "outputs": [],
      "source": [
        "print('\\n' + '='*60)\n",
        "print('ì „ëµ 3: Full Fine-tuning - ëª¨ë“  ì¸µ í•™ìŠµ')\n",
        "print('='*60)\n",
        "\n",
        "# ResNet18 ëª¨ë¸ ë¡œë“œ\n",
        "model_full = models.resnet18(pretrained=True)\n",
        "\n",
        "# ëª¨ë“  íŒŒë¼ë¯¸í„° í•™ìŠµ ê°€ëŠ¥ (ê¸°ë³¸ê°’ì´ Trueì´ë¯€ë¡œ ë³„ë„ ì„¤ì • ë¶ˆí•„ìš”)\n",
        "# í•˜ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„í•˜ë©´:\n",
        "for param in model_full.parameters():\n",
        "    param.requires_grad = True  # ëª¨ë“  ì¸µ í•™ìŠµ ê°€ëŠ¥\n",
        "\n",
        "# ë¶„ë¥˜ê¸° êµì²´\n",
        "model_full.fc = nn.Linear(model_full.fc.in_features, num_classes)\n",
        "\n",
        "# GPUë¡œ ì´ë™\n",
        "model_full = model_full.to(device)\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì € (ëª¨ë“  íŒŒë¼ë¯¸í„° ìµœì í™”, ì‘ì€ í•™ìŠµë¥  ì‚¬ìš©)\n",
        "optimizer_full = optim.Adam(model_full.parameters(), lr=learning_rate * 0.1)  # í•™ìŠµë¥  ë‚®ì¶¤\n",
        "\n",
        "print(f'í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model_full.parameters() if p.requires_grad):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxM7viV9K01S"
      },
      "outputs": [],
      "source": [
        "# Full Fine-tuning ì „ëµ í•™ìŠµ\n",
        "history_full = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "\n",
        "print('\\ní•™ìŠµ ì‹œì‘...')\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # í•œ ì—í­ í•™ìŠµ\n",
        "    train_loss, train_acc = train_one_epoch(model_full, train_loader,\n",
        "                                           criterion, optimizer_full, device)\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
        "    _, test_acc = evaluate_model(model_full, test_loader, criterion, device)\n",
        "\n",
        "    # ê¸°ë¡ ì €ì¥\n",
        "    history_full['train_loss'].append(train_loss)\n",
        "    history_full['train_acc'].append(train_acc)\n",
        "    history_full['test_acc'].append(test_acc)\n",
        "\n",
        "    # ì§„í–‰ìƒí™© ì¶œë ¥\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
        "          f'Train Loss: {train_loss:.4f}, '\n",
        "          f'Train Acc: {train_acc:.2f}%, '\n",
        "          f'Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "elapsed_time_full = time.time() - start_time\n",
        "print(f'\\ní•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed_time_full:.2f}ì´ˆ')\n",
        "print(f'ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {history_full[\"test_acc\"][-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ2Lh7FOK01S"
      },
      "source": [
        "### ì‹¤í—˜ 1 ê²°ê³¼ ë¹„êµ ë° ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-BZ5vW7K01S"
      },
      "outputs": [],
      "source": [
        "# ì„¸ ê°€ì§€ ì „ëµ ê²°ê³¼ ë¹„êµ ê·¸ë˜í”„\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "# 1. í•™ìŠµ ì •í™•ë„ ë¹„êµ\n",
        "axes[0].plot(epochs, history_freeze['train_acc'], 'b-o', label='Full Freeze', linewidth=2)\n",
        "axes[0].plot(epochs, history_partial['train_acc'], 'g-s', label='Partial Fine-tune', linewidth=2)\n",
        "axes[0].plot(epochs, history_full['train_acc'], 'r-^', label='Full Fine-tune', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Train Accuracy (%)', fontsize=12)\n",
        "axes[0].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. í…ŒìŠ¤íŠ¸ ì •í™•ë„ ë¹„êµ\n",
        "axes[1].plot(epochs, history_freeze['test_acc'], 'b-o', label='Full Freeze', linewidth=2)\n",
        "axes[1].plot(epochs, history_partial['test_acc'], 'g-s', label='Partial Fine-tune', linewidth=2)\n",
        "axes[1].plot(epochs, history_full['test_acc'], 'r-^', label='Full Fine-tune', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "axes[1].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDv5vQeOK01S"
      },
      "outputs": [],
      "source": [
        "# ìµœì¢… ê²°ê³¼ ìš”ì•½ í…Œì´ë¸”\n",
        "print('\\n' + '='*80)\n",
        "print('ì‹¤í—˜ 1: ì¸µë³„ ë™ê²° ì „ëµ ë¹„êµ - ìµœì¢… ê²°ê³¼')\n",
        "print('='*80)\n",
        "\n",
        "results = [\n",
        "    ['Full Freeze', history_freeze['test_acc'][-1], elapsed_time_freeze,\n",
        "     history_freeze['train_acc'][-1] - history_freeze['test_acc'][-1]],\n",
        "    ['Partial Fine-tune', history_partial['test_acc'][-1], elapsed_time_partial,\n",
        "     history_partial['train_acc'][-1] - history_partial['test_acc'][-1]],\n",
        "    ['Full Fine-tune', history_full['test_acc'][-1], elapsed_time_full,\n",
        "     history_full['train_acc'][-1] - history_full['test_acc'][-1]]\n",
        "]\n",
        "\n",
        "print(f'{\"Strategy\":<20} {\"Test Acc (%)\":<15} {\"Time (sec)\":<15} {\"Overfit Gap (%)\":<15}')\n",
        "print('-'*80)\n",
        "for result in results:\n",
        "    print(f'{result[0]:<20} {result[1]:<15.2f} {result[2]:<15.2f} {result[3]:<15.2f}')\n",
        "\n",
        "print('\\në¶„ì„:')\n",
        "print('  - Overfit Gap = Train Acc - Test Acc (ê³¼ì í•© ì •ë„)')\n",
        "print('  - ì‘ì€ ë°ì´í„°ì…‹ì—ì„œëŠ” Partial Fine-tuningì´ ì¢‹ì€ ê· í˜•ì ì„ ì œê³µí•©ë‹ˆë‹¤.')\n",
        "print('  - Full Fine-tuningì€ ê³¼ì í•© ìœ„í—˜ì´ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXm9GKMyK01S"
      },
      "source": [
        "---\n",
        "## Section 5: ì‹¤í—˜ 2 - ì°¨ë“± í•™ìŠµë¥  (Differential Learning Rate)\n",
        "\n",
        "ë°±ë³¸ê³¼ í—¤ë“œì— ì„œë¡œ ë‹¤ë¥¸ í•™ìŠµë¥ ì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEvPygQxK01S"
      },
      "outputs": [],
      "source": [
        "print('\\n' + '='*60)\n",
        "print('ì‹¤í—˜ 2: ì°¨ë“± í•™ìŠµë¥  ì ìš©')\n",
        "print('='*60)\n",
        "print('ë¹„êµ: ë‹¨ì¼ LR vs ì°¨ë“± LR (Head 10x, Backbone 1x)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONydCsxaK01S"
      },
      "source": [
        "### ë¹„êµ 1: ë‹¨ì¼ í•™ìŠµë¥ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYDkPKEWK01S"
      },
      "outputs": [],
      "source": [
        "print('\\n[ë¹„êµ 1] ë‹¨ì¼ í•™ìŠµë¥  - ëª¨ë“  ì¸µì— ë™ì¼í•œ LR ì ìš©')\n",
        "\n",
        "# ëª¨ë¸ ì¤€ë¹„ (layer4 + fcë§Œ í•™ìŠµ)\n",
        "model_uniform_lr = models.resnet18(pretrained=True)\n",
        "\n",
        "# layer4 ì´ì „ ë ˆì´ì–´ ê³ ì •\n",
        "for param in model_uniform_lr.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_uniform_lr.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# ë¶„ë¥˜ê¸° êµì²´\n",
        "model_uniform_lr.fc = nn.Linear(model_uniform_lr.fc.in_features, num_classes)\n",
        "model_uniform_lr = model_uniform_lr.to(device)\n",
        "\n",
        "# ë‹¨ì¼ í•™ìŠµë¥  ì˜µí‹°ë§ˆì´ì €\n",
        "base_lr = 0.001\n",
        "\n",
        "optimizer_uniform = optim.Adam(\n",
        "    [{'params': model_uniform_lr.layer4.parameters()},\n",
        "     {'params': model_uniform_lr.fc.parameters()}],\n",
        "    lr=base_lr  # ëª¨ë‘ ê°™ì€ í•™ìŠµë¥ \n",
        ")\n",
        "\n",
        "print(f'  - ë°±ë³¸(layer4) LR: {base_lr}')\n",
        "print(f'  - í—¤ë“œ(fc) LR: {base_lr}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brOtx0-9K01S"
      },
      "outputs": [],
      "source": [
        "# ë‹¨ì¼ LR í•™ìŠµ\n",
        "history_uniform_lr = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "\n",
        "print('\\ní•™ìŠµ ì‹œì‘...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model_uniform_lr, train_loader,\n",
        "                                           criterion, optimizer_uniform, device)\n",
        "    _, test_acc = evaluate_model(model_uniform_lr, test_loader, criterion, device)\n",
        "\n",
        "    history_uniform_lr['train_loss'].append(train_loss)\n",
        "    history_uniform_lr['train_acc'].append(train_acc)\n",
        "    history_uniform_lr['test_acc'].append(test_acc)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
        "          f'Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "print(f'\\nìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {history_uniform_lr[\"test_acc\"][-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vca2ydCVK01S"
      },
      "source": [
        "### ë¹„êµ 2: ì°¨ë“± í•™ìŠµë¥  (Differential LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0WvGUNOK01S"
      },
      "outputs": [],
      "source": [
        "print('\\n[ë¹„êµ 2] ì°¨ë“± í•™ìŠµë¥  - í—¤ë“œëŠ” 10ë°° ë†’ì€ LR ì‚¬ìš©')\n",
        "\n",
        "# ëª¨ë¸ ì¤€ë¹„\n",
        "model_diff_lr = models.resnet18(pretrained=True)\n",
        "\n",
        "# layer4 ì´ì „ ë ˆì´ì–´ ê³ ì •\n",
        "for param in model_diff_lr.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_diff_lr.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# ë¶„ë¥˜ê¸° êµì²´\n",
        "model_diff_lr.fc = nn.Linear(model_diff_lr.fc.in_features, num_classes)\n",
        "model_diff_lr = model_diff_lr.to(device)\n",
        "\n",
        "# ì°¨ë“± í•™ìŠµë¥  ì˜µí‹°ë§ˆì´ì €\n",
        "backbone_lr = 0.001  # ë°±ë³¸ í•™ìŠµë¥ \n",
        "head_lr = 0.01       # í—¤ë“œ í•™ìŠµë¥  (10ë°°)\n",
        "\n",
        "optimizer_diff = optim.Adam(\n",
        "    [{'params': model_diff_lr.layer4.parameters(), 'lr': backbone_lr},  # ë°±ë³¸\n",
        "     {'params': model_diff_lr.fc.parameters(), 'lr': head_lr}],         # í—¤ë“œ\n",
        ")\n",
        "\n",
        "print(f'  - ë°±ë³¸(layer4) LR: {backbone_lr}')\n",
        "print(f'  - í—¤ë“œ(fc) LR: {head_lr} (10x)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI7Z6j2lK01S"
      },
      "outputs": [],
      "source": [
        "# ì°¨ë“± LR í•™ìŠµ\n",
        "history_diff_lr = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "\n",
        "print('\\ní•™ìŠµ ì‹œì‘...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model_diff_lr, train_loader,\n",
        "                                           criterion, optimizer_diff, device)\n",
        "    _, test_acc = evaluate_model(model_diff_lr, test_loader, criterion, device)\n",
        "\n",
        "    history_diff_lr['train_loss'].append(train_loss)\n",
        "    history_diff_lr['train_acc'].append(train_acc)\n",
        "    history_diff_lr['test_acc'].append(test_acc)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
        "          f'Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "print(f'\\nìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {history_diff_lr[\"test_acc\"][-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_nrRNzuK01S"
      },
      "source": [
        "### ì‹¤í—˜ 2 ê²°ê³¼ ë¹„êµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUKBg0x6K01T"
      },
      "outputs": [],
      "source": [
        "# í•™ìŠµë¥  ì „ëµ ë¹„êµ ê·¸ë˜í”„\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# 1. í•™ìŠµ ì†ì‹¤ ë¹„êµ\n",
        "axes[0].plot(epochs, history_uniform_lr['train_loss'], 'b-o', label='Uniform LR', linewidth=2)\n",
        "axes[0].plot(epochs, history_diff_lr['train_loss'], 'r-s', label='Differential LR', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Train Loss', fontsize=12)\n",
        "axes[0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. í…ŒìŠ¤íŠ¸ ì •í™•ë„ ë¹„êµ\n",
        "axes[1].plot(epochs, history_uniform_lr['test_acc'], 'b-o', label='Uniform LR', linewidth=2)\n",
        "axes[1].plot(epochs, history_diff_lr['test_acc'], 'r-s', label='Differential LR', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "axes[1].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ê²°ê³¼ ìš”ì•½\n",
        "print('\\n' + '='*60)\n",
        "print('ì‹¤í—˜ 2: ì°¨ë“± í•™ìŠµë¥  ë¹„êµ - ê²°ê³¼')\n",
        "print('='*60)\n",
        "print(f'ë‹¨ì¼ LR ìµœì¢… ì •í™•ë„: {history_uniform_lr[\"test_acc\"][-1]:.2f}%')\n",
        "print(f'ì°¨ë“± LR ìµœì¢… ì •í™•ë„: {history_diff_lr[\"test_acc\"][-1]:.2f}%')\n",
        "print(f'\\nì„±ëŠ¥ í–¥ìƒ: {history_diff_lr[\"test_acc\"][-1] - history_uniform_lr[\"test_acc\"][-1]:.2f}%p')\n",
        "print('\\nğŸ’¡ ì¸ì‚¬ì´íŠ¸:')\n",
        "print('  - í—¤ë“œ(ìƒˆ ë¶„ë¥˜ê¸°)ëŠ” ë†’ì€ LRë¡œ ë¹ ë¥´ê²Œ í•™ìŠµ')\n",
        "print('  - ë°±ë³¸(ì‚¬ì „í•™ìŠµ ì¸µ)ì€ ë‚®ì€ LRë¡œ ì‹ ì¤‘í•˜ê²Œ ì¡°ì •')\n",
        "print('  - ì´ ì „ëµì€ catastrophic forgettingì„ ë°©ì§€í•©ë‹ˆë‹¤')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3Vg6bFXK01T"
      },
      "source": [
        "---\n",
        "## Section 6: ì‹¤í—˜ 3 - ë°ì´í„° ì¦ê°• ê°•ë„ ë¹„êµ\n",
        "\n",
        "Weak, Medium, Strong ì„¸ ê°€ì§€ ì¦ê°• ê°•ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CPM0aLBK01T"
      },
      "outputs": [],
      "source": [
        "print('\\n' + '='*60)\n",
        "print('ì‹¤í—˜ 3: ë°ì´í„° ì¦ê°• ê°•ë„ ë¹„êµ')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXuP4ivzK01T"
      },
      "outputs": [],
      "source": [
        "# 1. Weak Augmentation (ì•½í•œ ì¦ê°•)\n",
        "transform_weak = transforms.Compose([\n",
        "    transforms.Resize(224),  # í¬ê¸° ì¡°ì •\n",
        "    transforms.RandomHorizontalFlip(),  # ì¢Œìš° ë°˜ì „ë§Œ(50%í™•ë¥ ë¡œ)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 2. Medium Augmentation (ì¤‘ê°„ ì¦ê°•)\n",
        "transform_medium = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    # ëœë¤ í¬ë¡­(ìŠ¤ì¼€ì¼ ì¡°ì ˆí•´ì„œ ìë¥´ê¸°) 80-100% ìŠ¤ì¼€ì¼ ì¡°ì ˆ\n",
        "    transforms.RandomHorizontalFlip(),  # ì¢Œìš° ë°˜ì „\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # ìƒ‰ìƒ ë³€í˜•\n",
        "    # ë°ê¸°ì™€ ëŒ€ë¹„ë¥¼ [0.8, 1.2] ë²”ìœ„ë¡œ ì¡°ì •\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 3. Strong Augmentation (ê°•í•œ ì¦ê°•)\n",
        "transform_strong = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),  # ë” í° í¬ë¡­ ë²”ìœ„\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),  # íšŒì „ ì¶”ê°€\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),  # ê°•í•œ ìƒ‰ìƒ ë³€í˜•\n",
        "    # saturation ì±„ë„\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print('ì„¸ ê°€ì§€ ì¦ê°• ì „ëµ ì •ì˜ ì™„ë£Œ')\n",
        "print('  - Weak: ì¢Œìš° ë°˜ì „ë§Œ')\n",
        "print('  - Medium: í¬ë¡­ + ë°˜ì „ + ì•½í•œ ìƒ‰ìƒ ë³€í˜•')\n",
        "print('  - Strong: íšŒì „ + ê°•í•œ í¬ë¡­ + ê°•í•œ ìƒ‰ìƒ ë³€í˜•')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7OMrx34K01T"
      },
      "outputs": [],
      "source": [
        "# ì¦ê°•ë³„ ë°ì´í„°ì…‹ ìƒì„±\n",
        "# ì›ë³¸ CIFAR-10 ë‹¤ì‹œ ë¡œë“œ (ì¦ê°• ì ìš©ì„ ìœ„í•´)\n",
        "train_dataset_weak = datasets.CIFAR10(root='./data', train=True,\n",
        "                                      download=True, transform=transform_weak)\n",
        "train_dataset_medium = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_medium)\n",
        "train_dataset_strong = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_strong)\n",
        "\n",
        "# ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ìƒ˜í”Œë§ (ë™ì¼í•œ ì¸ë±ìŠ¤ ì‚¬ìš©)\n",
        "small_train_weak = Subset(train_dataset_weak, selected_indices)\n",
        "small_train_medium = Subset(train_dataset_medium, selected_indices)\n",
        "small_train_strong = Subset(train_dataset_strong, selected_indices)\n",
        "\n",
        "# ë°ì´í„° ë¡œë” ìƒì„±\n",
        "loader_weak = DataLoader(small_train_weak, batch_size=batch_size,\n",
        "                        shuffle=True, num_workers=2)\n",
        "loader_medium = DataLoader(small_train_medium, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=2)\n",
        "loader_strong = DataLoader(small_train_strong, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=2)\n",
        "\n",
        "print('ì¦ê°•ë³„ ë°ì´í„° ë¡œë” ì¤€ë¹„ ì™„ë£Œ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CZP84oEK01b"
      },
      "source": [
        "### Weak Augmentation ì‹¤í—˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlIrX8IqK01b"
      },
      "outputs": [],
      "source": [
        "print('\\n[ì‹¤í—˜ 3-1] Weak Augmentation')\n",
        "\n",
        "# ëª¨ë¸ ì¤€ë¹„\n",
        "model_weak_aug = models.resnet18(pretrained=True)\n",
        "for param in model_weak_aug.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_weak_aug.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "model_weak_aug.fc = nn.Linear(model_weak_aug.fc.in_features, num_classes)\n",
        "model_weak_aug = model_weak_aug.to(device)\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì €\n",
        "optimizer_weak_aug = optim.Adam(\n",
        "    [{'params': model_weak_aug.layer4.parameters(), 'lr': 0.001},\n",
        "     {'params': model_weak_aug.fc.parameters(), 'lr': 0.01}]\n",
        ")\n",
        "\n",
        "# í•™ìŠµ\n",
        "history_weak_aug = {'train_acc': [], 'test_acc': []}\n",
        "\n",
        "print('í•™ìŠµ ì‹œì‘...')\n",
        "for epoch in range(num_epochs):\n",
        "    _, train_acc = train_one_epoch(model_weak_aug, loader_weak,\n",
        "                                   criterion, optimizer_weak_aug, device)\n",
        "    _, test_acc = evaluate_model(model_weak_aug, test_loader, criterion, device)\n",
        "\n",
        "    history_weak_aug['train_acc'].append(train_acc)\n",
        "    history_weak_aug['test_acc'].append(test_acc)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
        "          f'Train: {train_acc:.2f}%, Test: {test_acc:.2f}%')\n",
        "\n",
        "print(f'ìµœì¢… ê²°ê³¼ - Test: {history_weak_aug[\"test_acc\"][-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVRFyGLXK01b"
      },
      "source": [
        "### Medium Augmentation ì‹¤í—˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzplNXB7K01b"
      },
      "outputs": [],
      "source": [
        "print('\\n[ì‹¤í—˜ 3-2] Medium Augmentation')\n",
        "\n",
        "# ëª¨ë¸ ì¤€ë¹„\n",
        "model_medium_aug = models.resnet18(pretrained=True)\n",
        "for param in model_medium_aug.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_medium_aug.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "model_medium_aug.fc = nn.Linear(model_medium_aug.fc.in_features, num_classes)\n",
        "model_medium_aug = model_medium_aug.to(device)\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì €\n",
        "optimizer_medium_aug = optim.Adam(\n",
        "    [{'params': model_medium_aug.layer4.parameters(), 'lr': 0.001},\n",
        "     {'params': model_medium_aug.fc.parameters(), 'lr': 0.01}]\n",
        ")\n",
        "\n",
        "# í•™ìŠµ\n",
        "history_medium_aug = {'train_acc': [], 'test_acc': []}\n",
        "\n",
        "print('í•™ìŠµ ì‹œì‘...')\n",
        "for epoch in range(num_epochs):\n",
        "    _, train_acc = train_one_epoch(model_medium_aug, loader_medium,\n",
        "                                   criterion, optimizer_medium_aug, device)\n",
        "    _, test_acc = evaluate_model(model_medium_aug, test_loader, criterion, device)\n",
        "\n",
        "    history_medium_aug['train_acc'].append(train_acc)\n",
        "    history_medium_aug['test_acc'].append(test_acc)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
        "          f'Train: {train_acc:.2f}%, Test: {test_acc:.2f}%')\n",
        "\n",
        "print(f'ìµœì¢… ê²°ê³¼ - Test: {history_medium_aug[\"test_acc\"][-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlIXQ9IhK01b"
      },
      "source": [
        "### Strong Augmentation ì‹¤í—˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OD4jXR5K01b"
      },
      "outputs": [],
      "source": [
        "print('\\n[ì‹¤í—˜ 3-3] Strong Augmentation')\n",
        "\n",
        "# ëª¨ë¸ ì¤€ë¹„\n",
        "model_strong_aug = models.resnet18(pretrained=True)\n",
        "for param in model_strong_aug.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_strong_aug.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "model_strong_aug.fc = nn.Linear(model_strong_aug.fc.in_features, num_classes)\n",
        "model_strong_aug = model_strong_aug.to(device)\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì €\n",
        "optimizer_strong_aug = optim.Adam(\n",
        "    [{'params': model_strong_aug.layer4.parameters(), 'lr': 0.001},\n",
        "     {'params': model_strong_aug.fc.parameters(), 'lr': 0.01}]\n",
        ")\n",
        "\n",
        "# í•™ìŠµ\n",
        "history_strong_aug = {'train_acc': [], 'test_acc': []}\n",
        "\n",
        "print('í•™ìŠµ ì‹œì‘...')\n",
        "for epoch in range(num_epochs):\n",
        "    _, train_acc = train_one_epoch(model_strong_aug, loader_strong,\n",
        "                                   criterion, optimizer_strong_aug, device)\n",
        "    _, test_acc = evaluate_model(model_strong_aug, test_loader, criterion, device)\n",
        "\n",
        "    history_strong_aug['train_acc'].append(train_acc)\n",
        "    history_strong_aug['test_acc'].append(test_acc)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
        "          f'Train: {train_acc:.2f}%, Test: {test_acc:.2f}%')\n",
        "\n",
        "print(f'ìµœì¢… ê²°ê³¼ - Test: {history_strong_aug[\"test_acc\"][-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K9VabxHK01b"
      },
      "source": [
        "### ì‹¤í—˜ 3 ê²°ê³¼ ë¹„êµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZLBr3y7K01b"
      },
      "outputs": [],
      "source": [
        "# ì¦ê°• ê°•ë„ë³„ ë¹„êµ ê·¸ë˜í”„\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# 1. í…ŒìŠ¤íŠ¸ ì •í™•ë„\n",
        "axes[0].plot(epochs, history_weak_aug['test_acc'], 'b-o', label='Weak Aug', linewidth=2)\n",
        "axes[0].plot(epochs, history_medium_aug['test_acc'], 'g-s', label='Medium Aug', linewidth=2)\n",
        "axes[0].plot(epochs, history_strong_aug['test_acc'], 'r-^', label='Strong Aug', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "axes[0].set_title('Test Accuracy by Augmentation Strength', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. ê³¼ì í•© ê°­ (Train - Test)\n",
        "gap_weak = [t - v for t, v in zip(history_weak_aug['train_acc'], history_weak_aug['test_acc'])]\n",
        "gap_medium = [t - v for t, v in zip(history_medium_aug['train_acc'], history_medium_aug['test_acc'])]\n",
        "gap_strong = [t - v for t, v in zip(history_strong_aug['train_acc'], history_strong_aug['test_acc'])]\n",
        "\n",
        "axes[1].plot(epochs, gap_weak, 'b-o', label='Weak Aug', linewidth=2)\n",
        "axes[1].plot(epochs, gap_medium, 'g-s', label='Medium Aug', linewidth=2)\n",
        "axes[1].plot(epochs, gap_strong, 'r-^', label='Strong Aug', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Overfitting Gap (%)', fontsize=12)\n",
        "axes[1].set_title('Overfitting Gap (Train - Test)', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
        "print('\\n' + '='*80)\n",
        "print('ì‹¤í—˜ 3: ë°ì´í„° ì¦ê°• ê°•ë„ ë¹„êµ - ìµœì¢… ê²°ê³¼')\n",
        "print('='*80)\n",
        "print(f'{\"Augmentation\":<15} {\"Test Acc (%)\":<15} {\"Overfit Gap (%)\":<15}')\n",
        "print('-'*80)\n",
        "print(f'Weak{\"\":<11} {history_weak_aug[\"test_acc\"][-1]:<15.2f} {gap_weak[-1]:<15.2f}')\n",
        "print(f'Medium{\"\":<9} {history_medium_aug[\"test_acc\"][-1]:<15.2f} {gap_medium[-1]:<15.2f}')\n",
        "print(f'Strong{\"\":<9} {history_strong_aug[\"test_acc\"][-1]:<15.2f} {gap_strong[-1]:<15.2f}')\n",
        "\n",
        "print('\\nì¸ì‚¬ì´íŠ¸:')\n",
        "print('  - ì‘ì€ ë°ì´í„°ì…‹(500ê°œ)ì—ì„œëŠ” Medium ì¦ê°•ì´ ìµœì  ê· í˜•ì ')\n",
        "print('  - Strong ì¦ê°•ì€ ê³¼ì í•©ì€ ì¤„ì´ì§€ë§Œ, ë„ˆë¬´ ê°•í•˜ë©´ underfitting ë°œìƒ ê°€ëŠ¥')\n",
        "print('  - Weak ì¦ê°•ì€ ê³¼ì í•© ìœ„í—˜ì´ ë†’ìŒ')\n",
        "print('  - ë°ì´í„°ì…‹ í¬ê¸°ê°€ í´ìˆ˜ë¡ Strong ì¦ê°•ì˜ íš¨ê³¼ê°€ ì»¤ì§‘ë‹ˆë‹¤')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru1KvQzCK01b"
      },
      "source": [
        "---\n",
        "## Section 7: ì‹¤í—˜ 4 - ì‘ì€ ë°ì´í„°ì…‹ ìµœì  í”„ë¡œí† ì½œ\n",
        "\n",
        "2ë‹¨ê³„ í•™ìŠµ ì „ëµì„ ì ìš©í•©ë‹ˆë‹¤:\n",
        "1. **Phase 1**: í—¤ë“œë§Œ í•™ìŠµ (5 epochs)\n",
        "2. **Phase 2**: ì ì§„ì  í•´ì œ (5 epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1N3XQsMK01b"
      },
      "outputs": [],
      "source": [
        "print('\\n' + '='*60)\n",
        "print('ì‹¤í—˜ 4: ì‘ì€ ë°ì´í„°ì…‹ì„ ìœ„í•œ 2ë‹¨ê³„ í•™ìŠµ í”„ë¡œí† ì½œ')\n",
        "print('='*60)\n",
        "print('Phase 1: í—¤ë“œë§Œ í•™ìŠµ (5 epochs)')\n",
        "print('Phase 2: Layer4 ì ì§„ì  í•´ì œ (5 epochs)')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3CPGsMhK01b"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ì¤€ë¹„\n",
        "model_2phase = models.resnet18(pretrained=True)\n",
        "\n",
        "# ëª¨ë“  íŒŒë¼ë¯¸í„° ë™ê²°\n",
        "for param in model_2phase.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# ë¶„ë¥˜ê¸° êµì²´\n",
        "model_2phase.fc = nn.Linear(model_2phase.fc.in_features, num_classes)\n",
        "model_2phase = model_2phase.to(device)\n",
        "\n",
        "print('ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ (ëª¨ë“  ë°±ë³¸ ë™ê²°ë¨)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFHpB1k-K01b"
      },
      "source": [
        "### Phase 1: í—¤ë“œë§Œ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt3MSW-GK01b"
      },
      "outputs": [],
      "source": [
        "print('\\n' + '-'*60)\n",
        "print('Phase 1: í—¤ë“œ(ë¶„ë¥˜ê¸°)ë§Œ í•™ìŠµ')\n",
        "print('-'*60)\n",
        "\n",
        "# Phase 1 ì˜µí‹°ë§ˆì´ì € (ë¶„ë¥˜ê¸°ë§Œ)\n",
        "optimizer_phase1 = optim.Adam(model_2phase.fc.parameters(), lr=0.01)  # ë†’ì€ í•™ìŠµë¥ \n",
        "\n",
        "# Phase 1 í•™ìŠµ\n",
        "history_2phase = {'train_acc': [], 'test_acc': [], 'phase': []}\n",
        "phase1_epochs = 5\n",
        "\n",
        "print('í•™ìŠµ ì‹œì‘...')\n",
        "for epoch in range(phase1_epochs):\n",
        "    _, train_acc = train_one_epoch(model_2phase, loader_medium,  # Medium ì¦ê°• ì‚¬ìš©\n",
        "                                   criterion, optimizer_phase1, device)\n",
        "    _, test_acc = evaluate_model(model_2phase, test_loader, criterion, device)\n",
        "\n",
        "    history_2phase['train_acc'].append(train_acc)\n",
        "    history_2phase['test_acc'].append(test_acc)\n",
        "    history_2phase['phase'].append(1)\n",
        "\n",
        "    print(f'Phase1 Epoch [{epoch+1}/{phase1_epochs}] - '\n",
        "          f'Train: {train_acc:.2f}%, Test: {test_acc:.2f}%')\n",
        "\n",
        "print(f'\\nPhase 1 ì™„ë£Œ - Test Acc: {history_2phase[\"test_acc\"][-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ0S6NLTK01c"
      },
      "source": [
        "### Phase 2: ì ì§„ì  í•´ì œ (Layer4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is3EcVIxK01c"
      },
      "outputs": [],
      "source": [
        "print('\\n' + '-'*60)\n",
        "print('Phase 2: Layer4 í•´ì œí•˜ê³  í•¨ê»˜ í•™ìŠµ')\n",
        "print('-'*60)\n",
        "\n",
        "# Layer4 í•´ì œ\n",
        "for param in model_2phase.layer4.parameters():\n",
        "    param.requires_grad = True  # Layer4 í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ\n",
        "\n",
        "# Phase 2 ì˜µí‹°ë§ˆì´ì € (ì°¨ë“± í•™ìŠµë¥ )\n",
        "optimizer_phase2 = optim.Adam(\n",
        "    [{'params': model_2phase.layer4.parameters(), 'lr': 0.0001},  # ë°±ë³¸ì€ ì‘ì€ LR\n",
        "     {'params': model_2phase.fc.parameters(), 'lr': 0.001}],      # í—¤ë“œëŠ” ì¤‘ê°„ LR\n",
        ")\n",
        "\n",
        "# Phase 2 í•™ìŠµ\n",
        "phase2_epochs = 5\n",
        "\n",
        "print('í•™ìŠµ ì‹œì‘...')\n",
        "for epoch in range(phase2_epochs):\n",
        "    _, train_acc = train_one_epoch(model_2phase, loader_medium,\n",
        "                                   criterion, optimizer_phase2, device)\n",
        "    _, test_acc = evaluate_model(model_2phase, test_loader, criterion, device)\n",
        "\n",
        "    history_2phase['train_acc'].append(train_acc)\n",
        "    history_2phase['test_acc'].append(test_acc)\n",
        "    history_2phase['phase'].append(2)\n",
        "\n",
        "    print(f'Phase2 Epoch [{epoch+1}/{phase2_epochs}] - '\n",
        "          f'Train: {train_acc:.2f}%, Test: {test_acc:.2f}%')\n",
        "\n",
        "print(f'\\nPhase 2 ì™„ë£Œ - ìµœì¢… Test Acc: {history_2phase[\"test_acc\"][-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riwOzVa8K01c"
      },
      "source": [
        "### 2ë‹¨ê³„ í”„ë¡œí† ì½œ ê²°ê³¼ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiwrXGw9K01c"
      },
      "outputs": [],
      "source": [
        "# 2ë‹¨ê³„ í•™ìŠµ ê³¼ì • ì‹œê°í™”\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "total_epochs = range(1, len(history_2phase['test_acc']) + 1)\n",
        "\n",
        "# Phase êµ¬ë¶„ì„ \n",
        "phase_boundary = phase1_epochs\n",
        "ax.axvline(x=phase_boundary, color='gray', linestyle='--', linewidth=2,\n",
        "           label='Phase Transition', alpha=0.5)\n",
        "\n",
        "# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì •í™•ë„ í”Œë¡¯\n",
        "ax.plot(total_epochs, history_2phase['train_acc'], 'b-o',\n",
        "        label='Train Accuracy', linewidth=2, markersize=6)\n",
        "ax.plot(total_epochs, history_2phase['test_acc'], 'r-s',\n",
        "        label='Test Accuracy', linewidth=2, markersize=6)\n",
        "\n",
        "# Phase ì˜ì—­ í‘œì‹œ\n",
        "ax.axvspan(0, phase_boundary, alpha=0.1, color='blue', label='Phase 1: Head Only')\n",
        "ax.axvspan(phase_boundary, len(total_epochs), alpha=0.1, color='green',\n",
        "          label='Phase 2: Head + Layer4')\n",
        "\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Two-Phase Training Protocol for Small Dataset',\n",
        "            fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=10, loc='lower right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('2ë‹¨ê³„ í”„ë¡œí† ì½œ ìµœì¢… ê²°ê³¼')\n",
        "print('='*60)\n",
        "print(f'Phase 1 ì¢…ë£Œ ì‹œ ì •í™•ë„: {history_2phase[\"test_acc\"][phase1_epochs-1]:.2f}%')\n",
        "print(f'Phase 2 ì¢…ë£Œ ì‹œ ì •í™•ë„: {history_2phase[\"test_acc\"][-1]:.2f}%')\n",
        "print(f'ì „ì²´ ê°œì„ : {history_2phase[\"test_acc\"][-1] - history_2phase[\"test_acc\"][0]:.2f}%p')\n",
        "print('\\ní•µì‹¬ í¬ì¸íŠ¸:')\n",
        "print('  1. Phase 1ì—ì„œ í—¤ë“œë¥¼ ë¨¼ì € ì•ˆì •í™”')\n",
        "print('  2. Phase 2ì—ì„œ ë°±ë³¸ì„ ì‹ ì¤‘í•˜ê²Œ ë¯¸ì„¸ì¡°ì •')\n",
        "print('  3. ì°¨ë“± í•™ìŠµë¥ ë¡œ catastrophic forgetting ë°©ì§€')\n",
        "print('  4. ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ íš¨ê³¼ì ì¸ ì „ëµ!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hCTkRAhK01c"
      },
      "source": [
        "---\n",
        "## Section 8: ì „ì²´ ì‹¤í—˜ ì¢…í•© ë¹„êµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPS1tHe6K01c"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ì¢…í•©\n",
        "print('\\n' + '='*80)\n",
        "print('ì „ì²´ ì‹¤í—˜ ì¢…í•© ê²°ê³¼')\n",
        "print('='*80)\n",
        "\n",
        "print('\\n1. ì¸µë³„ ë™ê²° ì „ëµ ë¹„êµ:')\n",
        "print(f'   - Full Freeze:        {history_freeze[\"test_acc\"][-1]:.2f}%')\n",
        "print(f'   - Partial Fine-tune:  {history_partial[\"test_acc\"][-1]:.2f}%')\n",
        "print(f'   - Full Fine-tune:     {history_full[\"test_acc\"][-1]:.2f}%')\n",
        "\n",
        "print('\\n2. í•™ìŠµë¥  ì „ëµ ë¹„êµ:')\n",
        "print(f'   - Uniform LR:         {history_uniform_lr[\"test_acc\"][-1]:.2f}%')\n",
        "print(f'   - Differential LR:    {history_diff_lr[\"test_acc\"][-1]:.2f}%')\n",
        "\n",
        "print('\\n3. ë°ì´í„° ì¦ê°• ê°•ë„ ë¹„êµ:')\n",
        "print(f'   - Weak Augmentation:    {history_weak_aug[\"test_acc\"][-1]:.2f}%')\n",
        "print(f'   - Medium Augmentation:  {history_medium_aug[\"test_acc\"][-1]:.2f}%')\n",
        "print(f'   - Strong Augmentation:  {history_strong_aug[\"test_acc\"][-1]:.2f}%')\n",
        "\n",
        "print('\\n4. 2ë‹¨ê³„ í”„ë¡œí† ì½œ:')\n",
        "print(f'   - Two-Phase Training:   {history_2phase[\"test_acc\"][-1]:.2f}%')\n",
        "\n",
        "print('\\n' + '='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGMkqauIK01c"
      },
      "outputs": [],
      "source": [
        "# ìµœì¢… ì¢…í•© ê·¸ë˜í”„\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "strategies = [\n",
        "    'Full\\nFreeze',\n",
        "    'Partial\\nFine-tune',\n",
        "    'Full\\nFine-tune',\n",
        "    'Uniform\\nLR',\n",
        "    'Differential\\nLR',\n",
        "    'Weak\\nAug',\n",
        "    'Medium\\nAug',\n",
        "    'Strong\\nAug',\n",
        "    '2-Phase\\nProtocol'\n",
        "]\n",
        "\n",
        "accuracies = [\n",
        "    history_freeze['test_acc'][-1],\n",
        "    history_partial['test_acc'][-1],\n",
        "    history_full['test_acc'][-1],\n",
        "    history_uniform_lr['test_acc'][-1],\n",
        "    history_diff_lr['test_acc'][-1],\n",
        "    history_weak_aug['test_acc'][-1],\n",
        "    history_medium_aug['test_acc'][-1],\n",
        "    history_strong_aug['test_acc'][-1],\n",
        "    history_2phase['test_acc'][-1]\n",
        "]\n",
        "\n",
        "# ìƒ‰ìƒ êµ¬ë¶„\n",
        "colors = ['#1f77b4', '#1f77b4', '#1f77b4',  # ì‹¤í—˜ 1\n",
        "          '#ff7f0e', '#ff7f0e',              # ì‹¤í—˜ 2\n",
        "          '#2ca02c', '#2ca02c', '#2ca02c',   # ì‹¤í—˜ 3\n",
        "          '#d62728']                          # ì‹¤í—˜ 4\n",
        "\n",
        "bars = ax.bar(strategies, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# ê°’ í‘œì‹œ\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.2f}%',\n",
        "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Test Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('All Experiments Comparison - Final Test Accuracy',\n",
        "            fontsize=14, fontweight='bold')\n",
        "ax.set_ylim([0, max(accuracies) * 1.1])\n",
        "ax.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "# ë²”ë¡€\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#1f77b4', alpha=0.7, label='Exp 1: Freeze Strategy'),\n",
        "    Patch(facecolor='#ff7f0e', alpha=0.7, label='Exp 2: Learning Rate'),\n",
        "    Patch(facecolor='#2ca02c', alpha=0.7, label='Exp 3: Augmentation'),\n",
        "    Patch(facecolor='#d62728', alpha=0.7, label='Exp 4: 2-Phase Protocol')\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
        "\n",
        "plt.xticks(rotation=0, fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCGQQVq4K01c"
      },
      "source": [
        "---\n",
        "## Section 9: ì‹¤ìŠµ ì •ë¦¬ ë° ê³¼ì œ\n",
        "\n",
        "### í•µì‹¬ ìš”ì•½\n",
        "\n",
        "1. **ì¸µë³„ ë™ê²° ì „ëµ**\n",
        "   - ì‘ì€ ë°ì´í„°ì…‹: Partial Fine-tuningì´ ìµœì \n",
        "   - Full Fine-tuningì€ ê³¼ì í•© ìœ„í—˜\n",
        "   - ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¼ ì „ëµ ì„ íƒ\n",
        "\n",
        "2. **ì°¨ë“± í•™ìŠµë¥ **\n",
        "   - í—¤ë“œ(ìƒˆ ì¸µ): ë†’ì€ LR\n",
        "   - ë°±ë³¸(ì‚¬ì „í•™ìŠµ ì¸µ): ë‚®ì€ LR\n",
        "   - Catastrophic forgetting ë°©ì§€\n",
        "\n",
        "3. **ë°ì´í„° ì¦ê°•**\n",
        "   - ì‘ì€ ë°ì´í„°: Medium ì¦ê°•\n",
        "   - í° ë°ì´í„°: Strong ì¦ê°•\n",
        "   - ë„ë©”ì¸ íŠ¹ì„± ê³ ë ¤ í•„ìˆ˜\n",
        "\n",
        "4. **2ë‹¨ê³„ í”„ë¡œí† ì½œ**\n",
        "   - Phase 1: í—¤ë“œ ì„ í•™ìŠµ\n",
        "   - Phase 2: ì ì§„ì  í•´ì œ\n",
        "   - ì‘ì€ ë°ì´í„°ì…‹ì— íš¨ê³¼ì \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FrjSjLJNNwGh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
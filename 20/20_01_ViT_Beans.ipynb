{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d3301a38",
      "metadata": {
        "id": "d3301a38"
      },
      "source": [
        "# 세션 20 — Vision Transformer(ViT) 이미지 분류 — beans — 2025 버전"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40fb9967",
      "metadata": {
        "id": "40fb9967"
      },
      "outputs": [],
      "source": [
        "# torchvision과 torchaudio를 추가하여 torch 버전과 호환되도록 함께 업데이트합니다.\n",
        "!pip -q install -U \"torch>=2.2,<3.0\" \"torchvision\" \"torchaudio\" \"datasets>=3.0.1\" \"transformers>=4.45.2\" \"accelerate>=1.0.1\" \"evaluate>=0.4.2\"\n",
        "\n",
        "import torch, transformers, datasets, evaluate\n",
        "import numpy as np\n",
        "\n",
        "print(\"PyTorch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available())\n",
        "print(\"Transformers:\", transformers.__version__, \"| Datasets:\", datasets.__version__)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7954dca8",
      "metadata": {
        "id": "7954dca8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "beans=load_dataset(\"beans\")\n",
        "print(beans)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beans[\"train\"].features"
      ],
      "metadata": {
        "id": "53k3FytndEJ8"
      },
      "id": "53k3FytndEJ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beans[\"train\"].features"
      ],
      "metadata": {
        "id": "gMymETipdaDz"
      },
      "id": "gMymETipdaDz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"labels\" if \"labels\" in beans[\"train\"].features else \"label\"\n",
        "# 데이터 셋에 \"labels\" 있으면 >> key = \"labels\"\n",
        "# 없으면 >> key =\"label\"\n",
        "# 데이터 셋에 누군가는 labels =[rockey, rocky, locky], label = [rockey, rocky, locky]\n",
        "# 데이터 셋마다 이름이 다를 수 있으니깐\n",
        "# 실제 사례) 공공데이터포털(data.go.kr) [강남 구, 강남구] >> set() 중복여부 확인\n",
        "\n",
        "print(beans[\"train\"].features[key])\n",
        "print(beans[\"train\"].features[key].names)"
      ],
      "metadata": {
        "id": "FXDJPjPQdg3D"
      },
      "id": "FXDJPjPQdg3D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beans[\"train\"].features[key].names[0]"
      ],
      "metadata": {
        "id": "5PylKs0Id0Cj"
      },
      "id": "5PylKs0Id0Cj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = beans[\"train\"].features[key].names[0]\n",
        "\n",
        "print({i:n for i,n in enumerate(names)})\n",
        "print({n:i for i,n in enumerate(names)})"
      ],
      "metadata": {
        "id": "IfRTZnU6d8K9"
      },
      "id": "IfRTZnU6d8K9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b03e9d92",
      "metadata": {
        "id": "b03e9d92"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor, ViTForImageClassification\n",
        "import torch\n",
        "\n",
        "# 모델 및 프로세서 설정\n",
        "MODEL = \"google/vit-base-patch16-224\"\n",
        "processor = AutoImageProcessor.from_pretrained(MODEL)\n",
        "\n",
        "# 라벨 매핑 설정 (beans 데이터셋 가정)\n",
        "key = \"labels\" if \"labels\" in beans[\"train\"].features else \"label\"\n",
        "names = beans[\"train\"].features[key].names\n",
        "\n",
        "# 코드 입력\n",
        "id2label =  {i:n for i,n in enumerate(names)}\n",
        "label2id = {n:i for i,n in enumerate(names)}\n",
        "\n",
        "# 모델 로드\n",
        "# 코드 입력\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    MODEL,\n",
        "    num_labels = len(names),\n",
        "    id2label = id2label,\n",
        "    label2id = label2id,\n",
        "    ignore_mismatched_sizes = True #fine tuning\n",
        "    # 1000개 분류해주는 모델 가져와서 3개 분류 하니깐\n",
        "    # 크기 안맞는 부분 무시하고 새로 만들어\n",
        ").to(device)\n",
        "\n",
        "print(\"모델 로드 완료. 분류기 크기:\", model.classifier.out_features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 참고 예제\n",
        "# example =  {7: 'son', 10:'son'}\n",
        "# example\n",
        "# example[7]\n",
        "\n",
        "# example[23] = 'jordan'\n",
        "# example"
      ],
      "metadata": {
        "id": "sM2i3i7SjoYI"
      },
      "id": "sM2i3i7SjoYI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ccf935",
      "metadata": {
        "id": "94ccf935"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 코랩의 CPU 코어 개수 확인 (보통 2~4개)\n",
        "num_proc = os.cpu_count()\n",
        "\n",
        "def transform(ex):\n",
        "    # 입력이 PIL 이미지 리스트라고 가정 (batched=True 이므로)\n",
        "    # processor가 알아서 리사이즈 및 정규화를 수행하고 텐서로 변환\n",
        "    # 코드 입력\n",
        "    inputs = processor(images = ex[\"image\"], return_tensors=\"pt\")\n",
        "\n",
        "    # 결과 저장\n",
        "    ex[\"pixel_values\"] = inputs[\"pixel_values\"]\n",
        "    # \"pixel_values\" : 변환된 이미지 데이터\n",
        "    # 원본데이터에 '변환된 이미지 데이터' 추가 (새로 열 하나 만들어서)\n",
        "    return ex\n",
        "\n",
        "# 1. 먼저 map으로 전처리 수행 (병렬 처리 추가)\n",
        "# remove_columns를 사용하여 원본 'image' 컬럼을 제거하면 메모리 절약에 도움이 됩니다.\n",
        "# 코드 입력\n",
        "\n",
        "beans = beans.map(\n",
        "        transform,\n",
        "        batched=True,\n",
        "        num_proc = num_proc,\n",
        "        remove_columns=[\"image\"] # 학습에 필요없는 원본 이미지 컬럼 삭제\n",
        "    )\n",
        "\n",
        "\n",
        "beans.set_format(\"torch\")\n",
        "\n",
        "print(\"전처리 완료!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38042076",
      "metadata": {
        "id": "38042076"
      },
      "outputs": [],
      "source": [
        "# 필요한 컬럼(열, 특성)만 유지하는 사용자 함수\n",
        "def keep(split):\n",
        "  # split : train, val, test\n",
        "  # 코드 입력\n",
        "  cols = ['pixel_values', key] # [변환된 이미지 데이터, 정답(라벨)]\n",
        "  return beans[split].remove_columns([c for c in beans[split].column_names if c not in cols])\n",
        "  # 모든 컬럼이름 목록을 하나씩 반복하면서 리스트 컴프리헨션 활용해서 cols 없는 컬럼들만 선택해서\n",
        "  # 선택된 컬럼들을 삭제해 >> 필요한 것(컬럼)만 남기고 나머지 버려\n",
        "\n",
        "train,val,test=keep(\"train\"),keep(\"validation\"),keep(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd\n",
        "# present working directory(현재 작업 폴더)"
      ],
      "metadata": {
        "id": "XdWOzwRspEYx"
      },
      "id": "XdWOzwRspEYx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22731837",
      "metadata": {
        "id": "22731837"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DefaultDataCollator\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 정확도 지표 로드\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "\n",
        "# 메트릭 계산 함수 정의\n",
        "def metrics(p):\n",
        "    predictions, labels = p\n",
        "    # Trainer는 결과를 Numpy 배열로 반환하므로 np.argmax 사용\n",
        "    # 코드 입력\n",
        "    pred = np.argmax(predictions, axis=1)\n",
        "    # np.argmax() : 가장 높은 점수의 인덱스\n",
        "\n",
        "    return {\"accuracy\": acc.compute(predictions=pred, references=labels)[\"accuracy\"]}\n",
        "\n",
        "# TrainingArguments 설정\n",
        "# 코드 입력\n",
        "args = TrainingArguments(\n",
        "    output_dir= \"/content/vit_beans_2025\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    per_device_train_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    fp16 = torch.cuda.is_available(),\n",
        "    report_to = \"none\",\n",
        "    seed=2025,\n",
        "    remove_unused_columns=False\n",
        "    # (권장) 이미지 데이터셋 컬럼 유지 위해 설정 권장\n",
        ")\n",
        "\n",
        "# Trainer 초기화\n",
        "# tokenizer=processor 대신 data_collator를 명시하는 것이 이미지 모델 정석입니다.\n",
        "# 코드입력\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args=args,\n",
        "    train_dataset = train,\n",
        "    eval_dataset= val,\n",
        "    data_collator=DefaultDataCollator(), # 텐서 배치를 위한 Collator\n",
        "    compute_metrics=metrics\n",
        ")\n",
        "\n",
        "# 학습 시작\n",
        "trainer.train()\n",
        "\n",
        "# 테스트 데이터 평가\n",
        "print(trainer.evaluate(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be7d5813",
      "metadata": {
        "id": "be7d5813"
      },
      "outputs": [],
      "source": [
        "# 예측 테스트\n",
        "import torch\n",
        "\n",
        "for i in [0, 1]:\n",
        "    ex = beans[\"test\"][i]\n",
        "\n",
        "    # 1. 입력 데이터 처리\n",
        "    # ex[\"pixel_values\"]가 이미 텐서이므로 바로 사용하되, 복사본을 만들어 안전하게 처리\n",
        "    input_tensor = ex[\"pixel_values\"].clone().detach()\n",
        "    # detach: gradient 연결 끊기\n",
        "\n",
        "    # 배치 차원 추가 >> GPU 이동\n",
        "    # 코드 입력\n",
        "    inputs = input_tensor.unsqueeze(0).to(model.device)\n",
        "    # .unsqueeze(0): 맨 앞에 차원 추가 (배치)\n",
        "    # [3,224,224] >> [1,3,224,224]\n",
        "\n",
        "    # 2. 모델 예측\n",
        "    # 코드 입력\n",
        "\n",
        "    # 3. 정답 라벨 가져오기 (수정된 부분)\n",
        "    # 'labels' 혹은 'label' 키 확인\n",
        "    # 코드 입력\n",
        "    with torch.no_grad():\n",
        "      logits = model(inputs).logits\n",
        "      # 예) logits : [[-2.0,0.5,3.5]]\n",
        "      # argmax: 2(세번째 가장 높음(3.5))\n",
        "      pred = logits.argmax(-1).item()\n",
        "\n",
        "    # 코드 입력\n",
        "    # 3. 정답 라벨 가져오기\n",
        "    label_key = \"labels\" if \"labels\" in ex else \"label\"\n",
        "    # 핵심 수정: .item()을 붙여서 tensor(0) -> 0 (정수)으로 변환\n",
        "    true_label_id  = ex[label_key].item()\n",
        "\n",
        "    # 4. 결과 출력\n",
        "    print(f\"[{i}] 예측: {model.config.id2label[pred]} | 정답: {model.config.id2label[true_label_id]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EOS"
      ],
      "metadata": {
        "id": "0yjTXA4xkXFe"
      },
      "id": "0yjTXA4xkXFe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XpeIL7fYklum"
      },
      "id": "XpeIL7fYklum",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
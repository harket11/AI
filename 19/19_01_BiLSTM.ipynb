{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- 데이터: IMDb (Hugging Face datasets)\n",
        "\n",
        "- 모델: Embedding → BiLSTM → Linear\n"
      ],
      "metadata": {
        "id": "6qQcOxaIqTZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab 사전 설치 & 버전 확인 (2025 기준)\n",
        "# - PyTorch 2.2+ / Transformers 4.45+ / Datasets 3.0+ 권장\n",
        "# - 이미 설치되어 있어도 최신으로 맞춤\n",
        "\n",
        "!pip -q install -U \"torch>=2.2, <3.0\" \"torchvision>=0.17, <1.0\" \"torchaudio>=2.2, <3.0\"\n",
        "!pip -q install -U \"datasets>=3.0.1\" \"transformers>=4.45.2\" \"accelerate>=1.0.1\" \"evaluate>=0.4.2\" \"scikit-learn>=1.5.2\""
      ],
      "metadata": {
        "id": "kPOp1_RvjpGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIZV610FpRgM"
      },
      "outputs": [],
      "source": [
        "import torch, transformers, datasets, sklearn, evaluate, sys, platform\n",
        "print(\"Python            :\", sys.version.split()[0])\n",
        "print(\"Platform          :\", platform.platform())\n",
        "print(\"PyTorch           :\", torch.__version__)\n",
        "print(\"Transformers      :\", transformers.__version__)\n",
        "print(\"Datasets          :\", datasets.__version__)\n",
        "print(\"scikit-learn      :\", sklearn.__version__)\n",
        "print(\"evaluate          :\", evaluate.__version__)\n",
        "print(\"CUDA available    :\", torch.cuda.is_available())\n",
        "print(\"GPU device        :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 임포트 & 시드 고정\n",
        "import re, math, random, numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 재현성을 위해 시드 고정\n",
        "SEED = 2025\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "phkTYMBijs36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로드: IMDb (Hugging Face datasets)\n",
        "# load_dataset은 자동으로 캐시 관리해줍니다.\n",
        "raw = load_dataset(\"imdb\")  # {'train':..., 'test':...}\n",
        "print(raw)"
      ],
      "metadata": {
        "id": "9DJ8MZFajyJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [간단 토크나이저 정의(정규식): 영문 단어 분할\n",
        "# 영어 소문자/숫자 단어만 추출, 나머지는 공백 처리\n",
        "token_pattern = re.compile(r\"[a-z0-9']+\")\n",
        "def simple_tokenize(text: str):\n",
        "    # (한국어 주석) 소문자 변환 후 정규식 매칭\n",
        "    return token_pattern.findall(text.lower())"
      ],
      "metadata": {
        "id": "-e6CRt2oj8dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어휘사전(Vocab) 구축\n",
        "# 상위 N개 토큰만 사용하여 희소 단어는 <unk> 처리\n",
        "from collections import Counter\n",
        "MAX_VOCAB = 30000\n",
        "PAD, UNK = \"<pad>\", \"<unk>\"\n",
        "\n",
        "# 스페셜 토큰(special token)\n",
        "# PAD : 짧은 문장이 있다면, 길이를 맞추어주기 위해 패딩 토큰을 사용\n",
        "# UNK : unknown word (내가 사용하고 있는 사전에 없는 단어)\n",
        "\n",
        "counter = Counter()\n",
        "for ex in raw[\"train\"]:\n",
        "    counter.update(simple_tokenize(ex[\"text\"]))"
      ],
      "metadata": {
        "id": "iuyBPXpQkBUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "itos_ex = ['<pad>', '<unk>','i','love','you']\n",
        "itos_ex[0]"
      ],
      "metadata": {
        "id": "BKC5aUFBo0od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특수 토큰 포함하여 vocab 생성\n",
        "most_common = counter.most_common(MAX_VOCAB - 2)\n",
        "print(most_common)"
      ],
      "metadata": {
        "id": "dfovZBdgpMfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([t for t, _ in most_common])"
      ],
      "metadata": {
        "id": "uosTB1OnpSj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([PAD, UNK] + [t for t, _ in most_common])"
      ],
      "metadata": {
        "id": "76DhE_NtpfIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# why \"-2\" : PAD, UNK 자리 남겨 놓기 위해서\n",
        "itos = [PAD, UNK] + [t for t, _ in most_common]  # index-to-string (t: token)\n",
        "print(itos)"
      ],
      "metadata": {
        "id": "Hx4INVWXpo1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print({t:i for i, t in enumerate(itos)})"
      ],
      "metadata": {
        "id": "AIvAfRikpwXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {t:i for i, t in enumerate(itos)}         # string-to-index\n",
        "print(stoi)"
      ],
      "metadata": {
        "id": "KfakSydgqKiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX, UNK_IDX = stoi[PAD], stoi[UNK]\n",
        "print(\"어휘 크기:\", len(itos))"
      ],
      "metadata": {
        "id": "tMroZ06skGGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 -> 인덱스 변환 & 패딩/트렁케이션\n",
        "MAX_LEN = 256  # 시퀀스 최대 길이\n",
        "# 모든 리뷰(댓글) 256개 단어 길이로 맞춰줘\n",
        "\n",
        "def encode(text: str):\n",
        "    tokens = simple_tokenize(text)\n",
        "    ids = [stoi.get(tok, UNK_IDX) for tok in tokens][:MAX_LEN]\n",
        "    # 단어가 있으면 >> 인덱스 가져오고\n",
        "    # 단어가 없으면 >> UNK_IDX(1) 반환\n",
        "    if len(ids) < MAX_LEN:\n",
        "        ids += [PAD_IDX] * (MAX_LEN - len(ids))\n",
        "        # PAD_IDX : 0\n",
        "        # MAX_LEN = 256 >> 즉 256 칸 중에서 len(ids)가 4라면 >> 4칸 채우고 252칸 0 채움\n",
        "    return ids\n",
        "\n",
        "def encode_label(y: int):\n",
        "    # IMDb: 0=neg, 1=pos\n",
        "    return int(y)  # 1 : 긍정, 0 : 부정"
      ],
      "metadata": {
        "id": "VuShgy82kKEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Dataset 래핑\n",
        "class IMDBTensor(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_split):\n",
        "        self.data = hf_split\n",
        "    # len() 길이(데이터 개수)\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    # getitem(): 특정 인덱스 데이터 반환\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx][\"text\"] #영화 리뷰 텍스트\n",
        "        label = self.data[idx][\"label\"] # 0 또는 1\n",
        "        x = torch.tensor(encode(text), dtype=torch.long) # 텍스트를 숫자(텐서의 정수)\n",
        "        y = torch.tensor(encode_label(label), dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "# 데이터 셋\n",
        "train_ds = IMDBTensor(raw[\"train\"])\n",
        "test_ds  = IMDBTensor(raw[\"test\"])\n",
        "\n",
        "# 데이터셋을 데이터 로더로 저장\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "test_loader  = DataLoader(test_ds,  batch_size=128, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "n4eS9bF1kNBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의: 임베딩 + 양방향 LSTM + FC\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb=128, hidden=128, num_layers=1, num_classes=2, pad_idx=0, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_idx)\n",
        "        # I love you\n",
        "        # 입력: [45, 250, 120]\n",
        "        # 임베딩 출력(voca_size * embedded_dimension(fixed=128))\n",
        "        # [[0.3, 0.2, 1.2, ..... 0.9]\n",
        "        #  [0.3, 0.2, 1.2, ..... 0.9]\n",
        "        #  [0.3, 0.2, 1.3, .. .0,0.0]]  # 각 128차원(고정)\n",
        "\n",
        "        self.lstm = nn.LSTM(emb, hidden, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=0.0)\n",
        "        # batch_first=True >> batch_size 를 첫 번째 차원으로\n",
        "        # bidirectional (양방향)?\n",
        "        # S1 = \"이 영화 진짜 재미있다\" This movie is great\n",
        "        # 순방향: 이 >> 영화 >> 진짜 >> 재미있다. (This >> movie >> *is* >> great) 앞에서 뒤로\n",
        "        # 역방향 : 재미있다 <<  진짜 << 영화 << 이 great << *is* << movie << this  뒤에서 앞으로\n",
        "        # >> 'not good' : not을 보려면 뒤도 봐야 함 : 맥락을 더 잘 이해함\n",
        "        # This movie ___ ___  good. (인간) / This movie [mask][mask] good. (컴퓨터)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden*2, num_classes)  # 양방향 → 2배\n",
        "        # 입력 : 256 (128 forward + 128 backward)\n",
        "        # num_classes : 2 (부정/긍정)\n",
        "        # 가중치 초기화(Kaiming 초기화)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "                # bias 는 0으로 초기화\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T) (배치, 시퀀스 길이) (64, 256)\n",
        "        e = self.emb(x)                 # (B, T, E) (64, 256, 128)\n",
        "        out, (h, c) = self.lstm(e)      # h: (num_layers*2, B, H) (2,64,128)\n",
        "        # out : 각각의 시점(time step) 의 출력값 (사용 안함)\n",
        "        # (h,c) : h(최종 hidden state **) c: cell state(내부기억, 사용안함)\n",
        "        # 마지막 레이어의 forward/backward hidden state 결합\n",
        "        # h = [ [순방향 layer], [역방향 layer]] >> h[0] =순방향 layer h[1]= 역방향 layer\n",
        "        last_f = h[-2]                  # (B, H) (64, 128) 순방향 마지막\n",
        "        last_b = h[-1]                  # (B, H) (64, 128) 역방향 마지막\n",
        "        h_cat = torch.cat([last_f, last_b], dim=1)  # (B, 2H) # (64, 256)\n",
        "        h_cat = self.dropout(h_cat)\n",
        "        logits = self.fc(h_cat)         # (B, 2) (64,2)\n",
        "        # 예) [[0.1. 1.5]]\n",
        "        return logits\n",
        "\n",
        "model = BiLSTM(len(itos), emb=128, hidden=128, num_layers=1, pad_idx=PAD_IDX).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "PW1xPQTtkUAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습/평가 루프\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    total_loss = total_correct = total = 0\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        # y.size(0) : batch_size\n",
        "        total_correct += (logits.argmax(1) == y).sum().item()\n",
        "        # logits.argmax(1)\n",
        "        # logits = [[0.5,1.5],[2.0,-0.5]]\n",
        "        # argmax = [1,0] # 각 행에서 가장 큰 값의 인덱스\n",
        "        total += y.size(0)\n",
        "    print(f\"[Train] Epoch {epoch} | loss={total_loss/total:.4f} | acc={total_correct/total:.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    total_loss = total_correct = total = 0\n",
        "    for X, y in test_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        logits = model(X)\n",
        "        loss = criterion(logits, y)\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        total_correct += (logits.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    print(f\"[Test ] loss={total_loss/total:.4f} | acc={total_correct/total:.4f}\")\n",
        "\n",
        "EPOCHS = 3  # 2~3 에폭 권장\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    train_one_epoch(ep)\n",
        "    evaluate()\n"
      ],
      "metadata": {
        "id": "fq4pl9bRpZFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EOS"
      ],
      "metadata": {
        "id": "ewPtLoJspSEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KqHkmvhnkl6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
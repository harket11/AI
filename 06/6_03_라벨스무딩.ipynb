{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU3TDUR9ta_g"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Part 3: 라벨 스무딩 (Label Smoothing)\n",
        "- 라벨 스무딩의 효과\n",
        "- 과신(Overconfidence) 방지\n",
        "- 일반화 성능 향상\n",
        "\n",
        "독립적으로 실행 가능합니다.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# 재현성을 위한 시드 고정\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Part 3: 라벨 스무딩 (Label Smoothing)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 실습 3-1: 라벨 스무딩 개념 이해\n",
        "# =====================================================================\n",
        "print(\"\\n[실습 3-1] 라벨 스무딩 개념\")\n",
        "\n",
        "# 원래 레이블 (3개 클래스)\n",
        "original_label = torch.tensor([1, 0, 0], dtype=torch.float)\n",
        "print(\"원래 레이블 (One-hot):\", original_label.numpy())\n",
        "print(\"  의미: 클래스 0이 100% 정답\")\n",
        "\n",
        "# 라벨 스무딩 적용\n",
        "smoothing = 0.1\n",
        "n_classes = 3\n",
        "\n",
        "# 스무딩 공식: y_smooth = y * (1 - alpha) + alpha / K(클래스 수)\n",
        "smooth_value = smoothing / n_classes\n",
        "smooth_label = original_label * (1 - smoothing) + smooth_value\n",
        "\n",
        "print(f\"\\n라벨 스무딩 (alpha={smoothing}) 적용 후:\")\n",
        "print(smooth_label.numpy())\n",
        "print(f\"  의미: 클래스 0이 {smooth_label[0]*100:.1f}% 정답\")\n",
        "print(f\"       나머지 클래스도 각각 {smooth_label[1]*100:.1f}% 가능성\")\n",
        "\n",
        "print(\"\\n효과:\")\n",
        "print(\"  모델이 100% 확신하지 않도록 함\")\n",
        "print(\"  과신(Overconfidence) 방지\")\n",
        "print(\"  일반화 능력 향상\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 실습 3-2: 라벨 스무딩 구현\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[실습 3-2] 라벨 스무딩 구현\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    라벨 스무딩이 적용된 CrossEntropyLoss\n",
        "    \"\"\"\n",
        "    def __init__(self, n_classes, smoothing=0.1):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \"\"\"\n",
        "        pred: (batch_size, n_classes) - 모델의 logits\n",
        "        target: (batch_size,) - 클래스 인덱스\n",
        "        \"\"\"\n",
        "        # Log Softmax\n",
        "        log_probs = torch.log_softmax(pred, dim=1)\n",
        "\n",
        "        # 타겟을 원-핫으로 변환\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(log_probs)\n",
        "            true_dist.fill_(self.smoothing / (self.n_classes - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "\n",
        "        # KL Divergence 계산\n",
        "        # 확률분포 2개가 있어요. 이 두개가 얼마나 다르지? (차이)가 있는지 확인하는 계산\n",
        "        # 즉, 진짜 존재하는 분포(P), 모델이 예측한 분포(Q)\n",
        "        # 모델이 예측한 분포(Q)가 얼마나 실제 분포(P)와 유사한가\n",
        "        # Q가 P 에서 얼마나 벗어났나? 확인\n",
        "        # CrossEntropyLoss = NLLLoss + KL Divergence\n",
        "        loss = torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n",
        "\n",
        "        return loss\n",
        "\n",
        "# 테스트\n",
        "print(\"\\n라벨 스무딩 손실 함수 테스트:\")\n",
        "\n",
        "# 예시 데이터\n",
        "batch_logits = torch.tensor([\n",
        "    [2.0, 0.5, 0.1],  # 클래스 0 예측\n",
        "    [0.3, 2.5, 0.2],  # 클래스 1 예측\n",
        "])\n",
        "batch_targets = torch.tensor([0, 1])\n",
        "\n",
        "# 일반 CrossEntropy\n",
        "ce_normal = nn.CrossEntropyLoss()\n",
        "loss_normal = ce_normal(batch_logits, batch_targets)\n",
        "\n",
        "# 라벨 스무딩 CrossEntropy\n",
        "ce_smooth = LabelSmoothingCrossEntropy(n_classes=3, smoothing=0.1)\n",
        "loss_smooth = ce_smooth(batch_logits, batch_targets)\n",
        "\n",
        "print(f\"일반 CrossEntropy: {loss_normal.item():.4f}\")\n",
        "print(f\"라벨 스무딩 CrossEntropy: {loss_smooth.item():.4f}\")\n",
        "print(f\"차이: {(loss_smooth - loss_normal).item():.4f}\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 실습 3-3: 데이터 준비 및 모델 정의\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[실습 3-3] 실험 데이터 준비\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 5개 클래스 분류 데이터\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=15,\n",
        "    n_classes=5, n_clusters_per_class=1, random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train_t = torch.FloatTensor(X_train)\n",
        "y_train_t = torch.LongTensor(y_train)\n",
        "X_test_t = torch.FloatTensor(X_test)\n",
        "y_test_t = torch.LongTensor(y_test)\n",
        "\n",
        "print(f\"훈련 데이터: {X_train.shape}\")\n",
        "print(f\"테스트 데이터: {X_test.shape}\")\n",
        "print(f\"클래스 수: 5\")\n",
        "\n",
        "\n",
        "# 분류 모델 정의\n",
        "class Classifier(nn.Module):\n",
        "    \"\"\"5개 클래스 분류 모델\"\"\"\n",
        "    def __init__(self, n_classes=5):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(20, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 실습 3-4: 일반 vs 라벨 스무딩 학습 비교\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[실습 3-4] 일반 vs 라벨 스무딩 학습 비교\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 실험 설정\n",
        "smoothing_values = [0.0, 0.05, 0.1, 0.2]\n",
        "results = {}\n",
        "\n",
        "for smoothing in smoothing_values:\n",
        "    if smoothing == 0.0:\n",
        "        print(f\"\\n라벨 스무딩 없음 학습 중...\")\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "        print(f\"\\n라벨 스무딩 alpha={smoothing} 학습 중...\")\n",
        "        criterion = LabelSmoothingCrossEntropy(n_classes=5, smoothing=smoothing)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = Classifier(n_classes=5)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # 학습\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "\n",
        "    num_epochs = 100\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(X_train_t)\n",
        "        loss = criterion(output, y_train_t)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # 훈련 정확도\n",
        "        with torch.no_grad():\n",
        "            train_pred = torch.argmax(output, dim=1)\n",
        "            train_acc = accuracy_score(y_train, train_pred.numpy())\n",
        "            train_accs.append(train_acc)\n",
        "\n",
        "    # 테스트\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_output = model(X_test_t)\n",
        "        test_pred = torch.argmax(test_output, dim=1)\n",
        "        test_probs = torch.softmax(test_output, dim=1)\n",
        "\n",
        "        test_acc = accuracy_score(y_test, test_pred.numpy())\n",
        "\n",
        "        # 예측 확신도 (최대 확률의 평균)\n",
        "        max_probs = torch.max(test_probs, dim=1)[0]\n",
        "        avg_confidence = max_probs.mean().item()\n",
        "\n",
        "    results[smoothing] = {\n",
        "        'train_losses': train_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'test_acc': test_acc,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'test_probs': test_probs\n",
        "    }\n",
        "\n",
        "    print(f\"  최종 훈련 손실: {train_losses[-1]:.4f}\")\n",
        "    print(f\"  최종 훈련 정확도: {train_accs[-1]:.4f}\")\n",
        "    print(f\"  테스트 정확도: {test_acc:.4f}\")\n",
        "    print(f\"  평균 확신도: {avg_confidence:.4f}\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 실습 3-5: 결과 분석 및 시각화\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[실습 3-5] 결과 분석\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n성능 요약:\")\n",
        "print(f\"{'Alpha':>10} {'Train Acc':>12} {'Test Acc':>12} {'Confidence':>12}\")\n",
        "print(\"-\" * 50)\n",
        "for smoothing in smoothing_values:\n",
        "    result = results[smoothing]\n",
        "    print(f\"{smoothing:10.2f} {result['train_accs'][-1]:12.4f} \"\n",
        "          f\"{result['test_acc']:12.4f} {result['avg_confidence']:12.4f}\")\n",
        "\n",
        "print(\"\\n관찰:\")\n",
        "print(\"  1. 라벨 스무딩 사용 시 테스트 정확도 향상 가능\")\n",
        "print(\"  2. 평균 확신도는 감소 (과신 방지)\")\n",
        "print(\"  3. alpha가 너무 크면 (0.2) 성능 저하 가능\")\n",
        "\n",
        "# 시각화\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. 학습 곡선 비교\n",
        "ax1 = axes[0, 0]\n",
        "colors = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']\n",
        "for smoothing, color in zip(smoothing_values, colors):\n",
        "    losses = results[smoothing]['train_losses']\n",
        "    ax1.plot(losses, label=f'alpha={smoothing}', linewidth=2, color=color)\n",
        "\n",
        "ax1.set_xlabel('Epoch', fontsize=11)\n",
        "ax1.set_ylabel('Training Loss', fontsize=11)\n",
        "ax1.set_title('Training Loss with Different Smoothing',\n",
        "              fontsize=12, weight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# 2. 정확도 비교\n",
        "ax2 = axes[0, 1]\n",
        "train_accs_final = [results[s]['train_accs'][-1] for s in smoothing_values]\n",
        "test_accs = [results[s]['test_acc'] for s in smoothing_values]\n",
        "\n",
        "x_pos = np.arange(len(smoothing_values))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax2.bar(x_pos - width/2, train_accs_final, width,\n",
        "                label='Train', color='#3498db', edgecolor='black', alpha=0.7)\n",
        "bars2 = ax2.bar(x_pos + width/2, test_accs, width,\n",
        "                label='Test', color='#2ecc71', edgecolor='black', alpha=0.7)\n",
        "\n",
        "ax2.set_ylabel('Accuracy', fontsize=11)\n",
        "ax2.set_title('Accuracy Comparison', fontsize=12, weight='bold')\n",
        "ax2.set_xticks(x_pos)\n",
        "ax2.set_xticklabels([f'{s}' for s in smoothing_values])\n",
        "ax2.set_xlabel('Smoothing Alpha', fontsize=11)\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "ax2.set_ylim(0.5, 1.0)\n",
        "\n",
        "# 값 표시\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                 f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# 3. 확신도 비교\n",
        "ax3 = axes[1, 0]\n",
        "confidences = [results[s]['avg_confidence'] for s in smoothing_values]\n",
        "\n",
        "bars = ax3.bar([f'{s}' for s in smoothing_values], confidences,\n",
        "               color=colors, edgecolor='black', alpha=0.7)\n",
        "ax3.set_xlabel('Smoothing Alpha', fontsize=11)\n",
        "ax3.set_ylabel('Average Confidence', fontsize=11)\n",
        "ax3.set_title('Model Confidence', fontsize=12, weight='bold')\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "ax3.set_ylim(0, 1.0)\n",
        "\n",
        "# 값 표시\n",
        "for bar, conf in zip(bars, confidences):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{conf:.3f}', ha='center', va='bottom', fontsize=9, weight='bold')\n",
        "\n",
        "# 4. 확률 분포 비교 (첫 10개 샘플)\n",
        "ax4 = axes[1, 1]\n",
        "\n",
        "# alpha=0.0과 alpha=0.1 비교\n",
        "probs_no_smooth = results[0.0]['test_probs'][:10].numpy()\n",
        "probs_smooth = results[0.1]['test_probs'][:10].numpy()\n",
        "\n",
        "sample_indices = range(10)\n",
        "for i in sample_indices:\n",
        "    max_prob_no = probs_no_smooth[i].max()\n",
        "    max_prob_smooth = probs_smooth[i].max()\n",
        "\n",
        "    ax4.plot([i-0.1, i+0.1], [max_prob_no, max_prob_smooth],\n",
        "             'o-', linewidth=2, markersize=8)\n",
        "\n",
        "ax4.axhline(y=1.0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
        "ax4.set_xlabel('Sample Index', fontsize=11)\n",
        "ax4.set_ylabel('Max Probability', fontsize=11)\n",
        "ax4.set_title('Confidence per Sample\\n(Left: No Smooth, Right: Smooth)',\n",
        "              fontsize=12, weight='bold')\n",
        "ax4.set_xticks(sample_indices)\n",
        "ax4.grid(alpha=0.3)\n",
        "ax4.set_ylim(0.5, 1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('part3_label_smoothing.png', dpi=150, bbox_inches='tight')\n",
        "print(\"\\n저장: part3_label_smoothing.png\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 실습 3-6: PyTorch 내장 라벨 스무딩 사용\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[실습 3-6] PyTorch 내장 라벨 스무딩 (PyTorch 1.10+)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# PyTorch 1.10 이상에서는 CrossEntropyLoss에 label_smoothing 파라미터 제공\n",
        "try:\n",
        "    criterion_builtin = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    print(\"\\nPyTorch 내장 라벨 스무딩 사용 가능\")\n",
        "    print(\"사용법:\")\n",
        "    print(\"  criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\")\n",
        "\n",
        "    # 간단한 테스트\n",
        "    test_logits = torch.randn(5, 3)\n",
        "    test_targets = torch.tensor([0, 1, 2, 0, 1])\n",
        "    loss = criterion_builtin(test_logits, test_targets)\n",
        "    print(f\"\\n테스트 손실: {loss.item():.4f}\")\n",
        "\n",
        "except TypeError:\n",
        "    print(\"\\nPyTorch 버전이 1.10 미만입니다.\")\n",
        "    print(\"위에서 구현한 LabelSmoothingCrossEntropy 클래스를 사용하세요.\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 최종 요약\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Part 3 완료\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n핵심 개념:\")\n",
        "print(\"\\n1. 라벨 스무딩\")\n",
        "print(\"   - 정답 레이블을 부드럽게 만듦\")\n",
        "print(\"   - [1, 0, 0] -> [0.93, 0.03, 0.03]\")\n",
        "print(\"   - 과신(Overconfidence) 방지\")\n",
        "\n",
        "print(\"\\n2. 효과\")\n",
        "print(\"   - 일반화 성능 향상\")\n",
        "print(\"   - 과적합 방지\")\n",
        "print(\"   - 모델 캘리브레이션 개선\")\n",
        "\n",
        "print(\"\\n3. 하이퍼파라미터\")\n",
        "print(\"   - alpha = 0.0: 스무딩 없음\")\n",
        "print(\"   - alpha = 0.1: 표준 설정 (권장)\")\n",
        "print(\"   - alpha = 0.2: 강한 스무딩 (주의)\")\n",
        "\n",
        "print(\"\\n실전 가이드:\")\n",
        "print(\"  - 대규모 데이터셋: alpha=0.1\")\n",
        "print(\"  - 작은 데이터셋: 사용 안 함 또는 alpha=0.05\")\n",
        "print(\"  - 과적합 문제: alpha=0.1~0.15\")\n",
        "print(\"  - PyTorch 1.10+: label_smoothing 파라미터 사용\")\n",
        "\n",
        "print(\"\\n생성된 파일:\")\n",
        "print(\"  part3_label_smoothing.png - 라벨 스무딩 효과\")\n",
        "\n",
        "print(\"\\n다음: Part 4 - 클래스 불균형 대응\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w959fkTVvbH"
      },
      "outputs": [],
      "source": [
        "# 6차시 보강 실습: 손실함수 비교 (Colab 호환)\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# (A) Binary: MSE vs BCEWithLogits\n",
        "X, y = make_classification(n_samples=4000, n_features=20, n_informative=8, weights=[0.6, 0.4], random_state=0)\n",
        "Xtr, Xte, ytr, yte = train_test_split(torch.tensor(X, dtype=torch.float32),\n",
        "                                      torch.tensor(y, dtype=torch.float32).unsqueeze(1), test_size=0.3, random_state=0)\n",
        "\n",
        "class BinNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.m = nn.Sequential(nn.Linear(20,64), nn.ReLU(), nn.Linear(64,1))\n",
        "    def forward(self,x): return self.m(x)\n",
        "\n",
        "def run_binary(loss_fn):\n",
        "    net = BinNet().to(device)\n",
        "    opt = optim.AdamW(net.parameters(), lr=1e-3)\n",
        "    for _ in range(8):\n",
        "        net.train(); opt.zero_grad()\n",
        "        out = net(Xtr.to(device))           # 훈련(학습)\n",
        "        loss = loss_fn(out, ytr.to(device)) # 손실계산\n",
        "        loss.backward(); opt.step()\n",
        "    with torch.no_grad():\n",
        "        p = torch.sigmoid(net(Xte.to(device)))\n",
        "        acc = ((p>0.5).float()==yte.to(device)).float().mean().item()\n",
        "    return acc\n",
        "\n",
        "acc_mse = run_binary(lambda out, y: nn.MSELoss()(torch.sigmoid(out), y))\n",
        "acc_bce = run_binary(nn.BCEWithLogitsLoss())\n",
        "print(f\"Binary Acc: MSE={acc_mse:.3f} vs BCEWithLogits={acc_bce:.3f}\")\n",
        "\n",
        "# (B) Multiclass: Label Smoothing & Class Weights\n",
        "tfm = transforms.Compose([transforms.ToTensor()]) # 전처리 #예) ex = float(string-value)\n",
        "train_ds = datasets.MNIST('/tmp/mnist2', train=True,  download=True, transform=tfm) #60,000 images\n",
        "test_ds  = datasets.MNIST('/tmp/mnist2', train=False, download=True, transform=tfm) #10,000 images\n",
        "tr = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "te = DataLoader(test_ds,  batch_size=512, shuffle=False)\n",
        "\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # feature extraction (특징 추출)\n",
        "            nn.Conv2d(1,16,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            # 컴퓨터에게 보내기 위해 1차원으로 만들어줌\n",
        "            nn.Flatten(), nn.Linear(32*7*7, 128), nn.ReLU(), nn.Linear(128,10)\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "def train_eval(criterion):\n",
        "    model = SmallCNN().to(device)\n",
        "    opt = optim.AdamW(model.parameters(), lr=2e-3)\n",
        "    for _ in range(3):\n",
        "        model.train()\n",
        "        for x,y in tr:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = criterion(model(x), y)\n",
        "            loss.backward(); opt.step()\n",
        "    model.eval(); correct=0; tot=0\n",
        "    with torch.no_grad():\n",
        "        for x,y in te:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            pred = model(x).argmax(1)\n",
        "            correct += (pred==y).sum().item(); tot += y.size(0)\n",
        "    return correct/tot\n",
        "\n",
        "# Label smoothing\n",
        "crit_ls = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "acc_ls = train_eval(crit_ls)\n",
        "\n",
        "# Class weight 예시(0~9 가중치 다르게, 임의)\n",
        "weights = torch.tensor([1,1,1,1,1,1.2,1,1.2,1,1.2], dtype=torch.float32).to(device)\n",
        "crit_w = nn.CrossEntropyLoss(weight=weights)\n",
        "acc_w = train_eval(crit_w)\n",
        "\n",
        "print(f\"MNIST Acc: LabelSmoothing={acc_ls:.3f} | WeightedCE={acc_w:.3f}\")\n"
      ]
    }
  ]
}
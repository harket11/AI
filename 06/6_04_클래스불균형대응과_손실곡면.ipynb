{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y fonts-nanum* | tail -n 1\n",
        "!sudo fc-cache -fv\n",
        "!rm -rf ~/.cache/matplotlib"
      ],
      "metadata": {
        "id": "oFH8fMU1Hef9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요 라이브러리 설치\n",
        "\n",
        "!pip install torchviz | tail -n 1\n",
        "!pip install torchinfo | tail -n 1"
      ],
      "metadata": {
        "id": "Qf9mPxBwHoaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "세션 다시 시작"
      ],
      "metadata": {
        "id": "-vMt9UTrHrXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 임포트\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "# 폰트 관련 용도\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 나눔 고딕 폰트의 경로 명시\n",
        "path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "font_name = fm.FontProperties(fname=path, size=10).get_name()"
      ],
      "metadata": {
        "id": "hodqk9LdHpQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDXF5cRvWJuW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "6차시 통합 실행 파일: 손실 함수\n",
        "모든 실습을 한 번에 실행할 수 있습니다.\n",
        "\n",
        "Part 1: 회귀 손실함수 (MSE, MAE, Huber)\n",
        "Part 2: 분류 손실함수 (BCE, CrossEntropy)\n",
        "Part 3: 라벨 스무딩\n",
        "Part 4: 클래스 불균형 대응\n",
        "Part 5: 손실 곡면\n",
        "\n",
        "필수 라이브러리:\n",
        "pip install torch numpy matplotlib seaborn scikit-learn\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
        "\n",
        "# 재현성\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"6차시 통합 실습: 손실 함수\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# Part 1: 회귀 손실함수\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 1] 회귀 손실함수\")\n",
        "\n",
        "# 간단한 예시로 손실함수 이해\n",
        "y_true = torch.tensor([10.0, 20.0, 30.0, 40.0])\n",
        "y_pred = torch.tensor([12.0, 19.0, 35.0, 38.0])\n",
        "\n",
        "mse = nn.MSELoss()(y_pred, y_true)\n",
        "mae = nn.L1Loss()(y_pred, y_true)  # MAE\n",
        "huber = nn.HuberLoss()(y_pred, y_true)\n",
        "\n",
        "print(f\"\\nMSE:   {mse.item():.4f} - 큰 오차에 민감\")\n",
        "print(f\"MAE:   {mae.item():.4f} - 모든 오차 동등\")\n",
        "print(f\"Huber: {huber.item():.4f} - MSE와 MAE의 절충\")\n",
        "\n",
        "# 이상치 포함 시\n",
        "y_pred_outlier = torch.tensor([12.0, 19.0, 100.0, 38.0])  # 하나의 큰 오차\n",
        "\n",
        "mse_out = nn.MSELoss()(y_pred_outlier, y_true)\n",
        "mae_out = nn.L1Loss()(y_pred_outlier, y_true)\n",
        "\n",
        "print(f\"\\n이상치 포함 시:\")\n",
        "print(f\"MSE: {mse.item():.4f} -> {mse_out.item():.4f} ({mse_out/mse:.1f}배 증가)\")\n",
        "print(f\"MAE: {mae.item():.4f} -> {mae_out.item():.4f} ({mae_out/mae:.1f}배 증가)\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# Part 2: 분류 손실함수\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 2] 분류 손실함수\")\n",
        "\n",
        "# BCE 예시\n",
        "y_true_bin = torch.tensor([1.0, 0.0, 1.0, 0.0])\n",
        "y_pred_conf = torch.tensor([0.9, 0.1, 0.85, 0.15])  # 확신있는 예측\n",
        "y_pred_unce = torch.tensor([0.6, 0.4, 0.55, 0.45])  # 불확실한 예측\n",
        "\n",
        "bce = nn.BCELoss()\n",
        "loss_conf = bce(y_pred_conf, y_true_bin)\n",
        "loss_unce = bce(y_pred_unce, y_true_bin)\n",
        "\n",
        "print(f\"\\nBCE 손실:\")\n",
        "print(f\"확신 있는 예측: {loss_conf.item():.4f}\")\n",
        "print(f\"불확실한 예측: {loss_unce.item():.4f}\")\n",
        "print(\"확신 있을수록 손실 감소\")\n",
        "\n",
        "# BCEWithLogits vs BCE\n",
        "print(\"\\n중요: BCEWithLogitsLoss 사용 권장\")\n",
        "print(\"  - 수치적 안정성\")\n",
        "print(\"  - 출력층에 Sigmoid 제거\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# Part 3: 라벨 스무딩 간단 비교\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 3] 라벨 스무딩\")\n",
        "\n",
        "# 분류 데이터\n",
        "X, y = make_classification(n_samples=500, n_features=20,\n",
        "                          n_classes=3, n_informative=15, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train_t = torch.FloatTensor(X_train)\n",
        "y_train_t = torch.LongTensor(y_train)\n",
        "\n",
        "# 간단한 모델\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(20, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 32), nn.ReLU(),\n",
        "            nn.Linear(32, n_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# 일반 vs 라벨 스무딩\n",
        "print(\"\\n라벨 스무딩 비교:\")\n",
        "\n",
        "# 일반\n",
        "model_no_smooth = SimpleNet(3)\n",
        "criterion_no_smooth = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_no_smooth.parameters(), lr=0.01)\n",
        "\n",
        "for _ in range(30):\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion_no_smooth(model_no_smooth(X_train_t), y_train_t)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# 라벨 스무딩 (PyTorch 1.10+)\n",
        "try:\n",
        "    model_smooth = SimpleNet(3)\n",
        "    criterion_smooth = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.Adam(model_smooth.parameters(), lr=0.01)\n",
        "\n",
        "    for _ in range(30):\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion_smooth(model_smooth(X_train_t), y_train_t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"  라벨 스무딩 (alpha=0.1) 적용 가능\")\n",
        "    print(\"  효과: 과신 방지, 일반화 향상\")\n",
        "except:\n",
        "    print(\"  PyTorch 1.10 미만: label_smoothing 파라미터 없음\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# Part 4: 클래스 불균형\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 4] 클래스 불균형 대응\")\n",
        "\n",
        "# 불균형 데이터 (95:5)\n",
        "X_imb, y_imb = make_classification(\n",
        "    n_samples=500, n_features=20,\n",
        "    weights=[0.95, 0.05], random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\n불균형 데이터:\")\n",
        "print(f\"  클래스 0: {np.sum(y_imb==0)}개 ({np.sum(y_imb==0)/len(y_imb)*100:.0f}%)\")\n",
        "print(f\"  클래스 1: {np.sum(y_imb==1)}개 ({np.sum(y_imb==1)/len(y_imb)*100:.0f}%)\")\n",
        "\n",
        "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n",
        "    X_imb, y_imb, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_i = StandardScaler()\n",
        "X_train_i = scaler_i.fit_transform(X_train_i)\n",
        "X_test_i = scaler_i.transform(X_test_i)\n",
        "\n",
        "X_train_i_t = torch.FloatTensor(X_train_i)\n",
        "y_train_i_t = torch.FloatTensor(y_train_i).unsqueeze(1)\n",
        "X_test_i_t = torch.FloatTensor(X_test_i)\n",
        "\n",
        "# 가중치 계산\n",
        "n_0 = np.sum(y_train_i == 0)\n",
        "n_1 = np.sum(y_train_i == 1)\n",
        "weight = n_0 / n_1\n",
        "\n",
        "print(f\"\\n클래스 1 가중치: {weight:.1f}\")\n",
        "\n",
        "# Weighted BCE\n",
        "class BinClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(20, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 32), nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model_weighted = BinClassifier()\n",
        "criterion_weighted = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([weight]))\n",
        "optimizer = optim.Adam(model_weighted.parameters(), lr=0.001)\n",
        "\n",
        "for _ in range(50):\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion_weighted(model_weighted(X_train_i_t), y_train_i_t)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# 평가\n",
        "model_weighted.eval()\n",
        "with torch.no_grad():\n",
        "    pred = torch.sigmoid(model_weighted(X_test_i_t))\n",
        "    pred_bin = (pred > 0.5).float().numpy()\n",
        "\n",
        "f1 = f1_score(y_test_i, pred_bin, zero_division=0)\n",
        "print(f\"\\nWeighted BCE F1-Score: {f1:.4f}\")\n",
        "print(\"소수 클래스 탐지 개선!\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# Part 5: 손실 곡면 개념\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 5] 손실 곡면\")\n",
        "\n",
        "print(\"\\n손실 곡면 (Loss Landscape):\")\n",
        "print(\"  - 가중치 공간에서의 손실값 분포\")\n",
        "print(\"  - 지형의 모양이 학습에 영향\")\n",
        "\n",
        "print(\"\\n좋은 손실 곡면:\")\n",
        "print(\"  1. 부드러운 표면 (Smooth)\")\n",
        "print(\"  2. 넓은 최소값 (Wide Minimum)\")\n",
        "print(\"  3. 적은 지역 최소값\")\n",
        "\n",
        "print(\"\\n안정적 학습 방법:\")\n",
        "print(\"  1. 적절한 초기화 (He/Xavier)\")\n",
        "print(\"  2. BatchNorm 사용\")\n",
        "print(\"  3. 적절한 학습률\")\n",
        "print(\"  4. Gradient Clipping\")\n",
        "\n",
        "# 간단한 시각화\n",
        "def simple_loss(w1, w2):\n",
        "    return w1**2 + w2**2 + 0.5 * np.sin(w1*3) * np.cos(w2*3)\n",
        "\n",
        "w = np.linspace(-2, 2, 50)\n",
        "W1, W2 = np.meshgrid(w, w)\n",
        "Z = simple_loss(W1, W2)\n",
        "\n",
        "print(\"\\n간단한 손실 곡면 생성...\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 통합 시각화\n",
        "# =====================================================================\n",
        "print(\"\\n[통합 시각화 생성 중...]\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "\n",
        "# 1. 회귀 손실함수 비교\n",
        "ax1 = axes[0, 0]\n",
        "errors = np.linspace(-5, 5, 100)\n",
        "mse_vals = 0.5 * errors**2\n",
        "mae_vals = np.abs(errors)\n",
        "huber_vals = []\n",
        "for e in errors:\n",
        "    if abs(e) <= 1.0:\n",
        "        huber_vals.append(0.5 * e**2)\n",
        "    else:\n",
        "        huber_vals.append(abs(e) - 0.5)\n",
        "\n",
        "ax1.plot(errors, mse_vals, label='MSE', linewidth=2, color='#e74c3c')\n",
        "ax1.plot(errors, mae_vals, label='MAE', linewidth=2, color='#3498db')\n",
        "ax1.plot(errors, huber_vals, label='Huber', linewidth=2, color='#2ecc71')\n",
        "ax1.set_xlabel('Prediction Error')\n",
        "ax1.set_ylabel('Loss Value')\n",
        "ax1.set_title('Regression Loss Functions', weight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "ax1.set_ylim(0, 10)\n",
        "\n",
        "# 2. BCE 동작\n",
        "ax2 = axes[0, 1]\n",
        "probs = np.linspace(0.01, 0.99, 100)\n",
        "bce_true1 = -np.log(probs)\n",
        "bce_true0 = -np.log(1 - probs)\n",
        "\n",
        "ax2.plot(probs, bce_true1, label='True=1', linewidth=2, color='#2ecc71')\n",
        "ax2.plot(probs, bce_true0, label='True=0', linewidth=2, color='#e74c3c')\n",
        "ax2.set_xlabel('Predicted Probability')\n",
        "ax2.set_ylabel('BCE Loss')\n",
        "ax2.set_title('Binary Cross Entropy', weight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=0.3)\n",
        "ax2.set_ylim(0, 5)\n",
        "\n",
        "# 3. 라벨 스무딩 효과\n",
        "ax3 = axes[0, 2]\n",
        "smoothing_alphas = [0.0, 0.05, 0.1, 0.15, 0.2]\n",
        "test_accs = [0.85, 0.87, 0.88, 0.87, 0.85]  # 예시\n",
        "ax3.plot(smoothing_alphas, test_accs, 'o-', linewidth=2,\n",
        "         markersize=8, color='#3498db')\n",
        "ax3.set_xlabel('Smoothing Alpha')\n",
        "ax3.set_ylabel('Test Accuracy')\n",
        "ax3.set_title('Label Smoothing Effect', weight='bold')\n",
        "ax3.grid(alpha=0.3)\n",
        "ax3.set_ylim(0.8, 0.9)\n",
        "\n",
        "# 4. 클래스 불균형 전략\n",
        "ax4 = axes[1, 0]\n",
        "methods = ['Baseline', 'Weighted\\nCE', 'Focal\\nLoss']\n",
        "f1_scores = [0.20, 0.65, 0.72]  # 예시\n",
        "colors = ['#e74c3c', '#f39c12', '#2ecc71']\n",
        "\n",
        "bars = ax4.bar(methods, f1_scores, color=colors, edgecolor='black', alpha=0.7)\n",
        "ax4.set_ylabel('F1-Score')\n",
        "ax4.set_title('Imbalance Handling', weight='bold')\n",
        "ax4.set_ylim(0, 1.0)\n",
        "ax4.grid(axis='y', alpha=0.3)\n",
        "\n",
        "for bar, f1 in zip(bars, f1_scores):\n",
        "    height = bar.get_height()\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{f1:.2f}', ha='center', va='bottom', fontsize=10, weight='bold')\n",
        "\n",
        "# 5. 손실 곡면 (등고선)\n",
        "ax5 = axes[1, 1]\n",
        "contour = ax5.contourf(W1, W2, Z, levels=15, cmap='viridis')\n",
        "ax5.contour(W1, W2, Z, levels=15, colors='white', linewidths=0.5, alpha=0.3)\n",
        "ax5.set_xlabel('Weight 1')\n",
        "ax5.set_ylabel('Weight 2')\n",
        "ax5.set_title('Loss Landscape', weight='bold')\n",
        "\n",
        "# 6. 요약\n",
        "ax6 = axes[1, 2]\n",
        "ax6.axis('off')\n",
        "\n",
        "summary = \"\"\"\n",
        "손실 함수 요약\n",
        "\n",
        "회귀:\n",
        " MSE  - 표준, 이상치 민감\n",
        " MAE  - 강건, 해석 쉬움\n",
        " Huber- 절충안\n",
        "\n",
        "분류:\n",
        " BCE  - 이진 분류\n",
        " CE   - 다중 분류\n",
        " Logits 버전 사용!\n",
        "\n",
        "고급:\n",
        " 스무딩 - 과신 방지\n",
        " 가중치 - 불균형 대응\n",
        " Focal  - 극심한 불균형\n",
        "\n",
        "손실 곡면:\n",
        " 부드러움 = 안정적\n",
        " 초기화 + BatchNorm\n",
        "\"\"\"\n",
        "\n",
        "ax6.text(0.1, 0.9, summary, transform=ax6.transAxes,\n",
        "         fontsize=9, verticalalignment='top',\n",
        "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
        "         family='monospace')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('loss_all_summary.png', dpi=150, bbox_inches='tight')\n",
        "print(\"저장: loss_all_summary.png\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 최종 요약\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"6차시 전체 실습 완료\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n핵심 요약:\")\n",
        "\n",
        "print(\"\\n1. 회귀 손실함수\")\n",
        "print(\"   MSE: 큰 오차 페널티\")\n",
        "print(\"   MAE: 이상치에 강건\")\n",
        "print(\"   Huber: 둘의 장점\")\n",
        "\n",
        "print(\"\\n2. 분류 손실함수\")\n",
        "print(\"   BCE/BCEWithLogits: 이진\")\n",
        "print(\"   CrossEntropy: 다중\")\n",
        "print(\"   Logits 버전 권장\")\n",
        "\n",
        "print(\"\\n3. 라벨 스무딩\")\n",
        "print(\"   alpha=0.1 표준\")\n",
        "print(\"   과신 방지\")\n",
        "print(\"   일반화 향상\")\n",
        "\n",
        "print(\"\\n4. 클래스 불균형\")\n",
        "print(\"   Weighted CE: 일반\")\n",
        "print(\"   Focal Loss: 극심\")\n",
        "print(\"   Recall 중시\")\n",
        "\n",
        "print(\"\\n5. 손실 곡면\")\n",
        "print(\"   부드러운 곡면\")\n",
        "print(\"   적절한 초기화\")\n",
        "print(\"   BatchNorm 사용\")\n",
        "\n",
        "print(\"\\n실전 체크리스트:\")\n",
        "print(\"  [v] 회귀: MSE 또는 Huber\")\n",
        "print(\"  [v] 이진: BCEWithLogitsLoss\")\n",
        "print(\"  [v] 다중: CrossEntropyLoss\")\n",
        "print(\"  [v] 대규모: label_smoothing=0.1\")\n",
        "print(\"  [v] 불균형: pos_weight 설정\")\n",
        "print(\"  [v] 극심한 불균형: Focal Loss\")\n",
        "print(\"  [v] 안정성: He 초기화 + BatchNorm\")\n",
        "\n",
        "print(\"\\n생성된 파일:\")\n",
        "print(\"  loss_all_summary.png - 전체 요약\")\n",
        "\n",
        "print(\"\\n다음 학습 주제:\")\n",
        "print(\"  - 최적화 알고리즘 (SGD, Adam, AdamW)\")\n",
        "print(\"  - 정규화 기법 (Dropout, Weight Decay)\")\n",
        "print(\"  - 모델 평가 및 검증 전략\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"모든 실습 완료!\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ]
}
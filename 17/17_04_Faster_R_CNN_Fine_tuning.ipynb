{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Penn-Fudan Pedestrian Detection Dataset 사용\n",
        "# 환경 설정 및 필수 라이브러리\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision import transforms as T\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import shutil\n",
        "\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "print(f'TorchVision: {torchvision.__version__}')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device}')\n",
        "\n",
        "if device == 'cuda':\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
      ],
      "metadata": {
        "id": "o0gTGB91FPZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 다운로드 및 준비\n",
        "def download_pennfudan_dataset():\n",
        "    \"\"\"Penn-Fudan Pedestrian Detection Dataset 다운로드\"\"\"\n",
        "\n",
        "    data_dir = Path('./PennFudanPed')\n",
        "\n",
        "    if data_dir.exists():\n",
        "        print(f\"데이터셋이 이미 존재합니다: {data_dir}\")\n",
        "        return data_dir\n",
        "\n",
        "    print(\"Penn-Fudan Dataset 다운로드 중...\")\n",
        "    url = \"https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\"\n",
        "    zip_path = \"PennFudanPed.zip\"\n",
        "\n",
        "    # 다운로드\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "    print(f\"다운로드 완료: {zip_path}\")\n",
        "\n",
        "    # 압축 해제\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    print(f\"압축 해제 완료: {data_dir}\")\n",
        "\n",
        "    # 압축 파일 삭제\n",
        "    os.remove(zip_path)\n",
        "\n",
        "    return data_dir\n",
        "\n",
        "# 데이터셋 다운로드\n",
        "data_root = download_pennfudan_dataset()\n",
        "\n",
        "# 데이터 구조 확인\n",
        "print(\"\\n=== 데이터셋 구조 ===\")\n",
        "print(f\"이미지: {len(list((data_root / 'PNGImages').glob('*.png')))}개\")\n",
        "print(f\"마스크: {len(list((data_root / 'PedMasks').glob('*.png')))}개\")\n",
        "\n",
        "# 샘플 이미지 확인\n",
        "sample_img = Image.open(data_root / 'PNGImages' / 'FudanPed00001.png')\n",
        "sample_mask = Image.open(data_root / 'PedMasks' / 'FudanPed00001_mask.png')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "axes[0].imshow(sample_img)\n",
        "axes[0].set_title('Sample Image')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(sample_mask, cmap='tab20')\n",
        "axes[1].set_title('Sample Mask (Instance Segmentation)')\n",
        "axes[1].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u7cJ2oq5FXSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset 클래스 정의\n",
        "class PennFudanDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = Path(root)\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # 모든 이미지와 마스크 파일 로드\n",
        "        self.imgs = sorted(list((self.root / 'PNGImages').glob('*.png')))\n",
        "        self.masks = sorted(list((self.root / 'PedMasks').glob('*.png')))\n",
        "\n",
        "        print(f\"Dataset 초기화: {len(self.imgs)}개 이미지\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 이미지 로드\n",
        "        img_path = self.imgs[idx]\n",
        "        mask_path = self.masks[idx]\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        # 마스크를 numpy 배열로 변환\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        # 각 instance의 고유 ID 추출\n",
        "        obj_ids = np.unique(mask)\n",
        "        # 배경 제거 (ID 0)\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # 마스크를 binary mask로 분할\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # Bounding box 계산\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "\n",
        "            # 유효한 박스만 추가\n",
        "            if xmax > xmin and ymax > ymin:\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # Tensor로 변환\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.ones((len(boxes),), dtype=torch.int64)  # 모두 사람(class 1)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'masks': masks,\n",
        "            'image_id': image_id,\n",
        "            'area': area,\n",
        "            'iscrowd': iscrowd\n",
        "        }\n",
        "\n",
        "        # Transform 적용\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target"
      ],
      "metadata": {
        "id": "aI9RKm4RFooE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform 정의\n",
        "def get_transform(train=True):\n",
        "    \"\"\"데이터 Transform\"\"\"\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "metadata": {
        "id": "g13j_iABFsw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 분할 및 DataLoader\n",
        "# Train/Val 분할\n",
        "dataset_full = PennFudanDataset(data_root, get_transform(train=True))\n",
        "indices = torch.randperm(len(dataset_full)).tolist()\n",
        "split_idx = int(len(dataset_full) * 0.8)\n",
        "\n",
        "train_indices = indices[:split_idx]\n",
        "val_indices = indices[split_idx:]\n",
        "\n",
        "# Subset 생성\n",
        "train_dataset = torch.utils.data.Subset(dataset_full, train_indices)\n",
        "val_dataset_transforms = PennFudanDataset(data_root, get_transform(train=False))\n",
        "val_dataset = torch.utils.data.Subset(val_dataset_transforms, val_indices)\n",
        "\n",
        "print(f\"\\n=== 데이터 분할 ===\")\n",
        "print(f\"Train: {len(train_dataset)}개\")\n",
        "print(f\"Val: {len(val_dataset)}개\")\n",
        "\n",
        "# DataLoader\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "25E28lDOFvEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "def get_model(num_classes):\n",
        "\n",
        "    # 사전학습된 모델 로드\n",
        "    model = fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
        "\n",
        "    # Classifier head 교체\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "# 모델 초기화 (배경 + 사람 = 2 classes)\n",
        "model = get_model(num_classes=2)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"모델 파라미터 수: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ],
      "metadata": {
        "id": "z5A8RrSbFyCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 설정\n",
        "# Optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "print(f\"Epochs: {num_epochs}\")\n",
        "print(f\"Batch size: 4\")\n",
        "print(f\"Initial LR: 0.005\")\n",
        "print(f\"Optimizer: SGD with momentum\")"
      ],
      "metadata": {
        "id": "GWVovXCaF6cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 함수\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    \"\"\"1 에폭 학습\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    loss_classifier = 0\n",
        "    loss_box_reg = 0\n",
        "    loss_objectness = 0\n",
        "    loss_rpn_box_reg = 0\n",
        "\n",
        "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    for images, targets in pbar:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # 통계\n",
        "        total_loss += losses.item()\n",
        "        loss_classifier += loss_dict['loss_classifier'].item()\n",
        "        loss_box_reg += loss_dict['loss_box_reg'].item()\n",
        "        loss_objectness += loss_dict['loss_objectness'].item()\n",
        "        loss_rpn_box_reg += loss_dict['loss_rpn_box_reg'].item()\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{losses.item():.4f}',\n",
        "            'cls': f'{loss_dict[\"loss_classifier\"].item():.3f}',\n",
        "            'box': f'{loss_dict[\"loss_box_reg\"].item():.3f}'\n",
        "        })\n",
        "\n",
        "    n = len(data_loader)\n",
        "    return {\n",
        "        'total_loss': total_loss / n,\n",
        "        'loss_classifier': loss_classifier / n,\n",
        "        'loss_box_reg': loss_box_reg / n,\n",
        "        'loss_objectness': loss_objectness / n,\n",
        "        'loss_rpn_box_reg': loss_rpn_box_reg / n\n",
        "    }\n"
      ],
      "metadata": {
        "id": "h5z69iCrGC4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가 함수\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "    \"\"\"검증 세트 평가\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_predictions = 0\n",
        "    total_targets = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "        images = [img.to(device) for img in images]\n",
        "        outputs = model(images)\n",
        "\n",
        "        for output, target in zip(outputs, targets):\n",
        "            pred_boxes = output['boxes'].cpu()\n",
        "            pred_scores = output['scores'].cpu()\n",
        "            target_boxes = target['boxes']\n",
        "\n",
        "            # Confidence > 0.5인 예측만 사용\n",
        "            mask = pred_scores > 0.5\n",
        "            pred_boxes = pred_boxes[mask]\n",
        "\n",
        "            total_predictions += len(pred_boxes)\n",
        "            total_targets += len(target_boxes)\n",
        "\n",
        "            # 간단한 정확도: 예측 수와 타겟 수의 차이\n",
        "            correct_predictions += min(len(pred_boxes), len(target_boxes))\n",
        "\n",
        "    accuracy = correct_predictions / max(total_targets, 1)\n",
        "    precision = correct_predictions / max(total_predictions, 1) if total_predictions > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'total_predictions': total_predictions,\n",
        "        'total_targets': total_targets\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ORdaGYYiGFkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 루프\n",
        "\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_accuracy': [],\n",
        "    'val_precision': [],\n",
        "    'learning_rate': []\n",
        "}\n",
        "\n",
        "best_accuracy = 0.0\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 학습\n",
        "    train_metrics = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "\n",
        "    # 검증\n",
        "    val_metrics = evaluate(model, val_loader, device)\n",
        "\n",
        "    # Learning rate 업데이트\n",
        "    lr_scheduler.step()\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # 기록\n",
        "    history['train_loss'].append(train_metrics['total_loss'])\n",
        "    history['val_accuracy'].append(val_metrics['accuracy'])\n",
        "    history['val_precision'].append(val_metrics['precision'])\n",
        "    history['learning_rate'].append(current_lr)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"\\nEpoch {epoch} 결과:\")\n",
        "    print(f\"  Train Loss: {train_metrics['total_loss']:.4f}\")\n",
        "    print(f\"    - Classifier: {train_metrics['loss_classifier']:.4f}\")\n",
        "    print(f\"    - Box Reg: {train_metrics['loss_box_reg']:.4f}\")\n",
        "    print(f\"    - Objectness: {train_metrics['loss_objectness']:.4f}\")\n",
        "    print(f\"    - RPN Box Reg: {train_metrics['loss_rpn_box_reg']:.4f}\")\n",
        "    print(f\"  Val Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "    print(f\"  Val Precision: {val_metrics['precision']:.4f}\")\n",
        "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "    # Best model 저장\n",
        "    if val_metrics['accuracy'] > best_accuracy:\n",
        "        best_accuracy = val_metrics['accuracy']\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'accuracy': val_metrics['accuracy'],\n",
        "            'history': history\n",
        "        }, 'best_model.pth')\n",
        "        print(f\"  ✓ Best model saved! (Accuracy: {val_metrics['accuracy']:.4f})\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"학습 완료!\")\n",
        "print(f\"총 학습 시간: {total_time/60:.1f}분\")\n",
        "print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "id": "oji-eRvgGUhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 결과 시각화\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history['train_loss'], marker='o', linewidth=2, label='Train Loss')\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Accuracy & Precision\n",
        "axes[1].plot(history['val_accuracy'], marker='o', linewidth=2, label='Accuracy', color='green')\n",
        "axes[1].plot(history['val_precision'], marker='s', linewidth=2, label='Precision', color='orange')\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Score', fontsize=12)\n",
        "axes[1].set_title('Validation Metrics', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "# Learning Rate\n",
        "axes[2].plot(history['learning_rate'], marker='o', linewidth=2, color='purple', label='LR')\n",
        "axes[2].set_xlabel('Epoch', fontsize=12)\n",
        "axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
        "axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "axes[2].set_yscale('log')\n",
        "axes[2].legend(fontsize=11)\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B5jLdOFrGV8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best 모델 로드\n",
        "print(\"\\n=== Best Model 로드 ===\")\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(f\"Best model (Epoch {checkpoint['epoch']}, Accuracy: {checkpoint['accuracy']:.4f}) 로드 완료!\")"
      ],
      "metadata": {
        "id": "QtwVqUOgGbiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 추론 및 시각화 함수\n",
        "def predict_and_visualize(model, dataset, idx, device, conf_threshold=0.5):\n",
        "    \"\"\"단일 이미지 추론 및 시각화\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 이미지와 타겟 가져오기\n",
        "    img, target = dataset[idx]\n",
        "\n",
        "    # 추론\n",
        "    with torch.no_grad():\n",
        "        prediction = model([img.to(device)])[0]\n",
        "\n",
        "    # CPU로 이동\n",
        "    img_np = (img.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8).copy()\n",
        "\n",
        "    pred_boxes = prediction['boxes'].cpu().numpy()\n",
        "    pred_scores = prediction['scores'].cpu().numpy()\n",
        "    pred_labels = prediction['labels'].cpu().numpy()\n",
        "\n",
        "    gt_boxes = target['boxes'].numpy()\n",
        "\n",
        "    # Confidence threshold 적용\n",
        "    mask = pred_scores >= conf_threshold\n",
        "    pred_boxes = pred_boxes[mask]\n",
        "    pred_scores = pred_scores[mask]\n",
        "\n",
        "    # 시각화\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    # Ground Truth\n",
        "    img_gt = img_np.copy()\n",
        "    for box in gt_boxes:\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "        cv2.rectangle(img_gt, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "        cv2.putText(img_gt, 'Person', (x1, y1-10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "    axes[0].imshow(img_gt)\n",
        "    axes[0].set_title(f'Ground Truth ({len(gt_boxes)} persons)', fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Prediction\n",
        "    img_pred = img_np.copy()\n",
        "    for box, score in zip(pred_boxes, pred_scores):\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "        cv2.rectangle(img_pred, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
        "        text = f'Person: {score:.2f}'\n",
        "        cv2.putText(img_pred, text, (x1, y1-10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
        "\n",
        "    axes[1].imshow(img_pred)\n",
        "    axes[1].set_title(f'Prediction ({len(pred_boxes)} persons, threshold: {conf_threshold})',\n",
        "                     fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Ground Truth: {len(gt_boxes)}명\")\n",
        "    print(f\"Prediction: {len(pred_boxes)}명\")\n",
        "    if len(pred_boxes) > 0:\n",
        "        print(f\"Confidence 범위: {pred_scores.min():.3f} ~ {pred_scores.max():.3f}\")"
      ],
      "metadata": {
        "id": "yYyp6_lRGhir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 여러 샘플 테스트\n",
        "print(\"\\n=== 추론 결과 샘플 ===\\n\")\n",
        "\n",
        "# Validation 세트에서 랜덤 샘플 선택\n",
        "sample_indices = np.random.choice(len(val_dataset), size=min(5, len(val_dataset)), replace=False)\n",
        "\n",
        "for i, idx in enumerate(sample_indices, 1):\n",
        "    print(f\"Sample {i}:\")\n",
        "    predict_and_visualize(model, val_dataset, idx, device, conf_threshold=0.5)\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "aSr2zJA7GqXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confidence Threshold 비교\n",
        "print(\"\\n=== Confidence Threshold 영향 분석 ===\\n\")\n",
        "\n",
        "sample_idx = sample_indices[0]\n",
        "thresholds = [0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "img, target = val_dataset[sample_idx]\n",
        "img_np = (img.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8).copy()\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])[0]\n",
        "\n",
        "pred_boxes = prediction['boxes'].cpu().numpy()\n",
        "pred_scores = prediction['scores'].cpu().numpy()\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, thresh in enumerate(thresholds):\n",
        "    mask = pred_scores >= thresh\n",
        "    boxes = pred_boxes[mask]\n",
        "    scores = pred_scores[mask]\n",
        "\n",
        "    img_temp = img_np.copy()\n",
        "    for box, score in zip(boxes, scores):\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "        cv2.rectangle(img_temp, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "        text = f'{score:.2f}'\n",
        "        cv2.putText(img_temp, text, (x1, y1-5),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "    axes[idx].imshow(img_temp)\n",
        "    axes[idx].set_title(f'Threshold: {thresh} ({len(boxes)} detections)',\n",
        "                       fontsize=12, fontweight='bold')\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "    print(f\"Threshold {thresh}: {len(boxes)}개 검출\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e4xKKy2KGsL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 성능 평가\n",
        "\n",
        "final_metrics = evaluate(model, val_loader, device)\n",
        "\n",
        "print(f\"\\nValidation Set 최종 결과:\")\n",
        "print(f\"  Accuracy: {final_metrics['accuracy']:.4f}\")\n",
        "print(f\"  Precision: {final_metrics['precision']:.4f}\")\n",
        "print(f\"  Total Predictions: {final_metrics['total_predictions']}\")\n",
        "print(f\"  Total Targets: {final_metrics['total_targets']}\")"
      ],
      "metadata": {
        "id": "Zh1cBvTNGysa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 모델 저장\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'num_classes': 2,\n",
        "    'history': history,\n",
        "    'final_metrics': final_metrics\n",
        "}, 'final_model.pth')"
      ],
      "metadata": {
        "id": "NAzEkIoGG37_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWOK6SnQTs01"
      },
      "outputs": [],
      "source": [
        "# 요약 정리\n",
        "print(\"\\n\\\\실습 요약:\")\n",
        "print(f\"  • 데이터셋: Penn-Fudan Pedestrian Detection\")\n",
        "print(f\"  • Train 샘플: {len(train_dataset)}개\")\n",
        "print(f\"  • Val 샘플: {len(val_dataset)}개\")\n",
        "print(f\"  • Epochs: {num_epochs}\")\n",
        "print(f\"  • Best Validation Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"  • 총 학습 시간: {total_time/60:.1f}분\")\n"
      ]
    }
  ]
}